<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://thematrixmaster.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thematrixmaster.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-09-04T05:54:39+00:00</updated><id>https://thematrixmaster.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Practical Foray into Network Analysis [Part 1]</title><link href="https://thematrixmaster.github.io/blog/2023/network-analysis-p1/" rel="alternate" type="text/html" title="A Practical Foray into Network Analysis [Part 1]"/><published>2023-09-03T00:00:00+00:00</published><updated>2023-09-03T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/network-analysis-p1</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/network-analysis-p1/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>A majority of biological data is most suitably modelled as graphs. From the atom-bond model of small molecules to the residue-backbone structure of proteins to the complex interaction networks of signal transduction pathways, the possible configurations are endless.</p> <p>Recently, I’ve been particularly interested by these research areas incorporating network science and graph representation learning to tackle problems in biology and chemistry such as drug design <d-cite key="bengio2021flow"></d-cite> and material science engineering <d-cite key="duval2023faenet"></d-cite>.</p> <p>In this series of blog posts, my goal is to document my learning process from the most basic principles in network science to advanced topics in graph neural networks through a hands-on application of techniques to interesting datasets that I can get my hands on.</p> <p>In this part 1, I focus on graph data visualization using the Canadian <a href="www.foodb.ca">FooDB</a> dataset covering detailed compositional, biochemical and physiological information about common food items.</p> <h2 id="food-centric-view">Food-Centric View</h2> <p>Given that the database documents the absolute enrichment of many compounds in common foods, a natural first approach is to take the food-centric view and ask whether certain compounds are especially enriched in some categories of food. To address this question in a visual way, let’s try to construct a food-centric graph. Let \(G=(V,E)\) denote our graph where each node \(v \in V\) represents a food item (ex: strawberries) and \(\exists (u,v) \in E \iff\) the foods \((u,v)\) share at least one compound. Then, for each undirected edge \((u,v) \in E\), we can add a normalized edge weight using the following function \(f(u,v)=\sum_{k\in K}{\frac{1}{S_k}}\) where \(K\) is the set of all compounds shared between foods \((u,v)\) and \(S_{k}\) is the number of foods in which compound \(k\) is found. This function \(f\) penalizes the weight of compounds that are shared by too many foods. Finally, we make node size proportional to node degree which highlights foods that are highly interconnected in the graph by sharing many compounds with other foods.</p> <h3 id="graph-visualization">Graph Visualization</h3> <p>For visualization, let’s use the network layout algorithm <a href="10.1371/journal.pone.0098679">ForceAtlas</a> in the Gephi<d-cite key="ICWSM09154"></d-cite> software that takes into account edge weight to produce a graph where strongly connected components are clustered together.</p> <p>However, I quickly realized that doing this on the entire compound dataset yielded highly connected inexplicable graphs because there were many highly shared compounds that were practically found in all food items. Further, there were also a number of compounds that were only measured in a single food item, so discarding these wouldn’t affect our visualization either. After removing these compounds, I still had too much data to process so I chose to only keep the observations for the 1500 least abundant compounds that were shared by at least one pair of food items. This yielded the following graph on all 1024 food items.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_food.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 1. Food-centric undirected graph where food item vertices and edges representing a normalized count of shared compounds between foods </div> <p>As you can expect, the layout yields pretty well clusters for some major food groups such as fruits, spices, animal foods (meat products), and vegetables. I also learned that pulses are the seeds of dry legumes such as beans, lentils, and peas.</p> <p>Looking at this graph, I saw that vegetables seem to cluster into two distinct families, so I decided to repeat the experiment, but only include foods from the vegetable family. This yielded the following graph:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 2. Food-centric undirected graph for the vegetable food category </div> <p>This graph shows that root vegetables, leaf vegetables, and tubers cluster together while fruit vegetables (tomato, pepper, etc.), onions, and mushrooms form their own distinct clusters.</p> <h3 id="post-analysis">Post Analysis</h3> <p>Now that we have these food-centric graphs that seem to cluster food categories by their composition, I’d like to know which compounds are actually more enriched in which kinds of foods. To accomplish this, I’ll group the foods by their category, then look for compounds that are relatively enriched in a given category. To choose an <strong>enrichment</strong> metric, I wanted to factor in the following considerations:</p> <ul> <li>Breadth: a compound should have higher enrichment value if it is found in a larger proportion of the individual foods of a food family</li> <li>Depth: a compound should have higher enrichment value if it has a higher concentration in a given food item of the family</li> <li>Normalization: a compound’s enrichment in a family should be computed with respect to its enrichment in the other food families. In other words, enrichment is relative and not absolute.</li> </ul> <p>To accomplish this, I first compute a local enrichment score \(S_{F,c}\) for each compound \(c\) in each food family \(F\) using the following formula:</p> \[S_{F,c} = \frac{1}{|F|} \sum_{f}[c]_f\] <p>where \(\|F\|\) is the number of foods in a food family, and \([c]_f\) is the concentration of compound \(c\) in food \(f\).</p> <p>Then, I compute the final relative enrichment score \(S^r_{F,c}\) by normalizing the local enrichment score against the scores of compound \(c\) accross all food families \(F\):</p> \[S^r_{F,c} = \frac{S_{F,c}}{\sum_F S_{F,c}}\] <p>Computing this metric accross all food categories, then sorting the values in non-ascending order yielded a list of the most relatively enriched compounds in each food family. I’ve summarized the most enriched compound in each food category in the table below. A value of 1.0 means the compound was only found in that food category.</p> <table> <thead> <tr> <th>Food Category</th> <th style="text-align: right">Most Enriched Compound</th> <th style="text-align: right">\(S^r_{F,c}\)</th> </tr> </thead> <tbody> <tr> <td>Animal Foods</td> <td style="text-align: right">4-Hydroxyproline</td> <td style="text-align: right">0.446359</td> </tr> <tr> <td>Aquatic Foods</td> <td style="text-align: right">Eicosapentaenoic acid</td> <td style="text-align: right">0.926140</td> </tr> <tr> <td>Baby Foods</td> <td style="text-align: right">beta-Lactose</td> <td style="text-align: right">0.945600</td> </tr> <tr> <td>Baking Goods</td> <td style="text-align: right">Caffeic acid ethyl ester</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Cocoa Products</td> <td style="text-align: right">Theophylline</td> <td style="text-align: right">0.988216</td> </tr> <tr> <td>Coffee Products</td> <td style="text-align: right">4-Feruloylquinic acid</td> <td style="text-align: right">0.999425</td> </tr> <tr> <td>Eggs</td> <td style="text-align: right">Arachidonic acid</td> <td style="text-align: right">0.730055</td> </tr> <tr> <td>Fats and oils</td> <td style="text-align: right">Vaccenic acid</td> <td style="text-align: right">0.916681</td> </tr> <tr> <td>Fruits</td> <td style="text-align: right">Cyanidin 3</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Gourds</td> <td style="text-align: right">Kynurenine</td> <td style="text-align: right">0.971037</td> </tr> <tr> <td>Herbs and Spices</td> <td style="text-align: right">Luteolin 7</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Milk Products</td> <td style="text-align: right">D-Tryptophan</td> <td style="text-align: right">0.997252</td> </tr> <tr> <td>Nuts</td> <td style="text-align: right">N-Dodecane</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Snack foods</td> <td style="text-align: right">D-Galactose</td> <td style="text-align: right">0.387740</td> </tr> <tr> <td>Soy</td> <td style="text-align: right">Formononetin</td> <td style="text-align: right">0.998967</td> </tr> <tr> <td>Tea</td> <td style="text-align: right">Theaflavin</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Vegetables</td> <td style="text-align: right">Isoorientin</td> <td style="text-align: right">1.0</td> </tr> </tbody> </table> <h3 id="relative-compound-enrichment-at-the-food-item-level">Relative Compound Enrichment at the Food Item Level</h3> <p>Now that we’ve identified some commonly enriched compounds, it seemed interesting to me to flip the perspective and identify the individual foods that are most relatively enriched with respect to a set of target compounds of interest. To visualize this information, I once again built a per-compound graph where each node represents a food item, but this time I decided to draw an undirected edge between two food items if they belong to the same food category. Finally, node size is representative of the relative compound enrichment in each food item. This visualization allows us to quickly see which food items and food families are relatively enriched in each target compound.</p> <p>Here is the graph for sugar compounds. As you would expect, the largest nodes correspond to foods such as <em>chocolate</em>, <em>candies</em> and some <em>fruits</em>.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 3. Relative food enrichment graph for sugar compounds </div> <p>Below, I’ve provided download links to the food enrichment graphs of some other compounds that I tested.</p> <table> <thead> <tr> <th>Compound</th> <th style="text-align: right">Link</th> </tr> </thead> <tbody> <tr> <td>Cholesterol</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_cholesterol.svg">Download Link</a></td> </tr> <tr> <td>Lactose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_lactose.svg">Download Link</a></td> </tr> <tr> <td>Maltose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_maltose.svg">Download Link</a></td> </tr> <tr> <td>Nitrogen</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_nitrogen.svg">Download Link</a></td> </tr> <tr> <td>Retinol</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_retinol.svg">Download Link</a></td> </tr> <tr> <td>Sodium</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_sodium.svg">Download Link</a></td> </tr> <tr> <td>Sucrose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_sucrose.svg">Download Link</a></td> </tr> </tbody> </table> <h2 id="compound-centric-view">Compound-Centric View</h2> <p>Now that we’ve analyzed the food-centric view, the next step is to look at the compound-centric view where we consider networks where each node represents a different compound. One interesting graph that we can build is a compound signature graph \(G=(V,E)\) for each food item \(f\) where each vertex \(v\in V\) corresponds to a compound that is measured in \(f\) with vertex size proportional to the concentration/enrichment of compound \(v\) in \(f\), denoted \([v]_f\). Then, for each pair of vertices \((u,v) \in V \times V\), we define an edge \((u,v,w) \in E\) where \(w\) is a weight metric that measures the co-occurence of compounds \((u,v)\) in different food items. Formally,</p> \[w_{uv} = \log_2 (S_{uv}+1)\] <p>where \(S_{uv}\) is the number of food items in which the compounds \((u,v)\) co-occur. Naturally, \(w_{uv}\) is lower bounded by 0 in the above equation. Before visualizing the graph for each food item, we prune the vertices that have no expression in the food which leaves us with a fully connected graph. However, edges with weight 1 in this graph are not very interesting to keep because it simply tells us that this edge joins two compounds that only co-occur in the current food item. So, to create a better sparse visualization, we also prune edges with unit weight.</p> <p>Here is the outcome for the <strong>beer</strong> food item.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/beer.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 4. Absolute compound enrichment in beer </div> <p>Here are some additional graphs for <strong>rice, chicken, and cow milk</strong>. I personally think that this provides a neat way to rapidly visualize a compound signature for every food item. A cool project idea to do next would be to compute a measure of graph edit distance between these structures to get a sense of <em>how different two food items are based on their composition</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/rice.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/chicken.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. Absolute compound enrichment in rice, chicken, and cow milk </div> <h2 id="conclusion">Conclusion</h2> <p>And that concludes the visualization work in this first part of the network analysis series. All I used to perform this analysis is some basic python code with the pandas and networkx libraries, as well as graph visualizations in Gephi. Although these graphs don’t provide very rigourous answers to some of the questions that we asked, I think that their main value lies in their capacity to convey meaningful information about the relationships between a very large amount of data points in a way that is natural for us to reason about. This large scale view of the dataset yields many insights for further analysis that I will explore in the next part of this series.</p>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="network"/><category term="graph"/><category term="visualization"/><category term="distill"/><summary type="html"><![CDATA[Motivation A majority of biological data is most suitably modelled as graphs. From the atom-bond model of small molecules to the residue-backbone structure of proteins to the complex interaction networks of signal transduction pathways, the possible configurations are endless.]]></summary></entry><entry><title type="html">Semantic Embedding w/ ChatGPT</title><link href="https://thematrixmaster.github.io/blog/2023/embedding-gpt/" rel="alternate" type="text/html" title="Semantic Embedding w/ ChatGPT"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/embedding-gpt</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/embedding-gpt/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Ever since the initial release of ChatGPT in November 2022, large language models have rapidly taken the spotlight of machine learning applications in industry. Leaders like Anthropic and Cohere have reached the unicorn status in the blink of an eye followed by a swarm of startups trying to apply <strong>ChatGPT to X</strong>. Like any other groundbreaking technology, large language models will take some time to fully integrate into our society, but it is undeniably something that is here to stay for the long term.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-1400.webp"/> <img src="/assets/img/blog/2023-08-23-embedding-gpt/funding.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Funding for generative AI has shot up by over 7 fold in the first half of 2023 (Image source: CB Insights, 2023) </div> <p>Given that progress is being made at blinding speed, I wanted to try out some of the awesome new tools being developed and write a tutorial on how to harness gpt-style models to your own use cases. Before we dive into the code, I want to briefly do an overview of the large language model architecture then explain the two main approaches being used to customize these models: finetuning versus embedding.</p> <h2 id="transformers">Transformers</h2> <p>At a very high level, large language models use a novel neural network architecture called transformers<d-cite key="vaswani2023attention"></d-cite> to generate the best responses to your prompt. You can think of transformers as extremely powerful sentence completion models that have <strong>efficiently</strong> learnt the underlying semantic patterns of written language to the extent that they can sample continuations to a textual prompt that are semantically meaningful. For an in depth understanding of the transformer architecture, I recommend this <a href="https://jalammar.github.io/illustrated-transformer/">blog post</a> by Jay Alammar. ChatGPT is an augmented version of the transformer model that was finetuned with reinforcement learning to better align with human instructions. Whether in the form of q&amp;a or simply listening to instructions, ChatGPT seems <strong>smart</strong> because it has been trained very well to align with human intent.<d-cite key="ouyang2022training"></d-cite></p> <h2 id="embedding-versus-finetuning">Embedding versus Finetuning</h2> <p>Although large language models are trained on data from the entire internet, the goal of these models is to learn the semantic patterns in language, not to memorize all the information on the internet. When we ask ChatGPT to answer a factual question like <em>“Which country won the most gold medals at the 2012 olympics?”</em>, the model does not have this information embedded in its finite set of parameters. Instead, it will perform a semantic search <d-footnote>You can think of this like a Google search</d-footnote> and insert the raw information it needs to answer our question into its context. This technique is known as <strong>semantic embedding</strong> and we can also use it to help ChatGPT access information that is relevant to our application domain. For example, if we want to build a personal gpt assistant that can answer relevant questions about all my previous tax returns, then I can put these relevant text documents into a database and allow a large language model to query it before answering my questions pertaining to this topic. As you can see, semantic embeddings don’t modify the underlying large language model at all. It just fetches the relevant information to better align the model’s answer with the users prompt.</p> <p>Finetuning on the other hand does modify the weights of the model. We already gave an example of finetuning earlier when we described how ChatGPT was trained downstream with reinforcement learning to better align with human instructions. Thus, it can be understood that the weights of the model encode something related to the built-in biases and choices made by the model when instructions are open to interpretation.</p> <p>A better way to explain this is by drawing a parallel with human behaviour. Humans are extremely good at following instructions, especially when these instructions are very detailed and leave little to no room for interpretation. However, when we are given vague instructions or open-ended questions, the responses that we provide reflect our personal opinions and biases. Even in the case when the possible responses are relatively constrained, biases are still present in low-level constructs such as sentence structure, word choice, and semantic sentiment. For example, although two doctors might give you the same diagnosis and prescription, the way they convey that information, the amount of detail they provide, and the tone they convey could be drastically different.</p> <p>The same can be said for large language models when they are prompted with tasks that are open to interpretation. If we want a model that consistently and reliably provides responses in a particular tone with a particular set of predetermined biases, then it makes more sense to finetune the model so that it aligns its decisions with this set of priors. For example, Claude is the flagship llm by Anthropic that is finetuned using a reinforcement learning policy aligned with a set of <a href="https://www.anthropic.com/index/claudes-constitution">constitutional principles</a> such as being <em>ethical</em>, <em>virtuous</em>, etc. In essense, finetuning can be thought of as introducing prior preferences into the llm that guide its responses on top of the fundamental prior that the model should be aligned with the user intent.</p> <h2 id="implementing-a-conversational-retrieval-qa-agent-in-langchain">Implementing a Conversational Retrieval Q&amp;A agent in LangChain</h2> <p>In the following tutorial, I will implement a conversational Q&amp;A agent such as ChatGPT that has access to the standard chat history context as well as a database of private documents that can be used for <strong>semantic embedding</strong> search. I will address model finetuning in a future blog post.</p> <h3 id="setting-up-a-zep-server">Setting up a Zep server</h3> <p>Recall that semantic embedding search involves searching for relevant documents in a database so that we can inject relevant information into the context of the model before providing a response. In our implementation, we will use <a href="https://github.com/getzep/zep">Zep</a> which is a long term RAM memory store that is optimized for storing text embeddings and performing various search algorithms over these embeddings.</p> <p>You can follow these instructions on their <a href="https://docs.getzep.com/deployment/quickstart/#starting-a-zep-server-locally-is-simple">getting started</a> page to start your Zep server. It is as easy as setting up some <code class="language-plaintext highlighter-rouge">API_KEY</code> environment variables and running a <code class="language-plaintext highlighter-rouge">docker-compose up</code> command.</p> <h3 id="adding-documents-to-the-zep-server">Adding documents to the Zep server</h3> <p>Now that the Zep database is up and running, we want to add our document embeddings into the database, so that we can perform retrieval against them. To do this, I wrote a python seeder script, but Zep also supports an SDK in native JavaScript as well as integration with 3rd party libraries such as LangChain and LlamaIndex which you can use to interact with the database as well.</p> <h4 id="creating-a-collection">Creating a collection</h4> <p>Now, the first step is to connect to the database and create a collection. Zep organizes its data in the following hierarchy: <strong>collection &gt; documents &gt; embeddings</strong>. Going back to the tax returns example, we could think of a document as individual tax related files, while a collection might group all such files for a particular year. Zep takes our documents and does the embedding for us, so we need to provide an embedding model. Here I choose the <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> model by OpenAI since we will be using ChatGPT for our llm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">zep_python</span> <span class="kn">import</span> <span class="n">ZepClient</span>
<span class="kn">from</span> <span class="n">zep_python.document</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">zep_api_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:8000</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">ZepClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">zep_api_url</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
Step 1: Create a new collection in which to store our documents for a given task class
        We will use chatgpt text-embedding-ada-002 model which has embedding size of 1536
</span><span class="sh">"""</span>

<span class="n">collection_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;name the collection&gt;</span><span class="sh">"</span>

<span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">delete_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">add_collection</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>  <span class="c1"># required
</span>        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">&lt;some description&gt;</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional
</span>        <span class="n">embedding_dimensions</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span>  <span class="c1"># this must match the model you've configured for 
</span>        <span class="n">is_auto_embedded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># use Zep's built-in embedder. Defaults to True
</span>    <span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <h4 id="chunking-up-documents">Chunking up documents</h4> <p>Our documents can have a large variation in their length and content size, so it doesn’t really make sense to squeeze each document into a fixed size embedding. Instead, we split up documents into fixed size chunks that have a predetermined token length, then embed each chunk into a vector embedding. Here I split my documents into chunks with a maximum of 1600 tokens per chunk, but this process will heavily depend on the nature and format of your documents. The code I provide below is just an example of how this chunking might be done, but you should write your own routine for this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DOC_DIR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents</span><span class="sh">"</span>
<span class="n">FILE_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents/raw_convo.txt</span><span class="sh">"</span>

<span class="c1"># Custom splitting for .txt file such that each entry in qa_data is a tuple of ([questions], answer)
# TODO: Add support for csv, json, and yml files
</span><span class="n">sections</span> <span class="o">=</span> <span class="nf">split_into_sections</span><span class="p">(</span><span class="n">FILE_NAME</span><span class="p">)</span>
<span class="n">qa_sections</span> <span class="o">=</span> <span class="nf">split_into_qa_pairs</span><span class="p">(</span><span class="n">sections</span><span class="p">)</span>
<span class="n">qa_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">section</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">qa_sections</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">questions</span><span class="p">,</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">qa_data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">questions</span><span class="p">,</span> <span class="n">answer</span><span class="p">))</span>

<span class="c1"># Split the qa pairs into chunks with a predefined max token length
</span><span class="n">MAX_TOKENS</span> <span class="o">=</span> <span class="mi">1600</span>
<span class="n">qa_strings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="n">qa_data</span><span class="p">:</span>
    <span class="n">qa_strings</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">split_strings_from_subsection</span><span class="p">(</span><span class="n">section</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="n">MAX_TOKENS</span><span class="p">))</span>
</code></pre></div></div> <h4 id="embedding-chunks-into-zep">Embedding chunks into Zep</h4> <p>The last step is to embed the chunks into Zep using the <code class="language-plaintext highlighter-rouge">Document</code> class. Here we could choose to add metadata to each chunk to identify, for example, which original file it belongs to. These metadata can then serve as future <em>filters</em> when we search against the Zep database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 3: Embed the document chunks and store them into the collection
</span><span class="sh">"""</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">Document</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
        <span class="n">document_id</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional document ID
</span>        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">bar</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>  <span class="c1"># optional metadata
</span>    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">qa_strings</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">uuids</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we can also spin up a busy waiting watcher process that waits for the documents to be embedded before exiting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 4: Wait for the documents to be embedded and monitor the process
</span><span class="sh">"""</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">get_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">Embedding status: </span><span class="sh">"</span>
        <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_embedded_count</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_count</span><span class="si">}</span><span class="s"> documents embedded</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="sh">"</span><span class="s">ready</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Added </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">uuids</span><span class="p">)</span><span class="si">}</span><span class="s"> documents to collection </span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="giving-chatgpt-access-to-the-zep-database">Giving ChatGPT access to the Zep database</h3> <p>Now that we have our vector database ready to go, we just need to hook up a language model to query from it and add the relevant embeddings to its context before answering the user. I used <a href="https://github.com/hwchase17/langchainjs">LangChain</a> which is an awesome framework that enables easy interaction with popular llms as well as integration with 3rd party plugins and databases such as Zep. Using the LangChain JavaScript SDK, I simply need to do the following steps:</p> <ol> <li>Connect to the Zep database</li> <li>Retrieve the user’s chat history along with his current active prompt</li> <li>Embed the chat history along with the current active prompt use as a prototype search vector</li> <li>Use the search vector to find semantically related embeddings in the Zep database</li> <li>Feed all the relevant embeddings from Zep and the chat context to an instance of ChatGPT model</li> <li>Return the model response to the user</li> </ol> <p>Using SvelteKit server module, this can be done in very few lines of code.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span> <span class="nx">OPENAI_ORGANIZATION</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">$env/static/private</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ChatOpenAI</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chat_models/openai</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">ConversationalRetrievalQAChain</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chains</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ZepVectorStore</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/vectorstores/zep</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">FakeEmbeddings</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/embeddings/fake</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">BufferMemory</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/memory</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">error</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">@sveltejs/kit</span><span class="dl">'</span><span class="p">;</span>

<span class="k">export</span> <span class="kd">type</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="p">{</span> 
    <span class="na">question</span><span class="p">:</span> <span class="kr">string</span><span class="p">;</span>
    <span class="nl">settings</span><span class="p">:</span> <span class="p">{</span> <span class="na">temperature</span><span class="p">:</span> <span class="kr">number</span><span class="p">,</span> <span class="na">relatedness</span><span class="p">:</span> <span class="kr">number</span> <span class="p">};</span>
<span class="p">}</span>

<span class="kd">const</span> <span class="nx">zepConfig</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">apiUrl</span><span class="p">:</span> <span class="dl">"</span><span class="s2">http://localhost:8000</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// the URL of your Zep implementation</span>
    <span class="na">collectionName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">&lt;collection_name&gt;</span><span class="dl">"</span><span class="p">,</span>  <span class="c1">// the name of your collection. alphanum values only</span>
    <span class="na">embeddingDimensions</span><span class="p">:</span> <span class="mi">1536</span><span class="p">,</span>  <span class="c1">// much match the embeddings you're using</span>
    <span class="na">isAutoEmbedded</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>  <span class="c1">// will automatically embed documents when they are added</span>
<span class="p">};</span>

<span class="kd">const</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span> <span class="o">=</span> <span class="s2">`
    Given the following conversation and a follow up question,
    return the conversation history excerpt that includes any relevant context to the question
    if it exists and rephrase the follow up question to be a standalone question.
    
    Chat History: {chat_history}
    Follow Up Input: {question}
    
    Your answer should follow the following format:

    </span><span class="se">\`\`\`</span><span class="s2">
    &lt;Here you can give some additional behavioural instructions to the model in the form of prompting. The result will not be as good as finetuning the model on a large amount of
    examples that properly introduce a set of behavioural guidelines for the model to respect.&gt;
    ----------------
    &lt;Relevant chat history excerpt as context here&gt;
    Standalone question: &lt;Rephrased question here&gt;
    </span><span class="se">\`\`\`</span><span class="s2">

    Your answer:
`</span><span class="p">;</span>

<span class="kd">const</span> <span class="nx">embeddings</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FakeEmbeddings</span><span class="p">();</span>

<span class="k">export</span> <span class="kd">const</span> <span class="nx">POST</span> <span class="o">=</span> <span class="k">async </span><span class="p">({</span> <span class="nx">request</span> <span class="p">})</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="na">body</span><span class="p">:</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">request</span><span class="p">.</span><span class="nf">json</span><span class="p">();</span>

    <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">body</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Data</span><span class="dl">'</span><span class="p">);</span>

    <span class="c1">// Connect to the Zep vector store server</span>
    <span class="kd">const</span> <span class="nx">vectorStore</span> <span class="o">=</span> <span class="k">await</span> <span class="k">new</span> <span class="nc">ZepVectorStore</span><span class="p">(</span><span class="nx">embeddings</span><span class="p">,</span> <span class="nx">zepConfig</span><span class="p">);</span>

    <span class="c1">// Create a new readable stream of the chat response</span>
    <span class="kd">const</span> <span class="nx">readableStream</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ReadableStream</span><span class="p">({</span>
        <span class="k">async</span> <span class="nf">start</span><span class="p">(</span><span class="nx">controller</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Create a new chat model</span>
            <span class="kd">const</span> <span class="nx">streamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-4</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">streaming</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
                <span class="na">callbacks</span><span class="p">:</span> <span class="p">[{</span>
                    <span class="na">handleLLMNewToken</span><span class="p">:</span> <span class="k">async </span><span class="p">(</span><span class="na">token</span><span class="p">:</span> <span class="kr">string</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nx">controller</span><span class="p">.</span><span class="nf">enqueue</span><span class="p">(</span><span class="nx">token</span><span class="p">),</span>
                <span class="p">}]</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">nonStreamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-3.5-turbo</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">chain</span> <span class="o">=</span> <span class="nx">ConversationalRetrievalQAChain</span><span class="p">.</span><span class="nf">fromLLM</span><span class="p">(</span>
                <span class="nx">streamingModel</span><span class="p">,</span>
                <span class="nx">vectorStore</span><span class="p">.</span><span class="nf">asRetriever</span><span class="p">(),</span>
                <span class="p">{</span>
                    <span class="na">memory</span><span class="p">:</span> <span class="k">new</span> <span class="nc">BufferMemory</span><span class="p">({</span>
                        <span class="na">memoryKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">chat_history</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// Must be set to "chat_history"</span>
                        <span class="na">inputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">question</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the input to the chain</span>
                        <span class="na">outputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">text</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the final conversational output of the chain</span>
                        <span class="na">returnMessages</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span> <span class="c1">// If using with a chat model</span>
                    <span class="p">}),</span>
                    <span class="na">returnSourceDocuments</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                    <span class="na">questionGeneratorChainOptions</span><span class="p">:</span> <span class="p">{</span>
                        <span class="na">llm</span><span class="p">:</span> <span class="nx">nonStreamingModel</span><span class="p">,</span>
                        <span class="na">template</span><span class="p">:</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span>
                    <span class="p">},</span>
                <span class="p">},</span>
            <span class="p">);</span>

            <span class="kd">const</span> <span class="nx">question</span> <span class="o">=</span> <span class="nx">body</span><span class="p">.</span><span class="nx">question</span><span class="p">;</span>
            <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">question</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Question</span><span class="dl">'</span><span class="p">);</span>

            <span class="kd">const</span> <span class="nx">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">chain</span><span class="p">.</span><span class="nf">call</span><span class="p">({</span> <span class="nx">question</span> <span class="p">});</span>
            <span class="nx">controller</span><span class="p">.</span><span class="nf">close</span><span class="p">();</span>
        <span class="p">},</span>
    <span class="p">});</span>

    <span class="c1">// Create and return a response of the readable stream</span>
    <span class="k">return</span> <span class="k">new</span> <span class="nc">Response</span><span class="p">(</span><span class="nx">readableStream</span><span class="p">,</span> <span class="p">{</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">text/plain</span><span class="dl">'</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">});</span>
<span class="p">}</span>
</code></pre></div></div> <p>For more information on this script, please visit the following <a href="https://js.langchain.com/docs/modules/chains/popular/chat_vector_db">documentation</a>. The full code for my walkthrough can be found <a href="https://github.com/TheMatrixMaster/gpt-embeddings">here</a>.</p>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="llm"/><category term="nlp"/><category term="search"/><category term="distill"/><summary type="html"><![CDATA[Motivation Ever since the initial release of ChatGPT in November 2022, large language models have rapidly taken the spotlight of machine learning applications in industry. Leaders like Anthropic and Cohere have reached the unicorn status in the blink of an eye followed by a swarm of startups trying to apply ChatGPT to X. Like any other groundbreaking technology, large language models will take some time to fully integrate into our society, but it is undeniably something that is here to stay for the long term.]]></summary></entry><entry><title type="html">A Gentle Intro to Variational Autoencoders</title><link href="https://thematrixmaster.github.io/blog/2023/vae/" rel="alternate" type="text/html" title="A Gentle Intro to Variational Autoencoders"/><published>2023-08-12T00:00:00+00:00</published><updated>2023-08-12T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/vae</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/vae/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE)<d-cite key="kingma2022autoencoding"></d-cite>. This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.</p> <h2 id="how-to-frame-generative-models">How to frame generative models</h2> <p>When learning generative models, we assume that the dataset is generated by some unknown underlying source distribution \(p_r\). Our goal is to learn a distribution \(p_g\) that approximates \(p_r\) from which we can sample new realistic data points. Unfortunately, we often don’t have access to \(p_r\), so the best we can do is approximate another distribution \(p_{\hat{r}}\) such that \(p_{\hat{r}}\) maximizes the likelihood of producing the dataset if it were to repeatedly sample independently from it.</p> <p>Now, there are two main ways that we can go about learning \(p_g \to p_{\hat{r}}\)</p> <ol> <li>Learn the parameters of \(p_g\) directly through maximum likelihood estimation (MLE) by minimizing the KL divergence \(D_{KL}(p_{\hat{r}} \Vert p_g) = \lmoustache p_{\hat{r}}(x) \frac{p_{\hat{r}}(x)}{p_g(x)}dx\).</li> <li>Or, learn a differentiable generative function \(g_\theta\) that maps an existing prior distribution \(Z\) into \(p_g\) such that \(p_g = g_\theta(Z)\).</li> </ol> <p>The issue with the first approach is that the KL divergence loss is extremely unstable when the parameters we want to estimate (in this case the parameters of \(p_{\hat{r}}\)) can belong to an arbitrarily large family of distributions. Indeed, if we examine the KL divergence expression, we see that wherever \(p_{\hat{r}}(x) &gt; 0\), \(p_g(x) &gt; 0\) must also be true, otherwise we end up with an exploding gradient problem during learning as the loss goes to infinity. One way to get around this could be to use a “nicer” loss metric between distributions that is smooth and differentiable everywhere such as the Wasserstein distance.<d-footnote>For more information on Wasserstein methods, I recommend this great <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html"><b>blog post</b></a> by Alex Irpan on WGANs.</d-footnote>However, even if we were able to learn \(p_g\) in such a way, it may be difficult to sample from this distribution with diversity because we don’t know what the learnt parameters of \(p_g\) represent with respect to the data.</p> <p>The better approach, used by GAN and VAE, is the 2nd where we learn a generative function \(g_\theta\) that maps a handpicked prior distribution \(Z\) into the data space. The upside of this approach is that <em>hopefully</em>, if all goes well, the parameters of our prior distribution \(Z\) will be mapped to disentangled high-level features of the data. If we achieve this, then we can easily generate new samples with more control and variety, because we can now sample strategically from $z\sim Z$ (which we handpicked) and then evaluate $g_\theta(z)$.<d-footnote>Compare this to the previous approach where we have no idea what the latent distribution even looks like.</d-footnote> The downside of this approach is that we don’t know explicitly what $p_g$ is, but this is usually okay unless interpretability is very important to your task.</p> <h2 id="what-is-an-autoencoder">What is an Autoencoder</h2> <p>Before I introduce the variational autoencoder, I want to briefly go over its sibling, the autoencoder. The autoencoder is an unsupervised machine learning model whose purpose is to learn a more meaningful representation of the input data in lower dimensional space.<d-footnote>If you want a better intuitive understanding of why we would want to learn such a lower dimensional representation, I suggest this blog post on <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"><b>manifolds</b></a> by Chris Olah.</d-footnote> To accomplish this, the autoencoder trains two networks placed back-to-back — an encoder and a decoder.</p> <ul> <li>The encoder learns a function \(f_\phi\) that transforms the high-dimensional input into the low-dimensional latent representation \(z = f_\phi(x)\). Notice here that the latent distribution \(Z\) is unknown and uncontrolled for, meaning that we entirely let the model decide what is the best way to represent the latent embeddings.</li> <li>The decoder learns the inverse function \(g_\theta\) that attempts to transform the low-dimensional representation back into the original example such that \(x' = g_\theta(z)\).</li> <li>Naturally, the loss function aims to minimize the reconstruction error by minimizing the euclidean distance between the original example and the reconstructed example.</li> </ul> \[\mathbb{L}(\phi, \theta) = \frac{1}{n}\sum^n_{i=1}(x^i - g_\theta(f_\phi(x)))^2\] <p>To recap, the goal of the autoencoder is to learn meaningful representations of the data in a lower dimensional latent space by using an encoder-decoder pair.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/autoencoder-architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h2 id="vae-as-a-generative-autoencoder">VAE as a Generative Autoencoder</h2> <p>Recall that fundamentally, a generative model aims to learn a generative function \(g_\theta: Z \to p_g\) mapping the prior distribution \(Z\) into the data space that <strong>maximizes the likelihood of generating samples from the dataset</strong>. At this point, I hope that it is easier to notice that the function learned by the decoder above does exactly this likelihood maximization since it minimizes the mean squared error loss between the input examples <strong>from the dataset</strong> and the reconstructed outputs. However as mentioned before, the latent domain of this standard decoder function is uncontrolled for whereas we want our generative function \(g_\theta\) to have a handpicked prior domain \(Z\).</p> <p>The variational autoencoder (VAE) is thus simply an autoencoder supplemented with an inductive prior that the latent distribution \(Z\) should fit into a pre-selected family of handpicked probability distributions.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/vae-gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of variational autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h3 id="how-to-pick-the-distribution-family-for-z">How to pick the distribution family for Z</h3> <p>Most often, we constrain the distribution \(Z\) to be a multivariate gaussian distribution with diagonal covariance matrix \(\mathcal{N}(z|\mu, \sigma^2)\). Now that is mouthful, but the real question is why?</p> <p>Intuitively, we can think of the task of the generative function $g_\theta$ as having to learn something meaningful about the content in the data we wish to generate as well as learning to map the variation in this content to the variation in the low-dimensional latent space \(Z\). As I explained before, we don’t want to \(Z\) to be unconstrained, because we won’t know how to sample cheaply and representatively from it. On the opposite hand, we don’t want to over-constrain \(Z\) either because this might prevent the encoder from learning an expressive and meaningful latent representation of the data. The Gaussian distribution achieves this balance well because it introduces the least amount of prior knowledge into \(Z\) while being extremely easy to sample from.</p> <p>The diagonal covariance matrix constraint encourages the encoder to learn a multivariate gaussian where each dimension is independent from another. This is desirable when we want to learn the most fundamental sources of variation in the data which often happen to be independent. For example, in the MNIST dataset, we don’t want the model to conflate the representations of the number 1 and 7 just because they share some similarities.</p> <h3 id="deriving-the-vae-loss-function">Deriving the VAE loss function</h3> <p>This section of the blog post will be the most math heavy, but I hope that it can provide a better intuition for where the VAE loss comes from. Most resources that I’ve found online directly derive the loss starting from the KL divergence between the estimated and real bayesian posterior distributions of \(z\) conditioned on \(x\), but this seems like it skipped a few steps especially for those who aren’t well-versed in Bayesian theory. Instead, let’s start from first principles.</p> <p>Recall that our objective is to approximate a distribution \(p_{\hat{r}}\) that maximizes the likelihood of generating the dataset \(D\). To do this, we explicitly defined a prior distribution \(p(z)\) for the latent space and now we are attempting to learn a probabilistic decoder distribution \(p_\theta(x\vert z)\) through our generative function \(g_\theta\). Thus, it should be clear that our goal is to find the parameters \(\theta^*\) such that</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[p_\theta(x)] \\ &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}] \\ \end{split}\] <p>by <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p> <p>First, we note that attempting to learn \(\theta\) directly here using maximum likelihood estimation with loss \(-\log \mathbb{E}_{x \sim D}[p_\theta(x)]\) is impossible because \(p_\theta(x)\), which is known as the evidence in Bayesian statistics, is intractable. If we wanted to compute \(p_\theta(x)\), we would need to marginalize over all values of \(z\) and further, we don’t have access to the posterior distribution \(p_\theta(z\vert x)\).</p> <p>So we go for a different approach, notice that if we use a neural network to approximate the posterior \(p_\theta(z\vert x)\), then we can manipulate the expectation above to a tractable form. Notably, let \(q_\phi(z\vert x)\) be a probabilistic function learned by the VAE encoder parametrized by \(\phi\) such that our new objective function becomes</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}] \\ &amp; \propto \arg \max_{\theta} \mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{p(z)}{q_\phi(z|x)}] \\ &amp; = \arg \min_{\theta} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{q_\phi(z|x)}{p(z)}] \\ &amp; = \arg \min_{\theta, \phi} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{D_{KL}}(q_\phi(z|x)\Vert p(z)) \\ &amp; = \arg \min_{\theta, \phi} [-\text{likelihood} + \text{KL divergence}] \\ &amp; = \max_{\theta, \phi}ELBO(\theta, \phi) \\ &amp; = \min_{\theta, \phi}L_{VAE}(\theta, \phi) \end{split}\] <p>Notice that this loss expression is exactly what we intuitively wanted to do in the first place. <strong>Maximize the likelihood of generating the data from our dataset while adding a regularizer term that encourages the latent space distribution to fit in our gaussian prior \(p(z).\)</strong></p> <h2 id="implementing-a-vae-in-pytorch">Implementing a VAE in PyTorch</h2> <p>Now that we have all the pieces of the puzzle, let’s train a VAE in PyTorch to generate images of characters from the Simpsons. My implementation is based on this great github repository<d-cite key="subramanian2020"></d-cite> that offers a whole collection of the latest VAE architectures. For a comprehensive overview of a variety of autoencoder architectures, I recommend this blog post by Lillian Weng.<d-cite key="weng2018VAE"></d-cite></p> <h3 id="the-dataset">The dataset</h3> <p>I used a Simpsons <a href="https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset">dataset</a> of ~20000 character images. Loading the dataset into PyTorch is simply a matter of implementing the <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>      
        <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">([</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span><span class="p">.</span><span class="nf">iterdir</span><span class="p">()</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="n">suffix</span> <span class="o">==</span> <span class="sh">'</span><span class="s">.jpg</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[:</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)]</span> <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span> <span class="k">else</span> <span class="n">imgs</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">):]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="nf">default_loader</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transforms</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="mf">0.0</span> <span class="c1"># dummy data label to prevent breaking 
</span></code></pre></div></div> <h3 id="the-model">The model</h3> <p>We start with the encoder model which takes in a batch of images and outputs the parameters of our multi-variate gaussian distribution \(Z\). In the model architecture declaration below, we use convolutional layers in the encoder body to capture the image features followed by 2 different linear output layers for the mean and variance vectors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_channels</span> <span class="o">=</span> <span class="n">input_dim</span>
<span class="n">hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>

<span class="c1"># Declare the Encoder Body
</span><span class="k">for</span> <span class="n">h_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">h_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">in_channels</span> <span class="o">=</span> <span class="n">h_dim</span>

<span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="c1"># Declare the Encoder output layer
</span><span class="n">self</span><span class="p">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">fc_var</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
</code></pre></div></div> <p>Similarly to the encoder, the decoder architecture takes in a latent vector outputted by the encoder, then uses transposed convolution layers to upsample from the low dimensional latent representations. Finally, we use a conv output layer followed by tanh activation function to map the decoder output back to the normalized input pixel space \(\in [-1, 1]\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Declare Decoder Architecture
</span><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">hidden_dims</span><span class="p">.</span><span class="nf">reverse</span><span class="p">()</span>
<span class="n">self</span><span class="p">.</span><span class="n">decoder_input</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                <span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">())</span>
</code></pre></div></div> <h3 id="training">Training</h3> <p>During training, we feed the input batch through the encoder to obtain a list of mean and variance vectors. We then sample from this multivariate gaussian using a reparameterization function to obtain a list of latent vectors \([z]\). This step is important because it not only allows us to sample from \(Z\), but also to take a derivative with respect to the encoder parameters during backpropagation. Finally, we feed these latent vectors to the decoder, which outputs a tensor of reconstructed images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Split the result into mu and var components
</span>    <span class="c1"># of the latent Gaussian distribution
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_mu</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_var</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Use this to sample from the latent distribution Z
</span>    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mu</span>

<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder_input</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_layer</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
    <span class="k">return</span>  <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>
</code></pre></div></div> <p>Finally, we compute the ELBO loss derived above and backpropagate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">,</span> <span class="n">kld_weight</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="c1"># Maximizing the likelihood of the input dataset is equivalent to minimizing
</span>    <span class="c1"># the reconstruction loss of the variational autoencoder
</span>    <span class="n">recons_loss</span> <span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="c1"># KL divergence between our prior on Z and the learned latent space by the encoder
</span>    <span class="c1"># This measures how far the learned latent distribution deviates from a multivariate gaussian
</span>    <span class="n">kld_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span> <span class="n">mu</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">log_var</span><span class="p">.</span><span class="nf">exp</span><span class="p">(),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># The final loss is the reconstruction loss (likelihood) + the weighted KL divergence 
</span>    <span class="c1"># between our prior on Z and the learned latent distribution
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">recons_loss</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kld_weight</span> <span class="o">*</span> <span class="n">kld_loss</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Reconstruction_Loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">recons_loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">KLD</span><span class="sh">'</span><span class="p">:</span> <span class="n">kld_loss</span>
    <span class="p">}</span>
</code></pre></div></div> <h3 id="sampling">Sampling</h3> <p>When we want to sample, we can simply sample a latent vector \(z\) from our multivariate gaussian latent prior \(p(z)\), then feed it through the decoder.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">current_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div> <h3 id="results">Results</h3> <p>Here are the results of the VAE that I trained on the Simpsons datasets after 100 epochs with 64 batch size. On the left are images recostructed by the model, and on the right are images sampled from the decoder. The results are quite blurry which is a typical symptom of VAEs as the Gaussian prior inductive bias might be acting too strong. Training for more epochs should yield better results.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/recons-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/recons-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/recons-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/recons.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/sample-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/sample-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/sample-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/sample.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="vae"/><category term="generative"/><category term="distill"/><summary type="html"><![CDATA[Motivation Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE). This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.]]></summary></entry><entry><title type="html">The Beauty of the Late Night</title><link href="https://thematrixmaster.github.io/blog/2023/late-night/" rel="alternate" type="text/html" title="The Beauty of the Late Night"/><published>2023-08-07T23:01:00+00:00</published><updated>2023-08-07T23:01:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/late-night</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/late-night/"><![CDATA[<p>Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.</p> <p>Now I do agree that a 2am to 10am sleep schedule is not something to be proud of, but being a night owl also comes with its own unique merits. So, before I comply with my grandma’s stern request of sleeping each night before 11pm, I want to tell you about the beauty of the late night.</p> <p>If you’re like me and suffer from slight anxiety, you’ll likely find that late at night is when your productivity peaks. Personally, I am much more easily distractable during the day, while after 10pm is when I can maintain my longest concentration span. Perhaps it is due to the fact that late nights offer the most quiet, peaceful environment without distractions. Or maybe the reduced cortisol levels at night can downregulate the stress response and temporarily keep procrastination in check. Regardless of the reason, the late night has become a precious personal refuge for me to make progress on my todo list.</p> <p>Beyond productivity boosts, it is my belief that the late night provides a magical window during which it is easier to connect with the authentic soul of your surrounding environment. In my experience, when the moon shines at night, everything becomes more authentic — from people to cities to conversations and memories. It is difficult to describe this elusive feeling to you unless you’ve experienced it yourself. Personally, I don’t take it as a coincidence that my strongest friendships, deepest thoughts, and favourite memories have been shaped by the late night. If you don’t believe me, I encourage you to take a stroll around your neighbourhood next time you stay up past bedtime and take the time to slowly rediscover your surroundings. You might find something that you missed before.</p> <p>An activity that I have absolutely fallen for is nighttime photography. In the hopes of capturing the essense of the magical late night that I’ve introduced to you above, here is a collection of photographs of my favourite late night moments this year.</p> <h4 id="singapore">Singapore</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_5221.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_5254.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="korea--china">Korea &amp; China</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_6077.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_6929.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="japan">Japan</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_7774.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_7782.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="travel"/><category term="photography"/><summary type="html"><![CDATA[Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.]]></summary></entry></feed>