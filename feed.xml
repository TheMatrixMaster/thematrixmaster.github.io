<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://thematrixmaster.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thematrixmaster.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-15T22:48:14+00:00</updated><id>https://thematrixmaster.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Gentle Intro to Variational Autoencoders</title><link href="https://thematrixmaster.github.io/blog/2023/vae/" rel="alternate" type="text/html" title="A Gentle Intro to Variational Autoencoders"/><published>2023-08-12T00:00:00+00:00</published><updated>2023-08-12T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/vae</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/vae/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE)<d-cite key="kingma2022autoencoding"></d-cite>. This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.</p> <h2 id="how-to-frame-generative-models">How to frame generative models</h2> <p>When learning generative models, we assume that the dataset is generated by some unknown underlying source distribution \(p_r\). Our goal is to learn a distribution \(p_g\) that approximates \(p_r\) from which we can sample new realistic data points. Unfortunately, we often don’t have access to \(p_r\), so the best we can do is approximate another distribution \(p_{\hat{r}}\) such that \(p_{\hat{r}}\) maximizes the likelihood of producing the dataset if it were to repeatedly sample independently from it.</p> <p>Now, there are two main ways that we can go about learning \(p_g \to p_{\hat{r}}\)</p> <ol> <li>Learn the parameters of \(p_g\) directly through maximum likelihood estimation (MLE) by minimizing the KL divergence \(D_{KL}(p_{\hat{r}} \Vert p_g) = \lmoustache p_{\hat{r}}(x) \frac{p_{\hat{r}}(x)}{p_g(x)}dx\).</li> <li>Or, learn a differentiable generative function \(g_\theta\) that maps an existing prior distribution \(Z\) into \(p_g\) such that \(p_g = g_\theta(Z)\).</li> </ol> <p>The issue with the first approach is that the KL divergence loss is extremely unstable when the parameters we want to estimate (in this case the parameters of \(p_{\hat{r}}\)) can belong to an arbitrarily large family of distributions. Indeed, if we examine the KL divergence expression, we see that wherever \(p_{\hat{r}}(x) &gt; 0\), \(p_g(x) &gt; 0\) must also be true, otherwise we end up with an exploding gradient problem during learning as the loss goes to infinity. One way to get around this could be to use a “nicer” loss metric between distributions that is smooth and differentiable everywhere such as the Wasserstein distance.<d-footnote>For more information on Wasserstein methods, I recommend this great <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html"><b>blog post</b></a> by Alex Irpan on WGANs.</d-footnote>However, even if we were able to learn \(p_g\) in such a way, it may be difficult to sample from this distribution with diversity because we don’t know what the learnt parameters of \(p_g\) represent with respect to the data.</p> <p>The better approach, used by GAN and VAE, is the 2nd where we learn a generative function \(g_\theta\) that maps a handpicked prior distribution \(Z\) into the data space. The upside of this approach is that <em>hopefully</em>, if all goes well, the parameters of our prior distribution \(Z\) will be mapped to disentangled high-level features of the data. If we achieve this, then we can easily generate new samples with more control and variety, because we can now sample strategically from $z\sim Z$ (which we handpicked) and then evaluate $g_\theta(z)$.<d-footnote>Compare this to the previous approach where we have no idea what the latent distribution even looks like.</d-footnote> The downside of this approach is that we don’t know explicitly what $p_g$ is, but this is usually okay unless interpretability is very important to your task.</p> <h2 id="what-is-an-autoencoder">What is an Autoencoder</h2> <p>Before I introduce the variational autoencoder, I want to briefly go over its sibling, the autoencoder. The autoencoder is an unsupervised machine learning model whose purpose is to learn a more meaningful representation of the input data in lower dimensional space.<d-footnote>If you want a better intuitive understanding of why we would want to learn such a lower dimensional representation, I suggest this blog post on <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"><b>manifolds</b></a> by Chris Olah.</d-footnote> To accomplish this, the autoencoder trains two networks placed back-to-back — an encoder and a decoder.</p> <ul> <li>The encoder learns a function \(f_\phi\) that transforms the high-dimensional input into the low-dimensional latent representation \(z = f_\phi(x)\). Notice here that the latent distribution \(Z\) is unknown and uncontrolled for, meaning that we entirely let the model decide what is the best way to represent the latent embeddings.</li> <li>The decoder learns the inverse function \(g_\theta\) that attempts to transform the low-dimensional representation back into the original example such that \(x' = g_\theta(z)\).</li> <li>Naturally, the loss function aims to minimize the reconstruction error by minimizing the euclidean distance between the original example and the reconstructed example.</li> </ul> \[\mathbb{L}(\phi, \theta) = \frac{1}{n}\sum^n_{i=1}(x^i - g_\theta(f_\phi(x)))^2\] <p>To recap, the goal of the autoencoder is to learn meaningful representations of the data in a lower dimensional latent space by using an encoder-decoder pair.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/autoencoder-architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h2 id="vae-as-a-generative-autoencoder">VAE as a Generative Autoencoder</h2> <p>Recall that fundamentally, a generative model aims to learn a generative function \(g_\theta: Z \to p_g\) mapping the prior distribution \(Z\) into the data space that <strong>maximizes the likelihood of generating samples from the dataset</strong>. At this point, I hope that it is easier to notice that the function learned by the decoder above does exactly this likelihood maximization since it minimizes the mean squared error loss between the input examples <strong>from the dataset</strong> and the reconstructed outputs. However as mentioned before, the latent domain of this standard decoder function is uncontrolled for whereas we want our generative function \(g_\theta\) to have a handpicked prior domain \(Z\).</p> <p>The variational autoencoder (VAE) is thus simply an autoencoder supplemented with an inductive prior that the latent distribution \(Z\) should fit into a pre-selected family of handpicked probability distributions.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/vae-gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of variational autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h3 id="how-to-pick-the-distribution-family-for-z">How to pick the distribution family for Z</h3> <p>Most often, we constrain the distribution \(Z\) to be a multivariate gaussian distribution with diagonal covariance matrix \(\mathcal{N}(z|\mu, \sigma^2)\). Now that is mouthful, but the real question is why?</p> <p>Intuitively, we can think of the task of the generative function $g_\theta$ as having to learn something meaningful about the content in the data we wish to generate as well as learning to map the variation in this content to the variation in the low-dimensional latent space \(Z\). As I explained before, we don’t want to \(Z\) to be unconstrained, because we won’t know how to sample cheaply and representatively from it. On the opposite hand, we don’t want to over-constrain \(Z\) either because this might prevent the encoder from learning an expressive and meaningful latent representation of the data. The Gaussian distribution achieves this balance well because it introduces the least amount of prior knowledge into \(Z\) while being extremely easy to sample from.</p> <p>The diagonal covariance matrix constraint encourages the encoder to learn a multivariate gaussian where each dimension is independent from another. This is desirable when we want to learn the most fundamental sources of variation in the data which often happen to be independent. For example, in the MNIST dataset, we don’t want the model to conflate the representations of the number 1 and 7 just because they share some similarities.</p> <h3 id="deriving-the-vae-loss-function">Deriving the VAE loss function</h3> <p>This section of the blog post will be the most math heavy, but I hope that it can provide a better intuition for where the VAE loss comes from. Most resources that I’ve found online directly derive the loss starting from the KL divergence between the estimated and real bayesian posterior distributions of \(z\) conditioned on \(x\), but this seems like it skipped a few steps especially for those who aren’t well-versed in Bayesian theory. Instead, let’s start from first principles.</p> <p>Recall that our objective is to approximate a distribution \(p_{\hat{r}}\) that maximizes the likelihood of generating the dataset \(D\). To do this, we explicitly defined a prior distribution \(p(z)\) for the latent space and now we are attempting to learn a probabilistic decoder distribution \(p_\theta(x\vert z)\) through our generative function \(g_\theta\). Thus, it should be clear that our goal is to find the parameters \(\theta^*\) such that</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[p_\theta(x)] \\ &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}] \\ \end{split}\] <p>by <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p> <p>First, we note that attempting to learn \(\theta\) directly here using maximum likelihood estimation with loss \(-\log \mathbb{E}_{x \sim D}[p_\theta(x)]\) is impossible because \(p_\theta(x)\), which is known as the evidence in Bayesian statistics, is intractable. If we wanted to compute \(p_\theta(x)\), we would need to marginalize over all values of \(z\) and further, we don’t have access to the posterior distribution \(p_\theta(z\vert x)\).</p> <p>So we go for a different approach, notice that if we use a neural network to approximate the posterior \(p_\theta(z\vert x)\), then we can manipulate the expectation above to a tractable form. Notably, let \(q_\phi(z\vert x)\) be a probabilistic function learned by the VAE encoder parametrized by \(\phi\) such that our new objective function becomes</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}] \\ &amp; \propto \arg \max_{\theta} \mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{p(z)}{q_\phi(z|x)}] \\ &amp; = \arg \min_{\theta} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{q_\phi(z|x)}{p(z)}] \\ &amp; = \arg \min_{\theta, \phi} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{D_{KL}}(q_\phi(z|x)\Vert p(z)) \\ &amp; = \arg \min_{\theta, \phi} [-\text{likelihood} + \text{KL divergence}] \\ &amp; = \max_{\theta, \phi}ELBO(\theta, \phi) \\ &amp; = \min_{\theta, \phi}L_{VAE}(\theta, \phi) \end{split}\] <p>Notice that this loss expression is exactly what we intuitively wanted to do in the first place. <strong>Maximize the likelihood of generating the data from our dataset while adding a regularizer term that encourages the latent space distribution to fit in our gaussian prior \(p(z).\)</strong></p> <h2 id="implementing-a-vae-in-pytorch">Implementing a VAE in PyTorch</h2> <p>Now that we have all the pieces of the puzzle, let’s train a VAE in PyTorch to generate images of characters from the Simpsons. My implementation is based on this great github repository<d-cite key="subramanian2020"></d-cite> that offers a whole collection of the latest VAE architectures. For a comprehensive overview of a variety of autoencoder architectures, I recommend this blog post by Lillian Weng.<d-cite key="weng2018VAE"></d-cite></p> <h3 id="the-dataset">The dataset</h3> <p>I used a Simpsons <a href="https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset">dataset</a> of ~20000 character images. Loading the dataset into PyTorch is simply a matter of implementing the <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>      
        <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">([</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span><span class="p">.</span><span class="nf">iterdir</span><span class="p">()</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="n">suffix</span> <span class="o">==</span> <span class="sh">'</span><span class="s">.jpg</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[:</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)]</span> <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span> <span class="k">else</span> <span class="n">imgs</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">):]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="nf">default_loader</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transforms</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="mf">0.0</span> <span class="c1"># dummy data label to prevent breaking 
</span></code></pre></div></div> <h3 id="the-model">The model</h3> <p>We start with the encoder model which takes in a batch of images and outputs the parameters of our multi-variate gaussian distribution \(Z\). In the model architecture declaration below, we use convolutional layers in the encoder body to capture the image features followed by 2 different linear output layers for the mean and variance vectors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_channels</span> <span class="o">=</span> <span class="n">input_dim</span>
<span class="n">hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>

<span class="c1"># Declare the Encoder Body
</span><span class="k">for</span> <span class="n">h_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">h_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">in_channels</span> <span class="o">=</span> <span class="n">h_dim</span>

<span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="c1"># Declare the Encoder output layer
</span><span class="n">self</span><span class="p">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">fc_var</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
</code></pre></div></div> <p>Similarly to the encoder, the decoder architecture takes in a latent vector outputted by the encoder, then uses transposed convolution layers to upsample from the low dimensional latent representations. Finally, we use a conv output layer followed by tanh activation function to map the decoder output back to the normalized input pixel space \(\in [-1, 1]\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Declare Decoder Architecture
</span><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">hidden_dims</span><span class="p">.</span><span class="nf">reverse</span><span class="p">()</span>
<span class="n">self</span><span class="p">.</span><span class="n">decoder_input</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                <span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">())</span>
</code></pre></div></div> <h3 id="training">Training</h3> <p>During training, we feed the input batch through the encoder to obtain a list of mean and variance vectors. We then sample from this multivariate gaussian using a reparameterization function to obtain a list of latent vectors \([z]\). This step is important because it not only allows us to sample from \(Z\), but also to take a derivative with respect to the encoder parameters during backpropagation. Finally, we feed these latent vectors to the decoder, which outputs a tensor of reconstructed images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Split the result into mu and var components
</span>    <span class="c1"># of the latent Gaussian distribution
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_mu</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_var</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Use this to sample from the latent distribution Z
</span>    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mu</span>

<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder_input</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_layer</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
    <span class="k">return</span>  <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>
</code></pre></div></div> <p>Finally, we compute the ELBO loss derived above and backpropagate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">,</span> <span class="n">kld_weight</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="c1"># Maximizing the likelihood of the input dataset is equivalent to minimizing
</span>    <span class="c1"># the reconstruction loss of the variational autoencoder
</span>    <span class="n">recons_loss</span> <span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="c1"># KL divergence between our prior on Z and the learned latent space by the encoder
</span>    <span class="c1"># This measures how far the learned latent distribution deviates from a multivariate gaussian
</span>    <span class="n">kld_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span> <span class="n">mu</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">log_var</span><span class="p">.</span><span class="nf">exp</span><span class="p">(),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># The final loss is the reconstruction loss (likelihood) + the weighted KL divergence 
</span>    <span class="c1"># between our prior on Z and the learned latent distribution
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">recons_loss</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kld_weight</span> <span class="o">*</span> <span class="n">kld_loss</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Reconstruction_Loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">recons_loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">KLD</span><span class="sh">'</span><span class="p">:</span> <span class="n">kld_loss</span>
    <span class="p">}</span>
</code></pre></div></div> <h3 id="sampling">Sampling</h3> <p>When we want to sample, we can simply sample a latent vector \(z\) from our multivariate gaussian latent prior \(p(z)\), then feed it through the decoder.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">current_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div> <h3 id="results">Results</h3> <p>Here are the results of the VAE that I trained on the Simpsons datasets after 100 epochs with 64 batch size. On the left are images recostructed by the model, and on the right are images sampled from the decoder. The results are quite blurry which is a typical symptom of VAEs as the Gaussian prior inductive bias might be acting too strong. Training for more epochs should yield better results.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/recons-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/recons-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/recons-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/recons.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/sample-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/sample-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/sample-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/sample.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="vae"/><category term="generative"/><category term="distill"/><summary type="html"><![CDATA[Motivation Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE). This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.]]></summary></entry><entry><title type="html">The Beauty of the Late Night</title><link href="https://thematrixmaster.github.io/blog/2023/late-night/" rel="alternate" type="text/html" title="The Beauty of the Late Night"/><published>2023-08-07T23:01:00+00:00</published><updated>2023-08-07T23:01:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/late-night</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/late-night/"><![CDATA[<p>Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.</p> <p>Now I do agree that a 2am to 10am sleep schedule is not something to be proud of, but being a night owl also comes with its own unique merits. So, before I comply with my grandma’s stern request of sleeping each night before 11pm, I want to tell you about the beauty of the late night.</p> <p>If you’re like me and suffer from slight anxiety, you’ll likely find that late at night is when your productivity peaks. Personally, I am much more easily distractable during the day, while after 10pm is when I can maintain my longest concentration span. Perhaps it is due to the fact that late nights offer the most quiet, peaceful environment without distractions. Or maybe the reduced cortisol levels at night can downregulate the stress response and temporarily keep procrastination in check. Regardless of the reason, the late night has become a precious personal refuge for me to make progress on my todo list.</p> <p>Beyond productivity boosts, it is my belief that the late night provides a magical window during which it is easier to connect with the authentic soul of your surrounding environment. In my experience, when the moon shines at night, everything becomes more authentic — from people to cities to conversations and memories. It is difficult to describe this elusive feeling to you unless you’ve experienced it yourself. Personally, I don’t take it as a coincidence that my strongest friendships, deepest thoughts, and favourite memories have been shaped by the late night. If you don’t believe me, I encourage you to take a stroll around your neighbourhood next time you stay up past bedtime and take the time to slowly rediscover your surroundings. You might find something that you missed before.</p> <p>An activity that I have absolutely fallen for is nighttime photography. In the hopes of capturing the essense of the magical late night that I’ve introduced to you above, here is a collection of photographs of my favourite late night moments this year.</p> <h4 id="singapore">Singapore</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_5221-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_5221-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_5221-1400.webp"/> <img src="/assets/img/at-night/IMG_5221.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_5254-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_5254-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_5254-1400.webp"/> <img src="/assets/img/at-night/IMG_5254.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="korea--china">Korea &amp; China</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_6077-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_6077-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_6077-1400.webp"/> <img src="/assets/img/at-night/IMG_6077.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_6929-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_6929-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_6929-1400.webp"/> <img src="/assets/img/at-night/IMG_6929.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="japan">Japan</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_7774-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_7774-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_7774-1400.webp"/> <img src="/assets/img/at-night/IMG_7774.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/at-night/IMG_7782-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/at-night/IMG_7782-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/at-night/IMG_7782-1400.webp"/> <img src="/assets/img/at-night/IMG_7782.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="travel"/><category term="photography"/><summary type="html"><![CDATA[Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.]]></summary></entry></feed>