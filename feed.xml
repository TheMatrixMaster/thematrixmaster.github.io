<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://thematrixmaster.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://thematrixmaster.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-07-02T23:10:25+00:00</updated><id>https://thematrixmaster.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Latent Dirichlet Allocation</title><link href="https://thematrixmaster.github.io/blog/2023/latent-dirichlet/" rel="alternate" type="text/html" title="Latent Dirichlet Allocation"/><published>2023-12-27T00:00:00+00:00</published><updated>2023-12-27T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/latent-dirichlet</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/latent-dirichlet/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>If you’ve ever played around with unsupervised clustering algorithms like k-means, then the concept of topic modeling should already be familiar to you. Informally, topic modelling can be thought of as discovering the underlying “topics” or “themes” that are present in a collection of documents. For example, if we were to apply topic modeling to a collection of news articles, we might expect to find topics like “politics”, “sports”, “entertainment”, etc.</p> <p>Many topic models are Bayesian probabilistic models that make assumptions about how the dataset documents are generated. We call this the generative process. By performing MLE of the dataset with respect to the model parameters, we can discover the latent variables in the Bayesian model that are responsible for generating the documents. Naturally, different generative processes lead to different topic models. In this post, we’ll be looking at Latent Dirichlet Allocation (LDA) introduced by Blei et al.<d-cite key="blei2003lda"></d-cite> in 2003.</p> <h2 id="generative-process">Generative Process</h2> <p>Given a dataset of \(D\) documents, a vocabulary \(V\), and a set of \(K\) topics, the LDA generative process to create a document \(d\) is as follows:</p> <ol> <li> <p>Define a prior distribution over the topic proportions in \(d\) from a Dirichlet distribution with parameter \(\alpha\):</p> \[\theta_d \sim \text{Dir}(\alpha)\] </li> <li> <p>Define a global prior distribution over the word proportions in each topic from a Dirichlet distribution with parameter $\beta$:</p> \[\phi_k \sim \text{Dir}(\beta)\] </li> <li> <p>while not done:</p> <ol> <li> <p>Sample a topic \(z\) from the topic proportions in \(d\):</p> \[z \sim \text{Multinomial}(\theta_d)\] </li> <li> <p>Sample a word \(w\) from the word proportions in \(z\):</p> \[w \sim \text{Multinomial}(\phi_z)\] </li> <li> <p>Stop with probability \(\epsilon\) or continue.</p> </li> </ol> </li> </ol> <p>We often use the plate notation to represent the generative process. The plate notation for LDA is shown below:</p> <p><img src="/assets/img/blog/2023-12-27-latent-dirichlet/plate.png" alt="Plate notation for LDA"/></p> <h2 id="inference">Inference</h2> <p>First and foremost, the parameters that we wish to infer in this model are the topic proportions \(\theta_d\) and the word proportions \(\phi_k\). Notice that these two variables are sufficient to define the entire generative process. In other words, if we know the topic proportions \(\theta\) and the word proportions \(\phi\) that truly generated the dataset, then we can generate new documents that are indistinguishable from the original dataset. Since we don’t know the true values of \(\theta\) and \(\phi\), we must infer them from the observed documents $d$ using maximum likelihood estimation of the evidence with respect to these parameters:</p> \[\begin{align*} \theta^*, \phi^* &amp;= \arg\max_{\theta, \phi} \log p(d | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d, z | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d | z, \theta, \phi) p(z | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d | z, \theta, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z \prod_{n=1}^N p(w_n | z, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \log \prod_{n=1}^N \sum_z p(w_n | z, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \sum_{n=1}^N \log \sum_z p(w_n | z, \phi) p(z | \theta) \end{align*}\] <p>Unfortunately, this likelihood function is intractable to optimize directly, because we cannot marginalize over all the possible latent topic assignments \(z\) for each word \(w_n\). Instead, to tackle the partition function \(p(d \| \theta, \phi)\), we must turn to approximate inference methods such as <a href="/blog/2023/variational-inf-1/">variational inference</a> which we introduced in a previous post. Although VI still works here, we will use an approach from the Monte Carlo Markov Chain (MCMC) family of methods called Gibbs sampling<d-cite key="gibbs1992"></d-cite>.</p> <p>The idea behind Gibbs sampling<d-cite key="gibbs1992"></d-cite> is pretty straightforward. In cases where marginalizing the joint distribution \(p(z_1, w_1, z_2, w_2, \dots, z_n, w_n \vert \theta, \phi)\) over the latents \(z\) is intractable, we can instead sample individual latents from their conditional distributions \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\), use these sampled latents to update the parameters \(\theta\) and \(\phi\) to their new expected values, and repeat this process until convergence.</p> <p>In the case of LDA, we iteratively sample the latent topic assignments \(z_{i,d}\) for each word \(w_{i,d}\) in document \(d\), and then update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. The conditional distribution for \(z_{i,d}\) is given by:</p> \[\begin{align*} p(z_{i,d} &amp;= k \vert z_{(j,e) \neq (i,d)}, w_{i,d}=v, \theta, \phi) \\ &amp;\propto (\alpha_k + n_{(\cdot,d,k) \neq (v,d,\cdot)})\frac{\beta + n_{(v,\cdot,k) \neq (v,d,\cdot)}}{\sum_w \beta + n_{(w,\cdot,k) \neq (v,d,\cdot)}} \end{align*}\] <p>Here, \(n_{(w,d,k)}\) is the number of times that the word \(w \in V\) in document \(d\) is assigned to topic \(k\). Thus, \(n_{(\cdot,d,k) \neq (v,d,\cdot)}\) is the number of words in document \(d\) that are assigned to topic \(k\), excluding counts of the current word \(w_{i,d}=v\). Similarly, \(n_{(v,\cdot,k) \neq (v,d,\cdot)}\) is the number of times that word \(v\) is assigned to topic \(k\) in all documents, excluding the occurrences of \(w_{i,d}=v\) in document \(d\). Finally, \(\sum_w \beta + n_{(w,\cdot,k) \neq (v,d,\cdot)}\) is the total number of words in the vocabulary \(V\) that are assigned to topic \(k\), excluding the occurrences of \(w_{i,d}=v\) in document \(d\).</p> <p>After sampling the topic assignments \(z_{i,d}\) for each word \(w_{i,d}\) in document \(d\), we can update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values as follows:</p> \[\begin{align*} \theta_{d,k} &amp;= \frac{\alpha_k + n_{(\cdot,d,k)}}{\sum_{j=1}^K \alpha_j + n_{(\cdot,d,j)}} \\ \phi_{k,v} &amp;= \frac{\beta + n_{(v,\cdot,k)}}{\sum_{w=1}^V \beta_w + n_{(w,\cdot,k)}} \end{align*}\] <p>I do not want to claim falsehoods on my blog by deriving these equations myself, so I will refer you to this great <a href="https://miningthedetails.com/LDA_Inference_Book/lda-inference.html">resource</a> by Chris Tufts for the full derivation.</p> <h2 id="implementation">Implementation</h2> <p>Now onto the fun part! Let’s implement LDA from scratch in Python. I’ll be using a stripped down version of the MIMIC-III dataset<d-cite key="pmid27219127"></d-cite>, which is a collection of de-identified medical records from patients admitted to the intensive care unit (ICU). My version contains only the ICD9 codes<d-cite key="pmid27219127"></d-cite> of timestamped diagnoses for 689 patients with a vocabulary size of 389 unique codes. The dataset is available <a href="https://github.com/TheMatrixMaster/lda-model/tree/main/data">here</a></p> <p>First, we’ll load the dataset and process it into the desired input format for our model which is a list of lists of strings where each sublist represents a document (see patient) and each string represents a word (see ICD code) in the document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Format docs into the desired format list of lists
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/MIMIC3_DIAGNOSES_ICD_subset.csv.gz</span><span class="sh">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">SUBJECT_ID</span><span class="sh">'</span><span class="p">])</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">SUBJECT_ID</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="nb">list</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
</code></pre></div></div> <p>Next, I setup a class to handle manipulations with the latent conditional distribution \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\) which we derived above. This class is initialized with the initial parameters \(\alpha\) and \(\beta\) for the dirichlet priors, the number of topics \(K\), and the topic assignment counts matrix \(n_{(w,d,k)}\) which is a 3D tensor of shape \(V \times D \times K\) where \(V\) is the vocabulary size, \(D\) is the number of documents, and \(K\) is the number of topics.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LatentDistribution</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>   <span class="c1"># 1d array holding alpha hyperparams a_{k}
</span>    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span>         <span class="c1"># beta hyperparam
</span>    <span class="n">n_mat</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>   <span class="c1"># 3d array holding n_{k,d,w}
</span>    <span class="n">K</span><span class="p">:</span> <span class="nb">int</span>              <span class="c1"># number of topics
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span> <span class="o">=</span> <span class="n">n_mat</span>

        <span class="k">assert</span> <span class="n">n_mat</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">K</span>

        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">alpha</span>
</code></pre></div></div> <p>This class contains methods to sample the conditional distribution \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\) and to update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. The sampling method is implemented as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">alpha_dw</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">n_dk</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>
    <span class="n">n_wk</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,</span><span class="n">w</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>

    <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>
    
    <span class="nf">return </span><span class="p">(</span><span class="n">alpha_dw</span> <span class="o">+</span> <span class="n">n_dk</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">n_wk</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">n_k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pmf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">g_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">g_k</span><span class="o">/</span><span class="n">g_k</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    
<span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pmf</span><span class="p">).</span><span class="nf">argmax</span><span class="p">()</span>
</code></pre></div></div> <p>The update method is implemented as follows. We also have helper methods to get the current expected values of \(\theta\) and \(\phi\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_n</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">get_phi</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:].</span><span class="nf">sum</span><span class="p">()</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,</span><span class="n">w</span><span class="p">].</span><span class="nf">sum</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">n_k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_theta</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">n_d</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[:,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">()</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">n_d</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we are ready to implement the iterative Gibbs inference algorithm. In the class init, we initialize the topic assignment counts matrix \(n_{(w,d,k)}\) to zeros, and instantiate a <code class="language-plaintext highlighter-rouge">LatentDistribution</code> instance with the correct initial parameters. We also create a vocabulary object which maps each string word to a unique integer index.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LDA</span><span class="p">():</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of topics
</span>    <span class="n">d</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of documents
</span>    <span class="n">w</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of words in vocabulary
</span>    <span class="n">vocab</span><span class="p">:</span> <span class="n">defaultdict</span>                  <span class="c1"># vocabulary mapping words to indices
</span>    <span class="n">r_vocab</span><span class="p">:</span> <span class="n">defaultdict</span>                <span class="c1"># reverse vocabulary mapping indices to words
</span>    <span class="n">docs</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>                    <span class="c1"># 2d list holding documents with raw words
</span>    <span class="n">n_iter</span><span class="p">:</span> <span class="nb">int</span>                         <span class="c1"># number of iterations
</span>    <span class="n">latent_dist</span><span class="p">:</span> <span class="n">LatentDistribution</span>     <span class="c1"># latent distribution Z
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">setup_vocab</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">r_vocab</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">reversed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span> <span class="o">=</span> <span class="nc">LatentDistribution</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_vocab</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">):</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
                    <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vocab</span>
</code></pre></div></div> <p>In the fitting method, as explained above, we iterate over each word in each document and sample a new topic assignment \(z_{i,d}\) for that word from the conditional distribution. We then update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. We repeat this process for the specified number of iterations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_iter</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">docs</span><span class="p">[</span><span class="n">d</span><span class="p">])):</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">docs</span><span class="p">[</span><span class="n">d</span><span class="p">][</span><span class="n">n</span><span class="p">]]</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">update_n</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>Now that we have implemented the LDA model, let’s see what topics it discovers in the MIMIC-III dataset. Given the final topic proportions \(\theta\) and word proportions \(\phi\), we can get easily get the top words for each topic and the top documents for each topic by sorting the rows and columns of \(\phi\) and \(\theta\) respectively. We can also correlate the topics with certain sets of keywords to see if the learned topics take on the meaning that we expect. In my dataset, I correlate the topics with 3 key ICD9 categories: Alzheimer’s disease, Parkinson disease, and Multiple Sclerosis. The results are linked below:</p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/top_words.png">Top words for each topic</a></p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/top_docs.png">Top 100 documents for each topic</a></p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/word_topic_corr.png">Correlation of topics with key ICD9 categories</a></p> <p>For the full-code and more results, check out my <a href="https://github.com/TheMatrixMaster/lda-model/tree/main">GitHub repo</a></p> <h2 id="conclusion">Conclusion</h2> <p>In the next post on topic models, I will tackle the older brother of LDA, the embedded topic model (ETM)<d-cite key="dieng2019etm"></d-cite> which is a neural network based topic model that learns the topic proportions \(\theta\) and word proportions \(\phi\) in an end-to-end fashion in a shared word-topic latent space. Stay tuned!</p>]]></content><author><name>Stephen Lu</name></author><category term="bayesian"/><category term="topic-modeling"/><category term="latent-dirichlet-allocation"/><category term="lda"/><category term="gibbs-sampling"/><summary type="html"><![CDATA[Motivation If you’ve ever played around with unsupervised clustering algorithms like k-means, then the concept of topic modeling should already be familiar to you. Informally, topic modelling can be thought of as discovering the underlying “topics” or “themes” that are present in a collection of documents. For example, if we were to apply topic modeling to a collection of news articles, we might expect to find topics like “politics”, “sports”, “entertainment”, etc.]]></summary></entry><entry><title type="html">Degenerate Dot Product Attention</title><link href="https://thematrixmaster.github.io/blog/2023/degenerate-attention/" rel="alternate" type="text/html" title="Degenerate Dot Product Attention"/><published>2023-11-19T00:00:00+00:00</published><updated>2023-11-19T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/degenerate-attention</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/degenerate-attention/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>In a deep learning course that I’m taking this semester, we were recently asked to implement multi-headed dot product attention in a causal gpt-like transformer. I ran into an interesting pitfall that I wanted to share, so that others can avoid making the same mistake, while also digging deeper into the attention mechanism. If you want to skip the background and jump straight into the explanation, click <a href="#degenerate-attention">here</a>.</p> <h2 id="background">Background</h2> <p>For a more comprehensive review of the transformer architecture and dot-product attention, please refer to this great resource written by Jay Alammar: <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a>. If you are familiar with this material, feel free to skip this section.</p> <p>Here is a quick refresher on the attention mechanism:</p> <p>Given a sequence of token embeddings \(h \in \mathbb{R}^{L \times d_h}\), we first compute the query, key, and value matrices \(Q, K, V \in \mathbb{R}^{L \times d_k}\) by multiplying the embeddings by learned weight matrices \(W_Q, W_K, W_V \in \mathbb{R}^{d_h \times d_k}\):</p> \[Q = hW_Q, \quad K = hW_K, \quad V = hW_V\] <p>For this analysis, I will assume that we use a single head of attention, so \(d_k = d_h\). Next, the raw attention scores \(A \in \mathbb{R}^{L \times L}\) is computed as follows:</p> \[A_{\text{raw}} = \frac{QK^T}{\sqrt{d_k}}\] <p>\(A\) is an \(L \times L\) matrix where the row dimension corresponds to the queries and the column dimension corresponds to the keys. Thus, we can think of the entry at \((i,j)\) as the amount of attention that the \(i^{th}\) token should pay to the \(j^{th}\) token, when attempting to predict the next token in the sequence. Obviously, we don’t want the model to cheat by looking at future tokens, so we apply a lower triangular mask \(M\) to \(A\) that zeroes out the entries above the diagonal.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/self-attention.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/self-attention.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/self-attention.svg-1400.webp"/> <img src="/assets/img/blog/2023-11-19-degenerate-attention/self-attention.svg" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Attention matrix before and after applying the causal mask and row-wise softmax </div> <p>Finally, we apply a row-wise softmax to normalize the attention scores for each query. This softmax operation is performed on the rows (query) dimension, since we want to normalize the attention scores of each query \(i\) across the keys \(j&lt;i\) that come before it. This is the key insight that enables the transformer to be causal, since the model can only attend to tokens that come before it in the sequence. We obtain the next token embedding \(h_{i+1}\) by multiplying the softmaxed attention scores \(A_{\text{final}}\) by the value matrix \(V\).</p> \[\begin{split} A_{\text{final}} &amp;= \text{softmax}(A_{\text{raw}}) \\ h_{i+1} &amp;= A_{\text{final}}V \end{split}\] <h2 id="degenerate-attention">Degenerate Attention</h2> <p>Now that we have reviewed the attention mechanism, let’s look at the problem that I ran into. Instead of performing softmax over the query (row) dimension of the \(L \times L\) self-attention matrix, I instead performed softmax over the key (column) dimension. This is a subtle mistake that is easy to miss, since the softmax operation is symmetric and the row and column dimensions are interchangeable.</p> <p>After training my model for a few epochs on the wikitext-2 dataset, I noticed that my model was achieving state of the art perplexity on the validation set, and showed no signs of overfitting. Here are some plots of the training and validation perplexity over time. I knew that something was wrong, since the perplexity was too good to be true, so I decided to investigate further. I named this phenomenon <strong>degenerate attention</strong>, and ran some comparisons with the correct implementation of softmax over the row dimension.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/train_ppl_degen-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/train_ppl_degen-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/train_ppl_degen-1400.webp"/> <img src="/assets/img/blog/2023-11-19-degenerate-attention/train_ppl_degen.png" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/val_ppl_degen-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/val_ppl_degen-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-11-19-degenerate-attention/val_ppl_degen-1400.webp"/> <img src="/assets/img/blog/2023-11-19-degenerate-attention/val_ppl_degen.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Initially, I was extremely perplexed since the attention matrix in degenerate attention was still lower triangular, which prevented tokens from directly attending to future tokens. So, to elucidate the situation, I decided to generate some text using the degenerate transformer, and visualize the attention scores between layers using the <a href="https://github.com/jessevig/bertviz">bertviz</a> package.</p> <p>As expected, text generated from the degenerate transformer was complete gibberish, since the model was able to cheat by attending to future tokens. Here is an example passage:</p> <blockquote> <p>”,,,,,,,,,, in and in, in’from, from some in available available available playing some focused close in added interested re added some far re Al some self individual focused returning available far some some added Al re re some far serious re re half re construction re Cor self re forced re ill Rox beginning eight Villa necessary air Cal Secretary fast far re increased far re far Abu operating Villa re scored Less some re re re free re re concrete international re concrete some re re”</p> </blockquote> <p>However, I realized that the softmax operation was still allowing information to leak from future tokens to past tokens. This is because the softmax operation is performed on the column dimension, so the attention scores of each key \(j\) are normalized across the queries \(i&gt;j\) that come after it. This is a subtle but important difference,</p> <p>Further, when we look at the top-k output logits from the classifier head, we see that the degenerate transformer gives extremely high probabilities to the correct token, but the next highest probability tokens are incoherent with respect to the sentence. Here is an example:</p> <table> <tbody> <tr> <td><strong>Original sentence</strong></td> <td>“I went to the cinema where I saw a movie”</td> </tr> <tr> <td><strong>Top 1 prediction</strong></td> <td>“I went to the cinema where I saw a movie”</td> </tr> <tr> <td><strong>Top 2 prediction</strong></td> <td>“I resigned by a trust when appeared for the Year”</td> </tr> <tr> <td><strong>Top 3 prediction</strong></td> <td>“I Squadron of The Argentina after Y drew since to”</td> </tr> </tbody> </table> <p>Finally, here are the attention score visualizations obtained via bertviz. As expected, the model attends much more to recent tokens given that the softmax forces each column of the attention matrix to sum to 1.</p> <div style="margin: 20px auto;"> <iframe src="/assets/img/blog/2023-11-19-degenerate-attention/head_view.html" frameborder="0" scrolling="no" height="300px" width="auto" style="border: 1px dashed grey; background-color: white;"></iframe> </div> <h2 id="explanation">Explanation</h2> <p>These were all clear signs that there was data leakage allowing the degenerate model to see future tokens, which was making the next word prediction task trivial. However, I was still confused as to why the model was able to achieve such good performance. After some thought, I finally figured out where the issue was. The key is that although the self-attention matrix remains lower triangular, softmaxing over the column space (queries dimension) introduces a source of correlation between the independent rows that the model can exploit. By tuning the raw attention scores, the model “hacks” the softmax operation by using it to pass information from future tokens to past tokens. Let’s look at this two step process in more detail:</p> <ol> <li>Each token \(i\) in the sequence computes the pre-softmax attention values over the previous tokens \(j\leq i\) in the input sequence.</li> <li>Degenerate attention normalizes the attention scores column-wise, across each query token, which introduces a source of correlation among the attention scores of each row (key)</li> </ol> <p>Thus, the model is smart enough to figure out a way to tune the pre-softmax attention values in step 1 so that information is then passed from future tokens to past tokens in step 2 of the column-wise softmax. Compare this to normal attention where there is no way for the rows in the attention matrix to influence each other since the softmax is performed on the row dimension.</p> <h2 id="conclusion">Conclusion</h2> <p>Although this blog post was a bit of a rant, I hope that it was helpful for those who are trying to understand the attention mechanism in transformers. I also hope that it will help others avoid making the same mistake that I did, while provide an important reminder that neural networks will always take the path of least resistance. Finally, I’d like to thank my dear friend <a href="https://superkaiba.github.io">Thomas Jiralerspong</a> for helping me debug this issue and providing feedback on this blog post.</p> <p>The code for this project can be found <a href="https://github.com/TheMatrixMaster/degenerate-attention">here</a>, if you want to try degenerate attention yourself for some reason…</p>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="transformers"/><category term="attention"/><category term="nlp"/><category term="pitfall"/><summary type="html"><![CDATA[Motivation In a deep learning course that I’m taking this semester, we were recently asked to implement multi-headed dot product attention in a causal gpt-like transformer. I ran into an interesting pitfall that I wanted to share, so that others can avoid making the same mistake, while also digging deeper into the attention mechanism. If you want to skip the background and jump straight into the explanation, click here.]]></summary></entry><entry><title type="html">Variational Inference w/ EM algorithm (Part 2)</title><link href="https://thematrixmaster.github.io/blog/2023/variational-inf-2/" rel="alternate" type="text/html" title="Variational Inference w/ EM algorithm (Part 2)"/><published>2023-10-28T00:00:00+00:00</published><updated>2023-10-28T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/variational-inf-2</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/variational-inf-2/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>In the last post of this two-part series, I introduced the theory behind mean-field variational inference. In this post, I’ll walk through my implementation of mean-field VI on the task of polygenic risk score (PRS) regression with spike-and-slab prior.</p> <h3 id="what-is-prs">What is PRS?</h3> <p>To understand polygenic risk score prediction, we first have to introduce the concept of genome-wide association studies (GWAS). In a GWAS, we are given a dataset of \(N\) individuals, each with a set of \(M\) genetic variants (SNPs) and a binary phenotype (e.g. disease status). The goal of GWAS is to identify which SNPs are associated with the phenotype, and to quantify the strength of these associations. In other words, we want to find the SNPs that are statistically significant, and to estimate the effect size \(\beta_i\) of each SNP \(i\) on the phenotype \(y\).</p> <p>Typically, we use a linear model to fit the data, where the phenotype \(y\) is a linear combination of the SNPs \(x\), with some noise \(\epsilon\):</p> \[y = \beta_0 + \sum_{i=1}^M \beta_i x_i + \epsilon\] <p>We can then use \(\beta_i\) to quantify the strength of the association between SNP \(i\) and the phenotype \(y\). After doing some statistical tests on \(\beta_i\), we typically end up with a list of SNPs that are statistically significant and we can obtain nice Manhattan plots like this:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/manhattan.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Manhattan plot of a GWAS study investigating kidney stone disease, so the peaks indicate genetic variants that are found more often in individuals with kidney stones (Source: Howles et a. 2019) </div> <p>Naturally, a good follow-up question is: how well can we predict the phenotype \(y\) given the SNPs \(x\)? This is where polygenic risk score (PRS) prediction comes in. The idea is to use the estimated effect sizes \(\beta_i\) to compute a weighted sum of the SNPs \(x\), which we call the polygenic risk score \(s\). These scores can inform us about the genetic risk of an individual developing a disease, and can be used to predict the phenotype \(y\).</p> <p>However, there are a few issues with this approach. First, the effect sizes \(\beta_i\) are estimated from a linear model, which assumes that the phenotype \(y\) is a linear combination of the SNPs \(x\). This is not always the case, since the phenotype \(y\) is often a non-linear function of the SNPs \(x\), like in epistasis. Second, the linear model assumes independence between the SNPs \(x\), the phenotype \(y\), and the noise \(\epsilon\), which is not necessarily true. These assumptions can lead to poor predictive performance of the PRS.</p> <p>Given that the effect sizes \(\beta\) predicted by the linear model are noisy as explained above, we can use Bayesian inference to inject our prior domain knowledge into the model. If the prior is meaningful, then the new posterior estimate of \(\beta\) should be more accurate than the original linear estimates, leading to better predictive performance of the PRS.</p> <p>In this post, we will use mean-field variational inference to estimate the posterior distribution of \(\beta\) with a spike-and-slab prior.</p> <h3 id="spike-and-slab-prior">Spike-and-slab prior</h3> <p>Intuitively, given a specific phenotype \(y\), only a very small subset of all the ~20000 genes will be causally related to the it. This means that the true effect sizes of most genes should be zero, and only a handful of genes should have non-zero effect sizes. The spike and slab prior allows us to model this belief by assuming that the effect sizes \(\beta_i\) are either zero or drawn from a normal distribution with zero mean and variance \(\sigma_{\beta}^2\). The probability of a non-zero effect size is given by the spike probability \(\pi\), which is the proportion hyperparameter of a Bernouilli.</p> \[\begin{equation} \label{eq:prior} \begin{split} s_i &amp; \sim \text{Bernouilli}(\pi) \\ \beta_i &amp; |s_i=1 \sim \mathcal{N}(0, \sigma_{\beta}^2) \\ p(\beta_i, s_i) &amp; = \mathcal{N}(\beta_i \vert 0, \sigma_{\beta}^2)\pi^{s_i}(1-\pi)^{1-s_i} \\\\ \end{split} \end{equation}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bernoulli</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="o">**</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">expectation</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span>
    
    <span class="k">def</span> <span class="nf">expectation_log</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_p</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
</code></pre></div></div> <h3 id="likehood">Likehood</h3> <p>The likelihood simply models the probability of the phenotype \(y\) given the SNPs \(x\), the effect sizes \(\beta\), and the boolean causal indicators \(s\) as a normal distribution centered at the model output with environmental variance \(\sigma_{\epsilon}^2\), that is not captured by the model.</p> \[\begin{split} p(y\vert X,\beta, s) = \mathcal{N}(X (s \circ \beta), \sigma_{\epsilon}^2) \\\\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">expectation</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">mu</span>
    
    <span class="k">def</span> <span class="nf">update_mu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">update_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
</code></pre></div></div> <h3 id="variational-distribution">Variational distribution</h3> <p>Now, we can derive the variational distribution \(q\) that we will use to approximate the posterior distribution \(p\). Recall from <a href="/blog/2023/variational-inf-1/">part 1</a>, that to get \(q\), we need to factorize the ELBO objective into a product of independent causal variables \(z_i\) using the mean-field assumption, and then maximize the ELBO with respect to each \(z_i\) to obtain closed form expression for \(q\).</p> <p>Recall that under the mean-field assumption, we can factorize the ELBO as follows:</p> \[\begin{equation} \label{eq:elbo} \begin{split} \text{ELBO}(q\vert\phi) &amp; = \sum_j \int q(z_j)\log \frac{\text{exp}(\mathbb{E}_{q(z_i)}[\log p(X,z)]_{i\neq j})}{q(z_j)}dz_j - \sum_{i\neq j}\int q(z_i)\log q(z_i)dz_i \\ &amp; = -\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] + \mathbb{H}(z_{i\neq j}) + C \\ \end{split} \end{equation}\] <p>Further, recall that this expression is maximized only if \(-\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] = 0\) for all \(j\), which occurs when \(\log q(z_j\vert\phi) = \log \tilde p_{i\neq j}\). We use this constraint to derive closed form for the variational distribution \(q(\beta,s\vert y,X) = \prod_{j} q(\beta_j\vert s_j)q(s_j)\).</p> <p>First, let’s derive \(q^*(\beta_j\vert s_j)\) by completing the square:</p> \[\begin{equation} \begin{split} \log q^*(\beta_j\vert s_j=1) &amp; = \log \tilde p_{i\neq j} \\ &amp; = \frac{1}{C'}\mathbb{E}_{q(\beta_i, s_i)}[\log p(y\vert X,\beta,s)+\log p(\beta_j)]_{i\neq j} \\ &amp; \propto \mathbb{E}_{q(\beta_i, s_i)}\Biggr[-\frac{1}{2}\Biggl\{(\tau_{\epsilon} x^T_jx_j+\tau_\beta)\beta_j^2 - 2\tau_\epsilon(y-\hat{y}_i)^Tx_j\beta_j\Biggl\}\Biggr]_{i\neq j} \\ &amp; \sim \mathcal{N}(\mu^*_{\beta_j}, \frac{1}{\tau^*_{\beta_j}}) \\ &amp; \text{where} \\ &amp; \tau^*_{\beta_j} = \tau_\epsilon x^T_jx_j+\tau_\beta \\ &amp; \mu^*_{\beta_j} = \frac{\tau_\epsilon}{\tau^*_{\beta_j}}\mathbb{E}_{q(\beta_i,s_i)}[(y-\hat{y_i})^Tx_j]_{i\neq j} = N\frac{\tau_\epsilon}{\tau^*_{\beta_j}}\Biggl(\frac{y^Tx_j}{N}-\sum_{i\neq j}\gamma^*_i \mu^*_{\beta_i} r_{ij} \Biggl) \\ \end{split} \end{equation}\] <p>Now, let’s derive \(q^*(s_j)\):</p> \[\begin{split} \log q(s_j=1) &amp; = \log \tilde p_{i\neq j} \\ &amp; = \frac{1}{C'}\mathbb{E}_{q(s_i)}[\log p(y\vert \hat{y_i},s_j=1)+\log p(s_j=1)]_{i\neq j} \\ &amp; \propto \frac{N}{2}\log\tau_{\epsilon}-C+\frac{1}{2}\log\tau_{\beta}-\frac{1}{2}\tau^*_{\beta_j}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2+\log{\pi} \\\\ \log q(s_j=0) &amp; \propto \frac{N}{2}\log\tau_{\epsilon}-C+\log{1-\pi} \\\\ q^*(s_j) &amp; = \frac{\text{exp}(\log q(s_j=1))}{\text{exp}(\log q(s_j=0)) + \text{exp}(\log q(s_j=1))} \\ &amp; = \frac{1}{1+\text{exp}(-u_j)} = \gamma^*_j \\ &amp; \text{where}\;\; u_j = \log\frac{\pi}{1-\pi} + \frac{1}{2}\log\frac{\tau_\beta}{\tau^*_{\beta_j}}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2 \\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Variational</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Normal</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">m</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">precision</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span> <span class="o">=</span> <span class="p">[(</span><span class="nc">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">precision</span><span class="p">),</span> <span class="nc">Bernoulli</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">pdf</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">pdf</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">update_mu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">update_mu</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">precision</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">update_sigma</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">precision</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">update_p</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_mu</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">mu</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">get_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">sigma</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">get_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">p</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
</code></pre></div></div> <p>Now that we have found closed form expressions for \(q^*(\beta_j\vert s_j)\) and \(q^*(s_j)\), we can use them to compute the ELBO objective and maximize it with respect to the hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). However, we will soon realize that we cannot directly solve this optimization problem since there is a circular dependency between the hyperparameters \(\phi\) and the variational distribution parameters \(\tau^*_{\beta_j}, \mu^*_{\beta_j}, \gamma^*_j\). In other words, we need to know the optimal hyperparameters to compute the optimal variational distribution, but we also need to know the optimal variational distribution to compute the optimal hyperparameters. This is where the EM algorithm comes in.</p> <h3 id="e-step">E-step</h3> <p>In the E-step of the EM algorithm, we simply update the variational distribution parameters \(\tau^*_{\beta_j}, \mu^*_{\beta_j}, \gamma^*_j\) using the current fixed hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). This is done by evaluating the closed form expressions we derived above, i.e.:</p> \[\begin{equation} \label{eq:estep} \begin{split} &amp; \tau^*_{\beta_j} = \tau_\epsilon x^T_jx_j+\tau_\beta \\ &amp; \mu^*_{\beta_j} = N\frac{\tau_\epsilon}{\tau^*_{\beta_j}}\Biggl(\frac{y^Tx_j}{N}-\sum_{i\neq j}\gamma^*_i \mu^*_{\beta_i} r_{ij} \Biggl) \\ &amp; \gamma^*_j = \frac{1}{1+\text{exp}(-u_j)},\;\; u_j = \log\frac{\pi}{1-\pi} + \frac{1}{2}\log\frac{\tau_\beta}{\tau^*_{\beta_j}}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2 \\\\ &amp; q(\beta_j\vert s_j=1) = \mathcal{N}(\mu^*_{\beta_j}, \frac{1}{\tau^*_{\beta_j}})\qquad q(s_j=1) = \gamma^*_j \end{split} \end{equation}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">E_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Update the latent distribution parameters using the other latents parameters,  
    the hyperparameters and the observed data.
    </span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="n">new_precision</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span>

        <span class="n">new_mu</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">/</span><span class="n">new_precision</span> <span class="o">*</span>  \
            <span class="p">(</span><span class="n">mbeta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span>    \
                <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">]]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">)))</span>

        <span class="n">new_uj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]))</span> \
            <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="n">new_precision</span><span class="p">)</span> \
            <span class="o">+</span> <span class="n">new_precision</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">new_mu</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">new_gamma</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">new_uj</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_mu</span><span class="p">(</span><span class="n">new_mu</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_sigma</span><span class="p">(</span><span class="n">new_precision</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_gamma</span><span class="p">(</span><span class="n">new_gamma</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>

    <span class="c1"># After a full cycle of updates, we cap gamma to avoid numerical instability
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_gamma</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="n">j</span><span class="p">)</span>
</code></pre></div></div> <h3 id="m-step">M-step</h3> <p>In the M-step, we use the updated variational distribution to maximize the ELBO objective with respect to the hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). This is done by setting the gradient of the ELBO objective with respect to each hyperparameter to zero, and solving for the optimal hyperparameters.</p> \[\begin{split} &amp; \frac{\partial \, \text{ELBO}}{\partial \tau_\epsilon} = 0 \iff \tau_\epsilon^{-1} = \mathbb{E}_q[\frac{1}{N}(y-X(s\circ\beta)^T(y-X(s\circ\beta)))] \\\\ &amp; \frac{\partial \, \text{ELBO}}{\partial \tau_\beta} = 0 \iff \tau_\beta^{-1} = \sum_j \gamma^*_j({\mu^*_j}^2+{\tau^*_{\beta_j}}^-1)/\sum_j\gamma^*_j \\\\ &amp; \frac{\partial \, \text{ELBO}}{\partial \pi} = 0 \iff \pi = \frac{1}{M}\sum_j \gamma^*_j \\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">M_step</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Update the hyperparameters using the current latent parameter estimates and the data.
    In this tutorial, we don</span><span class="sh">'</span><span class="s">t update the tau_epsilon hyperparameter for simplicity.
    </span><span class="sh">"""</span>
    <span class="n">new_tau_epsilon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span>

    <span class="n">new_tau_beta_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span>  \
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_sigma</span><span class="p">()))</span>   \
            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>
    
    <span class="n">new_pi</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>

    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_tau_epsilon</span>
    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">new_tau_beta_inv</span>
    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_pi</span>
</code></pre></div></div> <h3 id="algorithm">Algorithm</h3> <p>Now that we have derived the E and M steps, we can simply alternate between them until the ELBO objective converges to a maximum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_elbo</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Compute the evidence lower bound (ELBO) of the model by using the current variational 
    distribution and the joint likelihood of the data and the latent variables. These 
    distributions are parameterized by our current estimates of hyperparameter values and 
    latent distribution parameters.
    </span><span class="sh">"""</span>
    <span class="n">exp_var_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation_log</span><span class="p">()</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">])</span>
    <span class="n">exp_var_beta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>

    <span class="n">summand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> \
                              <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_sigma</span><span class="p">())</span>
    
    <span class="n">exp_true_beta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">summand</span><span class="p">)</span>
    <span class="n">exp_true_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">])</span>   \
                        <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]))</span>
    
    <span class="n">double_summand</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">gamma_j</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">mu_j</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">gamma_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">mu_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">double_summand</span> <span class="o">+=</span> <span class="n">gamma_j</span><span class="o">*</span><span class="n">mu_j</span><span class="o">*</span><span class="n">gamma_k</span><span class="o">*</span><span class="n">mu_k</span><span class="o">*</span><span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
    
    <span class="n">exp_true_y</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">])</span>  \
        <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">n</span> \
        <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">())</span><span class="o">@</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">mbeta</span><span class="p">)</span>    \
        <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">summand</span><span class="o">*</span><span class="n">ld</span><span class="p">.</span><span class="nf">diagonal</span><span class="p">())</span>   \
        <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">double_summand</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">exp_true_y</span> <span class="o">+</span> <span class="n">exp_true_beta</span> <span class="o">+</span> <span class="n">exp_true_s</span> <span class="o">-</span> <span class="n">exp_var_beta</span> <span class="o">-</span> <span class="n">exp_var_s</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Run the EM algorithm for a given number of iterations or until convergence.
    </span><span class="sh">"""</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nc">E_step</span><span class="p">(</span><span class="n">mbeta</span><span class="p">,</span> <span class="n">ld</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nc">M_step</span><span class="p">()</span>
        <span class="n">elbo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_elbo</span><span class="p">(</span><span class="n">mbeta</span><span class="p">,</span> <span class="n">ld</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nf">abs</span><span class="p">(</span><span class="n">elbo</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">elbo</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">elbo</span>
</code></pre></div></div> <h3 id="results">Results</h3> <p>Here are some results I obtained from a simulated dataset with M=100 SNPs given the marginal effect sizes \(\beta\) and the ld matrix \(R\). First let’s take a look at the training curve of the ELBO objective. We can see that the ELBO converges to a maximum after 5 iterations.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/em_elbo.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 1. ELBO loss as a function of interation number of EM algorithm </div> <p>Next, we can take a look at the linear model’s predictions of the phenotype \(y\) given the baseline marginal effect sizes \(\beta\), and compare them to the predictions of the variational model with the new posterior effect sizes \(\beta^{new}\). We can see that the variational model is able to better capture the relationship between the SNPs and the phenotype, which is expected since the variational model is able to incorporate our prior domain knowledge about the SNPs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/train_preds.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/test_preds.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Predictions of the phenotype y given the SNPs x using the variational model (left) and the marginal model (right) </div> <p>Finally, let’s take a look at posterior inclusion probability of each SNP, which is the probability that the SNP has a non-zero effect size. We can see that the variational distribution \(q(s)\) is able to correctly identify the SNPs with non-zero effect sizes.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-1400.webp"/> <img src="/assets/img/blog/2023-10-15-variational-inf/snp_pips.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 3. Posterior inclusion probability of each SNP. Causal SNPs are highlighted in red. </div> <p>The complete code for this project can be found <a href="https://github.com/TheMatrixMaster/variational-inference">here</a>.</p>]]></content><author><name>Stephen Lu</name></author><category term="bayesian"/><category term="variational"/><category term="inference"/><category term="statistics"/><category term="gwas"/><category term="prs"/><summary type="html"><![CDATA[Motivation In the last post of this two-part series, I introduced the theory behind mean-field variational inference. In this post, I’ll walk through my implementation of mean-field VI on the task of polygenic risk score (PRS) regression with spike-and-slab prior.]]></summary></entry><entry><title type="html">Variational Inference w/ EM algorithm (Part 1)</title><link href="https://thematrixmaster.github.io/blog/2023/variational-inf-1/" rel="alternate" type="text/html" title="Variational Inference w/ EM algorithm (Part 1)"/><published>2023-10-09T00:00:00+00:00</published><updated>2023-10-09T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/variational-inf-1</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/variational-inf-1/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Given an underlying distribution \(p_r\) with unknown parameters \(\theta\), the goal of Bayesian inference is to learn a posterior distribution over \(\theta\) given a dataset \(X\) whose examples are sampled independently from \(p_r\). Given that \(p_r\) might be a very complex distribution over many random variables, one trick to simplify the task involves introducing latent variables \(z\) that break down the overall inference problem into smaller subproblems (like a <strong>divide-and-conquer</strong> approach). Given these additional latents, our objective is to infer the joint posterior distribution over the unknown parameters \(\theta\) and latents \(z\) given an observed dataset \(X\) according to Bayes’ rule.</p> \[\begin{equation} \label{eq:bayes} p(\theta,z|X) = \frac{p(X\vert\theta,z)p(\theta|z)p(z)}{p(X)} \end{equation}\] <p>Although the latents simplify the problem, this posterior remains intractable given that we need to marginalize over all parameters and latents to compute the evidence \(p(X)=\int{\int{p(X,\theta,z)dz\,}d\theta}\).</p> <p>To address this, one can attempt to approximate the evidence using stochastic sampling (Monte Carlo methods), but this optimization procedure is not interpretable, is compute-intensive, and requires many samples for convergence. Another approach, known as mean-field variational inference (VI), allows us to completely bypass the evidence computation issue by making a few additional assumptions about \(\theta\) and \(z\).</p> <p>In this (part 1) post, I will go over the theory behind this method, and in the next post (part 2), I’ll walk through my implementation of mean-field VI on the task of polygenic risk score (PRS) regression with spike-and-slab prior.</p> <h3 id="variational-distribution-by-mean-field">Variational Distribution by Mean-Field</h3> <p>Given that the evidence term is intractable, variational inference proposes that we learn a simpler distribution over the unknown parameters and latents \(q(\theta,z)\) that approximates the true posterior \(p(\theta,z|X)\). Most importantly, the main idea behind mean-field VI is that we can strategically restrict \(q(\theta,z)\) to a simpler distribution family than \(p(\theta,z\vert X)\), while optimizing the parameters of \(q\) to obtain a good approximation of \(p\). The only assumption we need for this to work is the mean-field approximation, i.e. conditional independence among some partition of the latents \(z\) into \(z_1, ..., z_M\) such that</p> \[\begin{equation} \label{eq:assumption} q(\theta, z \vert X) = \prod_i q(\theta \vert z_i)q(z_i) \end{equation}\] <p>In the variational inference lingo, we call \(q\) the variational distribution.</p> <h3 id="evidence-lower-bound-elbo">Evidence Lower Bound (ELBO)</h3> <p>Now that we’ve defined a factorization of \(q\) into a partition of latent variables, we need to actually find an explicit distribution family for \(q(\theta \vert z_i)\) and \(q(z_i)\) that has enough capacity to adequately approximate \(p\). Intuitively, a good variational distribution \(q\), parametrized by \(\phi\), should minimize the KL divergence between itself and the target posterior \(p\). Let’s take a closer look at this KL divergence expression:</p> \[\begin{equation} \label{eq:elbo} \begin{split} \mathbb{KL}(q\,\|\, p) &amp; = \int{\int{q(\theta,z) \log\frac{q(\theta,z)}{p(\theta,z)} dz}\, d\theta} \\ &amp; = \int{\int{q(\theta,z) \log\frac{q(\theta,z)p(X)}{p(X,\theta,z)} dz\,}d\theta} \\ &amp; = \int{\int{q(\theta,z) [\log\frac{q(\theta,z)}{p(X,\theta,z)} + \log{p(X)}] dz}\, d\theta} \\ &amp; = \int{\int{q(\theta,z) \log\frac{q(\theta,z)}{p(X,\theta,z)} dz}\, d\theta} + \log{p(X)}\int{\int{q(\theta,z)dz}\, d\theta} \\ &amp; = -\text{ELBO}(q,\phi) + \log{p(X)} \end{split} \end{equation}\] <p>Given that \(p(X)\) is a constant, it is easy to see that minimizing \(\mathbb{KL}(q\,\|\, p)\) is <strong>equivalent</strong> to maximizing \(\text{ELBO}(q,\phi)\). Further, since KL divergence is a non-negative term, the following inequality holds \(\text{ELBO}(q,\phi) \leq \log(p(X))\) — hence the name <strong>evidence lower-bound</strong>.</p> <h3 id="variational-expectation-maximization">Variational Expectation Maximization</h3> <p>So, now that we’ve shown that maximizing \(\text{ELBO}(q,\phi)\) with respect to \(\phi\) yields a good variational distribution \(q(\theta,z)\), the next step is to derive a closed form distribution family for \(q(\theta,z\vert\phi)\) that we can feed into the ELBO maximization scheme with respect to \(\phi\). To do this, let’s examine the ELBO expression in more detail, while using the mean-field assumption from \(\eqref{eq:assumption}\). For simplicity, from now on, I’ll omit explicitly dealing with the \(\theta\) parameter by including it in the partition of latents \(z_1, ..., z_M, \theta\) as an additional partitioned independent variable.<d-cite key="choy2017"></d-cite></p> \[\begin{equation} \label{eq:varelbo} \begin{split} \text{ELBO}(q,\phi) &amp; = \int{q(z\vert\phi)\log{\frac{p(X,z)}{q(z\vert\phi)}}dz} \\ &amp; = \int \prod_i q(z_i\vert\phi) \log p(X,z) dz - \sum_i \int q(z_i\vert\phi) \log q(z_i\vert\phi) dz_i \\ &amp; = \sum_j \int q(z_j) (\int \prod_{i\neq j}q(z_i)\log p(X,z) \prod_{i\neq j}dz_i\, dz_j) \\ &amp; \qquad - \int q(z_j)\log q(z_j)dz_j - \sum_{i\neq j}\int q(z_i)\log q(z_i)dz_i \\ &amp; = \sum_j \int q(z_j)\log \frac{\text{exp}(\mathbb{E}_{q(z_i)}[\log p(X,z)]_{i\neq j})}{q(z_j)}dz_j - \sum_{i\neq j}\int q(z_i)\log q(z_i)dz_i \\ &amp; = -\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] + \mathbb{H}(z_{i\neq j}) + C \end{split} \end{equation}\] <p>Here, we see that the mean field assumption of independence between latents $z$ allows us to actually factorize the ELBO term into the sum over a function of each latent variable \(z_j \in \{z_1, ..., z_M, \theta\}\). Given that the entropy term in the above equation \(\mathbb{H}(z_{i\neq j})\) is fixed for a specific choice of \(\phi\), we conclude that the overall ELBO is maximized when the negative KL divergence term equals zero, i.e. \(-\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] = 0\) for all latents \(j\), which only occurs when</p> \[\begin{equation} \label{eq:objective} \begin{split} \log q(z_j\vert\phi) &amp; = \log \tilde p_{i\neq j} \\ &amp; = \mathbb{E}_{q(z_i\vert\phi)}[\log p(X,z)]_{i\neq j} + C \\ &amp; = \frac{\mathbb{E}_{q(z_i\vert\phi)}[\log p(X,z)]_{i\neq j}}{\int \mathbb{E}_{q(z_i\vert\phi)}[\log p(X,z)]_{i\neq j}\, dz_j} \\ &amp; = \frac{1}{C'}\mathbb{E}_{q(z_i\vert\phi)}[\log p(X,z)]_{i\neq j} \end{split} \end{equation}\] <p>It is important to understand that if \(\mathbb{E}_{q(z_i)}[\log p(X,z)]_{i\neq j}\) is tractable, then \(C'\) is also tractable given that we have a finite set of latent variables with size \(M\). And indeed, this expression is usually efficient to compute for a <strong>specific choice</strong> of \(\phi\) if we choose known distributions for the joint-likelihood \(p(X,z) = p(X\vert z)p(z)\). The keyword here is <strong>specific</strong> since it remains intractable to directly solve the above equation with respect to \(\phi\) for most complex (and interesting) models of \(p(X,z)\).</p> <p>Thus, an approach that naturally comes to mind then is to iteratively update \(\phi\) using gradient ascent on the ELBO objective. This approach is known as expectation maximization, or more specifically in this case as coordinate ascent in mean-field variational inference. This algorithm is very simple and comprises two steps that we iteratively apply until the ELBO objective converges to a maximum.<d-cite key="li2023"></d-cite></p> <h4 id="initialization">Initialization</h4> <p>Before we run the EM algorithm, we need to choose some initial value for the parameters \(\phi\), such that we may actually start the gradient ascent somewhere.</p> <h4 id="expectation-e-step">Expectation (E-Step)</h4> <p>In the E-step, we simply <em>update</em> the probability distribution of our variational distribution by evaluating \(\eqref{eq:objective}\) with the current version of \(\phi\) as follows:</p> \[\begin{equation} \label{eq:e-step} \begin{split} &amp; \log q(z_j)' = \frac{1}{C'}\mathbb{E}_{q(z_i\vert\phi)}[\log p(X,z\vert \phi)]_{i\neq j} \\ &amp; q(z)' = \prod_{i} q(z_j)' \end{split} \end{equation}\] <p>This update is performed for all latents \(j \in \{z_1,...,z_M, \theta\}\) which gives us a new variational distribution \(q(z)'\) that more closely approximates \(p(z)\).</p> <h4 id="maximization-m-step">Maximization (M-Step)</h4> <p>In the M-step, we find a new value for each parameter in \(\phi\), such that we maximize the ELBO objective with respect to the updated variational distribution \(q'\) that we just obtained from the E step.</p> \[\begin{equation} \label{eq:m-step} \begin{split} \hat\phi &amp; = \arg \max_\phi \text{ELBO}(q') \\ &amp; = \arg \max_\phi -\mathbb{KL}[q_j' \| \tilde{p}_{i\neq j}] + \mathbb{H}(z_{i\neq j}) + C \end{split} \end{equation}\] <p>We keep running these two steps until the ELBO converges to a maximum.</p> <h3 id="conclusion">Conclusion</h3> <p>Mean-field variational inference allows us to avoid approximating the intractable evidence in a Bayesian model to obtain an approximation of the posterior distribution by optimizing a factorized variational distribution over the latents and unknown parameters of interest through expectation maximization of the evidence lower bound.</p> <p>In part 2, I’ll go over my implementation of mean-field VI on the task of polygenic risk score (PRS) prediction using a gaussian likelihood model with spike-and-slab prior.</p>]]></content><author><name>Stephen Lu</name></author><category term="bayesian"/><category term="variational"/><category term="inference"/><category term="statistics"/><category term="theory"/><summary type="html"><![CDATA[Motivation Given an underlying distribution \(p_r\) with unknown parameters \(\theta\), the goal of Bayesian inference is to learn a posterior distribution over \(\theta\) given a dataset \(X\) whose examples are sampled independently from \(p_r\). Given that \(p_r\) might be a very complex distribution over many random variables, one trick to simplify the task involves introducing latent variables \(z\) that break down the overall inference problem into smaller subproblems (like a divide-and-conquer approach). Given these additional latents, our objective is to infer the joint posterior distribution over the unknown parameters \(\theta\) and latents \(z\) given an observed dataset \(X\) according to Bayes’ rule.]]></summary></entry><entry><title type="html">LLM Finetuning w/ SMILES-BERT</title><link href="https://thematrixmaster.github.io/blog/2023/finetuning-llm/" rel="alternate" type="text/html" title="LLM Finetuning w/ SMILES-BERT"/><published>2023-09-24T00:00:00+00:00</published><updated>2023-09-24T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/finetuning-llm</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/finetuning-llm/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>In a previous blog <a href="/blog/2023/embedding-gpt">post</a>, I introduced the concept of semantic embedding search to improve the performance of a large language model (llm), and provided the source code implenetation for a conversational retrieval Q&amp;A agent in LangChain. Today, I want to explore the alternative method of improving llm performance through finetuning on a downstream sequence classification task. This method is arguably more powerful than prompt tuning since finetuning can modify the model’s weights which parametrize its learned distribution over the dataset.</p> <h3 id="smiles-language-of-chemical-structure">SMILES: language of chemical structure</h3> <p>In the last blog post on <a href="/blog/2023/network-analysis-p1">network analysis</a>, I was working on the <a href="https://foodb.ca/">FooDB</a> dataset which documents a large collection of compounds found in common foods. Each compound has a chemical structure represented by a data format known as <a href="https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system">SMILES</a>, short for simplified molecular-input line-entry system. In formal language theory, SMILES is a context free language and can be modelled by a context free grammar (CFG) operating on finite sets of non-terminal, terminal, and start symbols. Alternatively, SMILES can also be interpreted as the string obtained from a depth-first tree traversal of a chemical graph. This graph is preprocessed into a traversable spanning tree by removing its hydrogen atoms and breaking its cycles. Numeric suffix labels are added to the symbols where the cycle was broken, and parentheses are used to indicate points of branching in the tree. Thus, the same chemical graph may have multiple valid SMILES representations, depending on our choice of where to break cycles, the starting atom for DFS, and the branching order. By adding constraints to some of these choices, algorithms have been developed to output <strong>canonical</strong> SMILES formats which can be converted back into its molecular graph.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-example-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-example.jpeg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> One possible SMILES string for 3-cyanoanisole. Notice that the choice of depth-first traversal highlighted on the right is one of many possible choices (Image source: <a href="https://www.researchgate.net/publication/261258149_Methods_for_Similarity-based_Virtual_Screening">Kristensen</a>, 2013) </div> <h3 id="smiles-bert">SMILES-BERT</h3> <p>Given the representational power of SMILES, attempts have been made to model this language through representation learning to capture the deep semantic knowledge contained in chemical graphs. Given that structure and function are intrinsically related to each other in biology, one would posit that learning the structural distribution of molecules could enable us to discover new relationships to function and other properties of interest. This leads us to <a href="https://dl.acm.org/doi/10.1145/3307339.3342186">SMILES-BERT</a>, a BERT-like model trained on SMILES strings through a masked recovery task and finetuned on three downstream tasks of chemical property prediction.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-bert-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-bert-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-bert-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/smiles-bert.jpeg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> SMILES-BERT model architecture (Wang, 2019) </div> <h3 id="finetuning-smiles-bert-on-foodb-compound-library">Finetuning SMILES-BERT on FooDB compound library</h3> <p>Given my recent work on the FooDB compound library, which contains SMILES representations, I thought that it would be interesting to finetune the SMILES-BERT model on this dataset to further study the chemical properties tracked by the database.</p> <p>To start, I decided to run the zero-shot SMILES-BERT model over the sequences and compress the embeddings using PCA to visualize the amount of inherent partitioning learned by the masked pretraining task. Here are the results when I color the projected embeddings by <strong>superclass</strong>, <strong>flavor</strong>, <strong>health effect</strong>, and <strong>ontology (function)</strong>:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_flavor_base5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_flavor_base5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_flavor_base5-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_flavor_base5.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_health_effects_top10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_health_effects_top10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_health_effects_top10-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_health_effects_top10.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level4top10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level4top10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level4top10-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level4top10.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level5top10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level5top10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level5top10-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_ontology_level5top10.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>As you can see, SMILES-BERT clearly learns to differentiate some properties such as superclass and flavor, but struggles on some other categories like ontology.</p> <p>Next, I wanted to actually finetune the model on these downstream classification tasks, then re-evaluate the model embedding projections on the held out test set to see if we can learn some non-linear transformations that actually enable us to better linearly separate these classes.</p> <p>Notably, I used this <a href="https://huggingface.co/seyonec/PubChem10M_SMILES_BPE_450k">model</a> from the huggingface modelhub and separated my dataset into (80, 10, 10) training, validation, and testing splits. I ran the finetuning over 5 epochs with a batch size of 16, learning rate of 2e-5, with 0.01 weight decay, and AdamW optimizer. The complete code can be found in the following github <a href="https://github.com/TheMatrixMaster/foodb-analysis">repository</a>.</p> <h3 id="results">Results</h3> <p>As expected, the finetuning yields better results on the categories that already had good embedding separation from the PCA results we saw earlier. From the training and eval loss curves plotted below, we can see that the model tends to overfit on the health effects and flavors categories.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/train_loss_all-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/train_loss_all-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/train_loss_all-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/train_loss_all.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_all-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_all-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_all-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_all.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The best results I obtained were on the superklass category, and I think that this is because this categorization had the largest data to num_labels ratio, as well being fairly reliant on molecule structure which is captured by the SMILES strings. Here are the learning curves for this run where I obtained ~98% true accuracy on the held out test set.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_superklass-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_superklass-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_superklass-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/eval_loss_superklass.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_acc_superklass-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_acc_superklass-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/eval_acc_superklass-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/eval_acc_superklass.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Finally, I reran this finetuned model on the raw SMILES strings to obtain embeddings and down-projected them using PCA. As you can see below, the model has clearly now learnt an embedding space that better separates the categories defined by the superklass label.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass_finetuned_top7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass_finetuned_top7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass_finetuned_top7-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass_finetuned_top7.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Compare this to the zero-shot embeddings below:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass-1400.webp"/> <img src="/assets/img/blog/2023-09-24-finetuning-smiles/pca_compound_superklass.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div>]]></content><author><name></name></author><category term="machine-learning"/><category term="llm"/><category term="nlp"/><category term="finetuning"/><category term="distill"/><summary type="html"><![CDATA[Motivation In a previous blog post, I introduced the concept of semantic embedding search to improve the performance of a large language model (llm), and provided the source code implenetation for a conversational retrieval Q&amp;A agent in LangChain. Today, I want to explore the alternative method of improving llm performance through finetuning on a downstream sequence classification task. This method is arguably more powerful than prompt tuning since finetuning can modify the model’s weights which parametrize its learned distribution over the dataset.]]></summary></entry><entry><title type="html">A Practical Foray into Network Analysis</title><link href="https://thematrixmaster.github.io/blog/2023/network-analysis-p1/" rel="alternate" type="text/html" title="A Practical Foray into Network Analysis"/><published>2023-09-03T00:00:00+00:00</published><updated>2023-09-03T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/network-analysis-p1</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/network-analysis-p1/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>A majority of biological data is most suitably modelled as graphs. From the atom-bond model of small molecules to the residue-backbone structure of proteins to the complex interaction networks of signal transduction pathways, the possible configurations are endless.</p> <p>Recently, I’ve been particularly interested by these research areas incorporating network science and graph representation learning to tackle problems in biology and chemistry such as drug design <d-cite key="bengio2021flow"></d-cite> and material science engineering <d-cite key="duval2023faenet"></d-cite>.</p> <p>In this series of blog posts, my goal is to document my learning process from the most basic principles in network science to advanced topics in graph neural networks through a hands-on application of techniques to interesting datasets that I can get my hands on.</p> <p>In this post, I focus on graph data visualization using the Canadian <a href="www.foodb.ca">FooDB</a> dataset covering detailed compositional, biochemical and physiological information about common food items.</p> <h2 id="food-centric-view">Food-Centric View</h2> <p>Given that the database documents the absolute enrichment of many compounds in common foods, a natural first approach is to take the food-centric view and ask whether certain compounds are especially enriched in some categories of food. To address this question in a visual way, let’s try to construct a food-centric graph. Let \(G=(V,E)\) denote our graph where each node \(v \in V\) represents a food item (ex: strawberries) and \(\exists (u,v) \in E \iff\) the foods \((u,v)\) share at least one compound. Then, for each undirected edge \((u,v) \in E\), we can add a normalized edge weight using the following function \(f(u,v)=\sum_{k\in K}{\frac{1}{S_k}}\) where \(K\) is the set of all compounds shared between foods \((u,v)\) and \(S_{k}\) is the number of foods in which compound \(k\) is found. This function \(f\) penalizes the weight of compounds that are shared by too many foods. Finally, we make node size proportional to node degree which highlights foods that are highly interconnected in the graph by sharing many compounds with other foods.</p> <h3 id="graph-visualization">Graph Visualization</h3> <p>For visualization, let’s use the network layout algorithm <a href="10.1371/journal.pone.0098679">ForceAtlas</a> in the Gephi<d-cite key="ICWSM09154"></d-cite> software that takes into account edge weight to produce a graph where strongly connected components are clustered together.</p> <p>However, I quickly realized that doing this on the entire compound dataset yielded highly connected inexplicable graphs because there were many highly shared compounds that were practically found in all food items. Further, there were also a number of compounds that were only measured in a single food item, so discarding these wouldn’t affect our visualization either. After removing these compounds, I still had too much data to process so I chose to only keep the observations for the 1500 least abundant compounds that were shared by at least one pair of food items. This yielded the following graph on all 1024 food items.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_food.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_food.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 1. Food-centric undirected graph where food item vertices and edges representing a normalized count of shared compounds between foods </div> <p>As you can expect, the layout yields pretty well clusters for some major food groups such as fruits, spices, animal foods (meat products), and vegetables. I also learned that pulses are the seeds of dry legumes such as beans, lentils, and peas.</p> <p>Looking at this graph, I saw that vegetables seem to cluster into two distinct families, so I decided to repeat the experiment, but only include foods from the vegetable family. This yielded the following graph:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_vegetable.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 2. Food-centric undirected graph for the vegetable food category </div> <p>This graph shows that root vegetables, leaf vegetables, and tubers cluster together while fruit vegetables (tomato, pepper, etc.), onions, and mushrooms form their own distinct clusters.</p> <h3 id="post-analysis">Post Analysis</h3> <p>Now that we have these food-centric graphs that seem to cluster food categories by their composition, I’d like to know which compounds are actually more enriched in which kinds of foods. To accomplish this, I’ll group the foods by their category, then look for compounds that are relatively enriched in a given category. To choose an <strong>enrichment</strong> metric, I wanted to factor in the following considerations:</p> <ul> <li>Breadth: a compound should have higher enrichment value if it is found in a larger proportion of the individual foods of a food family</li> <li>Depth: a compound should have higher enrichment value if it has a higher concentration in a given food item of the family</li> <li>Normalization: a compound’s enrichment in a family should be computed with respect to its enrichment in the other food families. In other words, enrichment is relative and not absolute.</li> </ul> <p>To accomplish this, I first compute a local enrichment score \(S_{F,c}\) for each compound \(c\) in each food family \(F\) using the following formula:</p> \[S_{F,c} = \frac{1}{|F|} \sum_{f}[c]_f\] <p>where \(\|F\|\) is the number of foods in a food family, and \([c]_f\) is the concentration of compound \(c\) in food \(f\).</p> <p>Then, I compute the final relative enrichment score \(S^r_{F,c}\) by normalizing the local enrichment score against the scores of compound \(c\) accross all food families \(F\):</p> \[S^r_{F,c} = \frac{S_{F,c}}{\sum_F S_{F,c}}\] <p>Computing this metric accross all food categories, then sorting the values in non-ascending order yielded a list of the most relatively enriched compounds in each food family. I’ve summarized the most enriched compound in each food category in the table below. A value of 1.0 means the compound was only found in that food category.</p> <table> <thead> <tr> <th>Food Category</th> <th style="text-align: right">Most Enriched Compound</th> <th style="text-align: right">\(S^r_{F,c}\)</th> </tr> </thead> <tbody> <tr> <td>Animal Foods</td> <td style="text-align: right">4-Hydroxyproline</td> <td style="text-align: right">0.446359</td> </tr> <tr> <td>Aquatic Foods</td> <td style="text-align: right">Eicosapentaenoic acid</td> <td style="text-align: right">0.926140</td> </tr> <tr> <td>Baby Foods</td> <td style="text-align: right">beta-Lactose</td> <td style="text-align: right">0.945600</td> </tr> <tr> <td>Baking Goods</td> <td style="text-align: right">Caffeic acid ethyl ester</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Cocoa Products</td> <td style="text-align: right">Theophylline</td> <td style="text-align: right">0.988216</td> </tr> <tr> <td>Coffee Products</td> <td style="text-align: right">4-Feruloylquinic acid</td> <td style="text-align: right">0.999425</td> </tr> <tr> <td>Eggs</td> <td style="text-align: right">Arachidonic acid</td> <td style="text-align: right">0.730055</td> </tr> <tr> <td>Fats and oils</td> <td style="text-align: right">Vaccenic acid</td> <td style="text-align: right">0.916681</td> </tr> <tr> <td>Fruits</td> <td style="text-align: right">Cyanidin 3</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Gourds</td> <td style="text-align: right">Kynurenine</td> <td style="text-align: right">0.971037</td> </tr> <tr> <td>Herbs and Spices</td> <td style="text-align: right">Luteolin 7</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Milk Products</td> <td style="text-align: right">D-Tryptophan</td> <td style="text-align: right">0.997252</td> </tr> <tr> <td>Nuts</td> <td style="text-align: right">N-Dodecane</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Snack foods</td> <td style="text-align: right">D-Galactose</td> <td style="text-align: right">0.387740</td> </tr> <tr> <td>Soy</td> <td style="text-align: right">Formononetin</td> <td style="text-align: right">0.998967</td> </tr> <tr> <td>Tea</td> <td style="text-align: right">Theaflavin</td> <td style="text-align: right">1.0</td> </tr> <tr> <td>Vegetables</td> <td style="text-align: right">Isoorientin</td> <td style="text-align: right">1.0</td> </tr> </tbody> </table> <h3 id="relative-compound-enrichment-at-the-food-item-level">Relative Compound Enrichment at the Food Item Level</h3> <p>Now that we’ve identified some commonly enriched compounds, it seemed interesting to me to flip the perspective and identify the individual foods that are most relatively enriched with respect to a set of target compounds of interest. To visualize this information, I once again built a per-compound graph where each node represents a food item, but this time I decided to draw an undirected edge between two food items if they belong to the same food category. Finally, node size is representative of the relative compound enrichment in each food item. This visualization allows us to quickly see which food items and food families are relatively enriched in each target compound.</p> <p>Here is the graph for sugar compounds. As you would expect, the largest nodes correspond to foods such as <em>chocolate</em>, <em>candies</em> and some <em>fruits</em>.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/by_sugars.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 3. Relative food enrichment graph for sugar compounds </div> <p>Below, I’ve provided download links to the food enrichment graphs of some other compounds that I tested.</p> <table> <thead> <tr> <th>Compound</th> <th style="text-align: right">Link</th> </tr> </thead> <tbody> <tr> <td>Cholesterol</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_cholesterol.svg">Download Link</a></td> </tr> <tr> <td>Lactose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_lactose.svg">Download Link</a></td> </tr> <tr> <td>Maltose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_maltose.svg">Download Link</a></td> </tr> <tr> <td>Nitrogen</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_nitrogen.svg">Download Link</a></td> </tr> <tr> <td>Retinol</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_retinol.svg">Download Link</a></td> </tr> <tr> <td>Sodium</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_sodium.svg">Download Link</a></td> </tr> <tr> <td>Sucrose</td> <td style="text-align: right"><a href="/assets/img/blog/2023-09-03-network-analysis/by_sucrose.svg">Download Link</a></td> </tr> </tbody> </table> <h2 id="compound-centric-view">Compound-Centric View</h2> <p>Now that we’ve analyzed the food-centric view, the next step is to look at the compound-centric view where we consider networks where each node represents a different compound. One interesting graph that we can build is a compound signature graph \(G=(V,E)\) for each food item \(f\) where each vertex \(v\in V\) corresponds to a compound that is measured in \(f\) with vertex size proportional to the concentration/enrichment of compound \(v\) in \(f\), denoted \([v]_f\). Then, for each pair of vertices \((u,v) \in V \times V\), we define an edge \((u,v,w) \in E\) where \(w\) is a weight metric that measures the co-occurence of compounds \((u,v)\) in different food items. Formally,</p> \[w_{uv} = \log_2 (S_{uv}+1)\] <p>where \(S_{uv}\) is the number of food items in which the compounds \((u,v)\) co-occur. Naturally, \(w_{uv}\) is lower bounded by 0 in the above equation. Before visualizing the graph for each food item, we prune the vertices that have no expression in the food which leaves us with a fully connected graph. However, edges with weight 1 in this graph are not very interesting to keep because it simply tells us that this edge joins two compounds that only co-occur in the current food item. So, to create a better sparse visualization, we also prune edges with unit weight.</p> <p>Here is the outcome for the <strong>beer</strong> food item.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/beer.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/beer.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Fig 4. Absolute compound enrichment in beer </div> <p>Here are some additional graphs for <strong>rice, chicken, and cow milk</strong>. I personally think that this provides a neat way to rapidly visualize a compound signature for every food item. A cool project idea to do next would be to compute a measure of graph edit distance between these structures to get a sense of <em>how different two food items are based on their composition</em>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/rice.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/rice.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/chicken.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/chicken.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg-1400.webp"/> <img src="/assets/img/blog/2023-09-03-network-analysis/cow_milk.svg" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Fig 5. Absolute compound enrichment in rice, chicken, and cow milk </div> <h2 id="conclusion">Conclusion</h2> <p>And that concludes the visualization work in this intro to network analysis series. All I used to perform this analysis is some basic python code with the pandas and networkx libraries, as well as graph visualizations in Gephi. Although these graphs don’t provide very rigourous answers to some of the questions that we asked, I think that their main value lies in their capacity to convey meaningful information about the relationships between a very large amount of data points in a way that is natural for us to reason about. This large scale view of the dataset yields many insights for further analysis that I will explore in the next part of this series.</p>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="network"/><category term="graph"/><category term="visualization"/><category term="distill"/><summary type="html"><![CDATA[Motivation A majority of biological data is most suitably modelled as graphs. From the atom-bond model of small molecules to the residue-backbone structure of proteins to the complex interaction networks of signal transduction pathways, the possible configurations are endless.]]></summary></entry><entry><title type="html">Semantic Embedding w/ ChatGPT</title><link href="https://thematrixmaster.github.io/blog/2023/embedding-gpt/" rel="alternate" type="text/html" title="Semantic Embedding w/ ChatGPT"/><published>2023-08-23T00:00:00+00:00</published><updated>2023-08-23T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/embedding-gpt</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/embedding-gpt/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Ever since the initial release of ChatGPT in November 2022, large language models have rapidly taken the spotlight of machine learning applications in industry. Leaders like Anthropic and Cohere have reached the unicorn status in the blink of an eye followed by a swarm of startups trying to apply <strong>ChatGPT to X</strong>. Like any other groundbreaking technology, large language models will take some time to fully integrate into our society, but it is undeniably something that is here to stay for the long term.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-1400.webp"/> <img src="/assets/img/blog/2023-08-23-embedding-gpt/funding.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Funding for generative AI has shot up by over 7 fold in the first half of 2023 (Image source: CB Insights, 2023) </div> <p>Given that progress is being made at blinding speed, I wanted to try out some of the awesome new tools being developed and write a tutorial on how to harness gpt-style models to your own use cases. Before we dive into the code, I want to briefly do an overview of the large language model architecture then explain the two main approaches being used to customize these models: finetuning versus embedding.</p> <h2 id="transformers">Transformers</h2> <p>At a very high level, large language models use a novel neural network architecture called transformers<d-cite key="vaswani2023attention"></d-cite> to generate the best responses to your prompt. You can think of transformers as extremely powerful sentence completion models that have <strong>efficiently</strong> learnt the underlying semantic patterns of written language to the extent that they can sample continuations to a textual prompt that are semantically meaningful. For an in depth understanding of the transformer architecture, I recommend this <a href="https://jalammar.github.io/illustrated-transformer/">blog post</a> by Jay Alammar. ChatGPT is an augmented version of the transformer model that was finetuned with reinforcement learning to better align with human instructions. Whether in the form of q&amp;a or simply listening to instructions, ChatGPT seems <strong>smart</strong> because it has been trained very well to align with human intent.<d-cite key="ouyang2022training"></d-cite></p> <h2 id="embedding-versus-finetuning">Embedding versus Finetuning</h2> <p>Although large language models are trained on data from the entire internet, the goal of these models is to learn the semantic patterns in language, not to memorize all the information on the internet. When we ask ChatGPT to answer a factual question like <em>“Which country won the most gold medals at the 2012 olympics?”</em>, the model does not have this information embedded in its finite set of parameters. Instead, it will perform a semantic search <d-footnote>You can think of this like a Google search</d-footnote> and insert the raw information it needs to answer our question into its context. This technique is known as <strong>semantic embedding</strong> and we can also use it to help ChatGPT access information that is relevant to our application domain. For example, if we want to build a personal gpt assistant that can answer relevant questions about all my previous tax returns, then I can put these relevant text documents into a database and allow a large language model to query it before answering my questions pertaining to this topic. As you can see, semantic embeddings don’t modify the underlying large language model at all. It just fetches the relevant information to better align the model’s answer with the users prompt.</p> <p>Finetuning on the other hand does modify the weights of the model. We already gave an example of finetuning earlier when we described how ChatGPT was trained downstream with reinforcement learning to better align with human instructions. Thus, it can be understood that the weights of the model encode something related to the built-in biases and choices made by the model when instructions are open to interpretation.</p> <p>A better way to explain this is by drawing a parallel with human behaviour. Humans are extremely good at following instructions, especially when these instructions are very detailed and leave little to no room for interpretation. However, when we are given vague instructions or open-ended questions, the responses that we provide reflect our personal opinions and biases. Even in the case when the possible responses are relatively constrained, biases are still present in low-level constructs such as sentence structure, word choice, and semantic sentiment. For example, although two doctors might give you the same diagnosis and prescription, the way they convey that information, the amount of detail they provide, and the tone they convey could be drastically different.</p> <p>The same can be said for large language models when they are prompted with tasks that are open to interpretation. If we want a model that consistently and reliably provides responses in a particular tone with a particular set of predetermined biases, then it makes more sense to finetune the model so that it aligns its decisions with this set of priors. For example, Claude is the flagship llm by Anthropic that is finetuned using a reinforcement learning policy aligned with a set of <a href="https://www.anthropic.com/index/claudes-constitution">constitutional principles</a> such as being <em>ethical</em>, <em>virtuous</em>, etc. In essense, finetuning can be thought of as introducing prior preferences into the llm that guide its responses on top of the fundamental prior that the model should be aligned with the user intent.</p> <h2 id="implementing-a-conversational-retrieval-qa-agent-in-langchain">Implementing a Conversational Retrieval Q&amp;A agent in LangChain</h2> <p>In the following tutorial, I will implement a conversational Q&amp;A agent such as ChatGPT that has access to the standard chat history context as well as a database of private documents that can be used for <strong>semantic embedding</strong> search. I will address model finetuning in a future blog post.</p> <h3 id="setting-up-a-zep-server">Setting up a Zep server</h3> <p>Recall that semantic embedding search involves searching for relevant documents in a database so that we can inject relevant information into the context of the model before providing a response. In our implementation, we will use <a href="https://github.com/getzep/zep">Zep</a> which is a long term RAM memory store that is optimized for storing text embeddings and performing various search algorithms over these embeddings.</p> <p>You can follow these instructions on their <a href="https://docs.getzep.com/deployment/quickstart/#starting-a-zep-server-locally-is-simple">getting started</a> page to start your Zep server. It is as easy as setting up some <code class="language-plaintext highlighter-rouge">API_KEY</code> environment variables and running a <code class="language-plaintext highlighter-rouge">docker-compose up</code> command.</p> <h3 id="adding-documents-to-the-zep-server">Adding documents to the Zep server</h3> <p>Now that the Zep database is up and running, we want to add our document embeddings into the database, so that we can perform retrieval against them. To do this, I wrote a python seeder script, but Zep also supports an SDK in native JavaScript as well as integration with 3rd party libraries such as LangChain and LlamaIndex which you can use to interact with the database as well.</p> <h4 id="creating-a-collection">Creating a collection</h4> <p>Now, the first step is to connect to the database and create a collection. Zep organizes its data in the following hierarchy: <strong>collection &gt; documents &gt; embeddings</strong>. Going back to the tax returns example, we could think of a document as individual tax related files, while a collection might group all such files for a particular year. Zep takes our documents and does the embedding for us, so we need to provide an embedding model. Here I choose the <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> model by OpenAI since we will be using ChatGPT for our llm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">zep_python</span> <span class="kn">import</span> <span class="n">ZepClient</span>
<span class="kn">from</span> <span class="n">zep_python.document</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">zep_api_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:8000</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">ZepClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">zep_api_url</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
Step 1: Create a new collection in which to store our documents for a given task class
        We will use chatgpt text-embedding-ada-002 model which has embedding size of 1536
</span><span class="sh">"""</span>

<span class="n">collection_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;name the collection&gt;</span><span class="sh">"</span>

<span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">delete_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">add_collection</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>  <span class="c1"># required
</span>        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">&lt;some description&gt;</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional
</span>        <span class="n">embedding_dimensions</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span>  <span class="c1"># this must match the model you've configured for 
</span>        <span class="n">is_auto_embedded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># use Zep's built-in embedder. Defaults to True
</span>    <span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <h4 id="chunking-up-documents">Chunking up documents</h4> <p>Our documents can have a large variation in their length and content size, so it doesn’t really make sense to squeeze each document into a fixed size embedding. Instead, we split up documents into fixed size chunks that have a predetermined token length, then embed each chunk into a vector embedding. Here I split my documents into chunks with a maximum of 1600 tokens per chunk, but this process will heavily depend on the nature and format of your documents. The code I provide below is just an example of how this chunking might be done, but you should write your own routine for this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DOC_DIR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents</span><span class="sh">"</span>
<span class="n">FILE_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents/raw_convo.txt</span><span class="sh">"</span>

<span class="c1"># Custom splitting for .txt file such that each entry in qa_data is a tuple of ([questions], answer)
# TODO: Add support for csv, json, and yml files
</span><span class="n">sections</span> <span class="o">=</span> <span class="nf">split_into_sections</span><span class="p">(</span><span class="n">FILE_NAME</span><span class="p">)</span>
<span class="n">qa_sections</span> <span class="o">=</span> <span class="nf">split_into_qa_pairs</span><span class="p">(</span><span class="n">sections</span><span class="p">)</span>
<span class="n">qa_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">section</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">qa_sections</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">questions</span><span class="p">,</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">qa_data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">questions</span><span class="p">,</span> <span class="n">answer</span><span class="p">))</span>

<span class="c1"># Split the qa pairs into chunks with a predefined max token length
</span><span class="n">MAX_TOKENS</span> <span class="o">=</span> <span class="mi">1600</span>
<span class="n">qa_strings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="n">qa_data</span><span class="p">:</span>
    <span class="n">qa_strings</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">split_strings_from_subsection</span><span class="p">(</span><span class="n">section</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="n">MAX_TOKENS</span><span class="p">))</span>
</code></pre></div></div> <h4 id="embedding-chunks-into-zep">Embedding chunks into Zep</h4> <p>The last step is to embed the chunks into Zep using the <code class="language-plaintext highlighter-rouge">Document</code> class. Here we could choose to add metadata to each chunk to identify, for example, which original file it belongs to. These metadata can then serve as future <em>filters</em> when we search against the Zep database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 3: Embed the document chunks and store them into the collection
</span><span class="sh">"""</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">Document</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
        <span class="n">document_id</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional document ID
</span>        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">bar</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>  <span class="c1"># optional metadata
</span>    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">qa_strings</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">uuids</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we can also spin up a busy waiting watcher process that waits for the documents to be embedded before exiting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 4: Wait for the documents to be embedded and monitor the process
</span><span class="sh">"""</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">get_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">Embedding status: </span><span class="sh">"</span>
        <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_embedded_count</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_count</span><span class="si">}</span><span class="s"> documents embedded</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="sh">"</span><span class="s">ready</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Added </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">uuids</span><span class="p">)</span><span class="si">}</span><span class="s"> documents to collection </span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="giving-chatgpt-access-to-the-zep-database">Giving ChatGPT access to the Zep database</h3> <p>Now that we have our vector database ready to go, we just need to hook up a language model to query from it and add the relevant embeddings to its context before answering the user. I used <a href="https://github.com/hwchase17/langchainjs">LangChain</a> which is an awesome framework that enables easy interaction with popular llms as well as integration with 3rd party plugins and databases such as Zep. Using the LangChain JavaScript SDK, I simply need to do the following steps:</p> <ol> <li>Connect to the Zep database</li> <li>Retrieve the user’s chat history along with his current active prompt</li> <li>Embed the chat history along with the current active prompt use as a prototype search vector</li> <li>Use the search vector to find semantically related embeddings in the Zep database</li> <li>Feed all the relevant embeddings from Zep and the chat context to an instance of ChatGPT model</li> <li>Return the model response to the user</li> </ol> <p>Using SvelteKit server module, this can be done in very few lines of code.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span> <span class="nx">OPENAI_ORGANIZATION</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">$env/static/private</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ChatOpenAI</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chat_models/openai</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">ConversationalRetrievalQAChain</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chains</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ZepVectorStore</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/vectorstores/zep</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">FakeEmbeddings</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/embeddings/fake</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">BufferMemory</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/memory</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">error</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">@sveltejs/kit</span><span class="dl">'</span><span class="p">;</span>

<span class="k">export</span> <span class="kd">type</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="p">{</span> 
    <span class="na">question</span><span class="p">:</span> <span class="kr">string</span><span class="p">;</span>
    <span class="nl">settings</span><span class="p">:</span> <span class="p">{</span> <span class="na">temperature</span><span class="p">:</span> <span class="kr">number</span><span class="p">,</span> <span class="na">relatedness</span><span class="p">:</span> <span class="kr">number</span> <span class="p">};</span>
<span class="p">}</span>

<span class="kd">const</span> <span class="nx">zepConfig</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">apiUrl</span><span class="p">:</span> <span class="dl">"</span><span class="s2">http://localhost:8000</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// the URL of your Zep implementation</span>
    <span class="na">collectionName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">&lt;collection_name&gt;</span><span class="dl">"</span><span class="p">,</span>  <span class="c1">// the name of your collection. alphanum values only</span>
    <span class="na">embeddingDimensions</span><span class="p">:</span> <span class="mi">1536</span><span class="p">,</span>  <span class="c1">// much match the embeddings you're using</span>
    <span class="na">isAutoEmbedded</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>  <span class="c1">// will automatically embed documents when they are added</span>
<span class="p">};</span>

<span class="kd">const</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span> <span class="o">=</span> <span class="s2">`
    Given the following conversation and a follow up question,
    return the conversation history excerpt that includes any relevant context to the question
    if it exists and rephrase the follow up question to be a standalone question.
    
    Chat History: {chat_history}
    Follow Up Input: {question}
    
    Your answer should follow the following format:

    </span><span class="se">\`\`\`</span><span class="s2">
    &lt;Here you can give some additional behavioural instructions to the model in the form of prompting. The result will not be as good as finetuning the model on a large amount of
    examples that properly introduce a set of behavioural guidelines for the model to respect.&gt;
    ----------------
    &lt;Relevant chat history excerpt as context here&gt;
    Standalone question: &lt;Rephrased question here&gt;
    </span><span class="se">\`\`\`</span><span class="s2">

    Your answer:
`</span><span class="p">;</span>

<span class="kd">const</span> <span class="nx">embeddings</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FakeEmbeddings</span><span class="p">();</span>

<span class="k">export</span> <span class="kd">const</span> <span class="nx">POST</span> <span class="o">=</span> <span class="k">async </span><span class="p">({</span> <span class="nx">request</span> <span class="p">})</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="na">body</span><span class="p">:</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">request</span><span class="p">.</span><span class="nf">json</span><span class="p">();</span>

    <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">body</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Data</span><span class="dl">'</span><span class="p">);</span>

    <span class="c1">// Connect to the Zep vector store server</span>
    <span class="kd">const</span> <span class="nx">vectorStore</span> <span class="o">=</span> <span class="k">await</span> <span class="k">new</span> <span class="nc">ZepVectorStore</span><span class="p">(</span><span class="nx">embeddings</span><span class="p">,</span> <span class="nx">zepConfig</span><span class="p">);</span>

    <span class="c1">// Create a new readable stream of the chat response</span>
    <span class="kd">const</span> <span class="nx">readableStream</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ReadableStream</span><span class="p">({</span>
        <span class="k">async</span> <span class="nf">start</span><span class="p">(</span><span class="nx">controller</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Create a new chat model</span>
            <span class="kd">const</span> <span class="nx">streamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-4</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">streaming</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
                <span class="na">callbacks</span><span class="p">:</span> <span class="p">[{</span>
                    <span class="na">handleLLMNewToken</span><span class="p">:</span> <span class="k">async </span><span class="p">(</span><span class="na">token</span><span class="p">:</span> <span class="kr">string</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nx">controller</span><span class="p">.</span><span class="nf">enqueue</span><span class="p">(</span><span class="nx">token</span><span class="p">),</span>
                <span class="p">}]</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">nonStreamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-3.5-turbo</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">chain</span> <span class="o">=</span> <span class="nx">ConversationalRetrievalQAChain</span><span class="p">.</span><span class="nf">fromLLM</span><span class="p">(</span>
                <span class="nx">streamingModel</span><span class="p">,</span>
                <span class="nx">vectorStore</span><span class="p">.</span><span class="nf">asRetriever</span><span class="p">(),</span>
                <span class="p">{</span>
                    <span class="na">memory</span><span class="p">:</span> <span class="k">new</span> <span class="nc">BufferMemory</span><span class="p">({</span>
                        <span class="na">memoryKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">chat_history</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// Must be set to "chat_history"</span>
                        <span class="na">inputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">question</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the input to the chain</span>
                        <span class="na">outputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">text</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the final conversational output of the chain</span>
                        <span class="na">returnMessages</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span> <span class="c1">// If using with a chat model</span>
                    <span class="p">}),</span>
                    <span class="na">returnSourceDocuments</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                    <span class="na">questionGeneratorChainOptions</span><span class="p">:</span> <span class="p">{</span>
                        <span class="na">llm</span><span class="p">:</span> <span class="nx">nonStreamingModel</span><span class="p">,</span>
                        <span class="na">template</span><span class="p">:</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span>
                    <span class="p">},</span>
                <span class="p">},</span>
            <span class="p">);</span>

            <span class="kd">const</span> <span class="nx">question</span> <span class="o">=</span> <span class="nx">body</span><span class="p">.</span><span class="nx">question</span><span class="p">;</span>
            <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">question</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Question</span><span class="dl">'</span><span class="p">);</span>

            <span class="kd">const</span> <span class="nx">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">chain</span><span class="p">.</span><span class="nf">call</span><span class="p">({</span> <span class="nx">question</span> <span class="p">});</span>
            <span class="nx">controller</span><span class="p">.</span><span class="nf">close</span><span class="p">();</span>
        <span class="p">},</span>
    <span class="p">});</span>

    <span class="c1">// Create and return a response of the readable stream</span>
    <span class="k">return</span> <span class="k">new</span> <span class="nc">Response</span><span class="p">(</span><span class="nx">readableStream</span><span class="p">,</span> <span class="p">{</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">text/plain</span><span class="dl">'</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">});</span>
<span class="p">}</span>
</code></pre></div></div> <p>For more information on this script, please visit the following <a href="https://js.langchain.com/docs/modules/chains/popular/chat_vector_db">documentation</a>. The full code for my walkthrough can be found <a href="https://github.com/TheMatrixMaster/gpt-embeddings">here</a>.</p>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="llm"/><category term="nlp"/><category term="search"/><category term="distill"/><summary type="html"><![CDATA[Motivation Ever since the initial release of ChatGPT in November 2022, large language models have rapidly taken the spotlight of machine learning applications in industry. Leaders like Anthropic and Cohere have reached the unicorn status in the blink of an eye followed by a swarm of startups trying to apply ChatGPT to X. Like any other groundbreaking technology, large language models will take some time to fully integrate into our society, but it is undeniably something that is here to stay for the long term.]]></summary></entry><entry><title type="html">A Gentle Intro to Variational Autoencoders</title><link href="https://thematrixmaster.github.io/blog/2023/vae/" rel="alternate" type="text/html" title="A Gentle Intro to Variational Autoencoders"/><published>2023-08-12T00:00:00+00:00</published><updated>2023-08-12T00:00:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/vae</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/vae/"><![CDATA[<h2 id="motivation">Motivation</h2> <p>Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE)<d-cite key="kingma2022autoencoding"></d-cite>. This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.</p> <h2 id="how-to-frame-generative-models">How to frame generative models</h2> <p>When learning generative models, we assume that the dataset is generated by some unknown underlying source distribution \(p_r\). Our goal is to learn a distribution \(p_g\) that approximates \(p_r\) from which we can sample new realistic data points. Unfortunately, we often don’t have access to \(p_r\), so the best we can do is approximate another distribution \(p_{\hat{r}}\) such that \(p_{\hat{r}}\) maximizes the likelihood of producing the dataset if it were to repeatedly sample independently from it.</p> <p>Now, there are two main ways that we can go about learning \(p_g \to p_{\hat{r}}\)</p> <ol> <li>Learn the parameters of \(p_g\) directly through maximum likelihood estimation (MLE) by minimizing the KL divergence \(D_{KL}(p_{\hat{r}} \Vert p_g) = \lmoustache p_{\hat{r}}(x) \frac{p_{\hat{r}}(x)}{p_g(x)}dx\).</li> <li>Or, learn a differentiable generative function \(g_\theta\) that maps an existing prior distribution \(Z\) into \(p_g\) such that \(p_g = g_\theta(Z)\).</li> </ol> <p>The issue with the first approach is that the KL divergence loss is extremely unstable when the parameters we want to estimate (in this case the parameters of \(p_{\hat{r}}\)) can belong to an arbitrarily large family of distributions. Indeed, if we examine the KL divergence expression, we see that wherever \(p_{\hat{r}}(x) &gt; 0\), \(p_g(x) &gt; 0\) must also be true, otherwise we end up with an exploding gradient problem during learning as the loss goes to infinity. One way to get around this could be to use a “nicer” loss metric between distributions that is smooth and differentiable everywhere such as the Wasserstein distance.<d-footnote>For more information on Wasserstein methods, I recommend this great <a href="https://www.alexirpan.com/2017/02/22/wasserstein-gan.html"><b>blog post</b></a> by Alex Irpan on WGANs.</d-footnote>However, even if we were able to learn \(p_g\) in such a way, it may be difficult to sample from this distribution with diversity because we don’t know what the learnt parameters of \(p_g\) represent with respect to the data.</p> <p>The better approach, used by GAN and VAE, is the 2nd where we learn a generative function \(g_\theta\) that maps a handpicked prior distribution \(Z\) into the data space. The upside of this approach is that <em>hopefully</em>, if all goes well, the parameters of our prior distribution \(Z\) will be mapped to disentangled high-level features of the data. If we achieve this, then we can easily generate new samples with more control and variety, because we can now sample strategically from $z\sim Z$ (which we handpicked) and then evaluate $g_\theta(z)$.<d-footnote>Compare this to the previous approach where we have no idea what the latent distribution even looks like.</d-footnote> The downside of this approach is that we don’t know explicitly what $p_g$ is, but this is usually okay unless interpretability is very important to your task.</p> <h2 id="what-is-an-autoencoder">What is an Autoencoder</h2> <p>Before I introduce the variational autoencoder, I want to briefly go over its sibling, the autoencoder. The autoencoder is an unsupervised machine learning model whose purpose is to learn a more meaningful representation of the input data in lower dimensional space.<d-footnote>If you want a better intuitive understanding of why we would want to learn such a lower dimensional representation, I suggest this blog post on <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"><b>manifolds</b></a> by Chris Olah.</d-footnote> To accomplish this, the autoencoder trains two networks placed back-to-back — an encoder and a decoder.</p> <ul> <li>The encoder learns a function \(f_\phi\) that transforms the high-dimensional input into the low-dimensional latent representation \(z = f_\phi(x)\). Notice here that the latent distribution \(Z\) is unknown and uncontrolled for, meaning that we entirely let the model decide what is the best way to represent the latent embeddings.</li> <li>The decoder learns the inverse function \(g_\theta\) that attempts to transform the low-dimensional representation back into the original example such that \(x' = g_\theta(z)\).</li> <li>Naturally, the loss function aims to minimize the reconstruction error by minimizing the euclidean distance between the original example and the reconstructed example.</li> </ul> \[\mathbb{L}(\phi, \theta) = \frac{1}{n}\sum^n_{i=1}(x^i - g_\theta(f_\phi(x)))^2\] <p>To recap, the goal of the autoencoder is to learn meaningful representations of the data in a lower dimensional latent space by using an encoder-decoder pair.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/autoencoder-architecture-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/autoencoder-architecture.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h2 id="vae-as-a-generative-autoencoder">VAE as a Generative Autoencoder</h2> <p>Recall that fundamentally, a generative model aims to learn a generative function \(g_\theta: Z \to p_g\) mapping the prior distribution \(Z\) into the data space that <strong>maximizes the likelihood of generating samples from the dataset</strong>. At this point, I hope that it is easier to notice that the function learned by the decoder above does exactly this likelihood maximization since it minimizes the mean squared error loss between the input examples <strong>from the dataset</strong> and the reconstructed outputs. However as mentioned before, the latent domain of this standard decoder function is uncontrolled for whereas we want our generative function \(g_\theta\) to have a handpicked prior domain \(Z\).</p> <p>The variational autoencoder (VAE) is thus simply an autoencoder supplemented with an inductive prior that the latent distribution \(Z\) should fit into a pre-selected family of handpicked probability distributions.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/vae-gaussian-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/vae-gaussian.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Illustration of variational autoencoder architecture (Image source: Weng, L., 2018<d-cite key="weng2018VAE"></d-cite>) </div> <h3 id="how-to-pick-the-distribution-family-for-z">How to pick the distribution family for Z</h3> <p>Most often, we constrain the distribution \(Z\) to be a multivariate gaussian distribution with diagonal covariance matrix \(\mathcal{N}(z|\mu, \sigma^2)\). Now that is mouthful, but the real question is why?</p> <p>Intuitively, we can think of the task of the generative function $g_\theta$ as having to learn something meaningful about the content in the data we wish to generate as well as learning to map the variation in this content to the variation in the low-dimensional latent space \(Z\). As I explained before, we don’t want to \(Z\) to be unconstrained, because we won’t know how to sample cheaply and representatively from it. On the opposite hand, we don’t want to over-constrain \(Z\) either because this might prevent the encoder from learning an expressive and meaningful latent representation of the data. The Gaussian distribution achieves this balance well because it introduces the least amount of prior knowledge into \(Z\) while being extremely easy to sample from.</p> <p>The diagonal covariance matrix constraint encourages the encoder to learn a multivariate gaussian where each dimension is independent from another. This is desirable when we want to learn the most fundamental sources of variation in the data which often happen to be independent. For example, in the MNIST dataset, we don’t want the model to conflate the representations of the number 1 and 7 just because they share some similarities.</p> <h3 id="deriving-the-vae-loss-function">Deriving the VAE loss function</h3> <p>This section of the blog post will be the most math heavy, but I hope that it can provide a better intuition for where the VAE loss comes from. Most resources that I’ve found online directly derive the loss starting from the KL divergence between the estimated and real bayesian posterior distributions of \(z\) conditioned on \(x\), but this seems like it skipped a few steps especially for those who aren’t well-versed in Bayesian theory. Instead, let’s start from first principles.</p> <p>Recall that our objective is to approximate a distribution \(p_{\hat{r}}\) that maximizes the likelihood of generating the dataset \(D\). To do this, we explicitly defined a prior distribution \(p(z)\) for the latent space and now we are attempting to learn a probabilistic decoder distribution \(p_\theta(x\vert z)\) through our generative function \(g_\theta\). Thus, it should be clear that our goal is to find the parameters \(\theta^*\) such that</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[p_\theta(x)] \\ &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{p_\theta(z|x)}] \\ \end{split}\] <p>by <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p> <p>First, we note that attempting to learn \(\theta\) directly here using maximum likelihood estimation with loss \(-\log \mathbb{E}_{x \sim D}[p_\theta(x)]\) is impossible because \(p_\theta(x)\), which is known as the evidence in Bayesian statistics, is intractable. If we wanted to compute \(p_\theta(x)\), we would need to marginalize over all values of \(z\) and further, we don’t have access to the posterior distribution \(p_\theta(z\vert x)\).</p> <p>So we go for a different approach, notice that if we use a neural network to approximate the posterior \(p_\theta(z\vert x)\), then we can manipulate the expectation above to a tractable form. Notably, let \(q_\phi(z\vert x)\) be a probabilistic function learned by the VAE encoder parametrized by \(\phi\) such that our new objective function becomes</p> \[\begin{split} \theta^* &amp; = \arg \max_{\theta} \mathbb{E}_{x \sim D}[\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}] \\ &amp; \propto \arg \max_{\theta} \mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{p(z)}{q_\phi(z|x)}] \\ &amp; = \arg \min_{\theta} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{E}_{z\sim q_\phi(z|x)}[\frac{q_\phi(z|x)}{p(z)}] \\ &amp; = \arg \min_{\theta, \phi} -\mathbb{E}_{z\sim q_\phi(z|x)}[\log{} p_\theta(x|z)] + \mathbb{D_{KL}}(q_\phi(z|x)\Vert p(z)) \\ &amp; = \arg \min_{\theta, \phi} [-\text{likelihood} + \text{KL divergence}] \\ &amp; = \max_{\theta, \phi}ELBO(\theta, \phi) \\ &amp; = \min_{\theta, \phi}L_{VAE}(\theta, \phi) \end{split}\] <p>Notice that this loss expression is exactly what we intuitively wanted to do in the first place. <strong>Maximize the likelihood of generating the data from our dataset while adding a regularizer term that encourages the latent space distribution to fit in our gaussian prior \(p(z).\)</strong></p> <h2 id="implementing-a-vae-in-pytorch">Implementing a VAE in PyTorch</h2> <p>Now that we have all the pieces of the puzzle, let’s train a VAE in PyTorch to generate images of characters from the Simpsons. My implementation is based on this great github repository<d-cite key="subramanian2020"></d-cite> that offers a whole collection of the latest VAE architectures. For a comprehensive overview of a variety of autoencoder architectures, I recommend this blog post by Lillian Weng.<d-cite key="weng2018VAE"></d-cite></p> <h3 id="the-dataset">The dataset</h3> <p>I used a Simpsons <a href="https://www.kaggle.com/datasets/alexattia/the-simpsons-characters-dataset">dataset</a> of ~20000 character images. Loading the dataset into PyTorch is simply a matter of implementing the <code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code> class.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MyDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">data_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">split</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">transform</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span> <span class="o">=</span> <span class="nc">Path</span><span class="p">(</span><span class="n">data_path</span><span class="p">)</span>      
        <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="n">transform</span>
        <span class="n">imgs</span> <span class="o">=</span> <span class="nf">sorted</span><span class="p">([</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">data_dir</span><span class="p">.</span><span class="nf">iterdir</span><span class="p">()</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="n">suffix</span> <span class="o">==</span> <span class="sh">'</span><span class="s">.jpg</span><span class="sh">'</span><span class="p">])</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">imgs</span> <span class="o">=</span> <span class="n">imgs</span><span class="p">[:</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)]</span> <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="sh">"</span><span class="s">train</span><span class="sh">"</span> <span class="k">else</span> <span class="n">imgs</span><span class="p">[</span><span class="nf">int</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">imgs</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">):]</span>

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">img</span> <span class="o">=</span> <span class="nf">default_loader</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">imgs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">transforms</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">img</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transforms</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">img</span><span class="p">,</span> <span class="mf">0.0</span> <span class="c1"># dummy data label to prevent breaking 
</span></code></pre></div></div> <h3 id="the-model">The model</h3> <p>We start with the encoder model which takes in a batch of images and outputs the parameters of our multi-variate gaussian distribution \(Z\). In the model architecture declaration below, we use convolutional layers in the encoder body to capture the image features followed by 2 different linear output layers for the mean and variance vectors.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">in_channels</span> <span class="o">=</span> <span class="n">input_dim</span>
<span class="n">hidden_dims</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">]</span>

<span class="c1"># Declare the Encoder Body
</span><span class="k">for</span> <span class="n">h_dim</span> <span class="ow">in</span> <span class="n">hidden_dims</span><span class="p">:</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="o">=</span><span class="n">h_dim</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">h_dim</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">in_channels</span> <span class="o">=</span> <span class="n">h_dim</span>

<span class="n">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="c1"># Declare the Encoder output layer
</span><span class="n">self</span><span class="p">.</span><span class="n">fc_mu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">fc_var</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">,</span> <span class="n">latent_dim</span><span class="p">)</span>
</code></pre></div></div> <p>Similarly to the encoder, the decoder architecture takes in a latent vector outputted by the encoder, then uses transposed convolution layers to upsample from the low dimensional latent representations. Finally, we use a conv output layer followed by tanh activation function to map the decoder output back to the normalized input pixel space \(\in [-1, 1]\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Declare Decoder Architecture
</span><span class="n">modules</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">hidden_dims</span><span class="p">.</span><span class="nf">reverse</span><span class="p">()</span>
<span class="n">self</span><span class="p">.</span><span class="n">decoder_input</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">latent_dim</span><span class="p">,</span> <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">modules</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span>
                                <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                <span class="n">stride</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
                                <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span>
            <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">()</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">modules</span><span class="p">)</span>

<span class="n">self</span><span class="p">.</span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
                        <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                        <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                        <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                        <span class="n">output_padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">BatchNorm2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">LeakyReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv2d</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">out_channels</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">())</span>
</code></pre></div></div> <h3 id="training">Training</h3> <p>During training, we feed the input batch through the encoder to obtain a list of mean and variance vectors. We then sample from this multivariate gaussian using a reparameterization function to obtain a list of latent vectors \([z]\). This step is important because it not only allows us to sample from \(Z\), but also to take a derivative with respect to the encoder parameters during backpropagation. Finally, we feed these latent vectors to the decoder, which outputs a tensor of reconstructed images.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encoder</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Split the result into mu and var components
</span>    <span class="c1"># of the latent Gaussian distribution
</span>    <span class="n">mu</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_mu</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc_var</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">logvar</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="c1"># Use this to sample from the latent distribution Z
</span>    <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">std</span> <span class="o">+</span> <span class="n">mu</span>

<span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder_input</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">result</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decoder</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">final_layer</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">encode</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">)</span>
    <span class="k">return</span>  <span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">]</span>
</code></pre></div></div> <p>Finally, we compute the ELBO loss derived above and backpropagate.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">log_var</span><span class="p">,</span> <span class="n">kld_weight</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="c1"># Maximizing the likelihood of the input dataset is equivalent to minimizing
</span>    <span class="c1"># the reconstruction loss of the variational autoencoder
</span>    <span class="n">recons_loss</span> <span class="o">=</span><span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">recons</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

    <span class="c1"># KL divergence between our prior on Z and the learned latent space by the encoder
</span>    <span class="c1"># This measures how far the learned latent distribution deviates from a multivariate gaussian
</span>    <span class="n">kld_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">log_var</span> <span class="o">-</span> <span class="n">mu</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">log_var</span><span class="p">.</span><span class="nf">exp</span><span class="p">(),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    
    <span class="c1"># The final loss is the reconstruction loss (likelihood) + the weighted KL divergence 
</span>    <span class="c1"># between our prior on Z and the learned latent distribution
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">recons_loss</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">*</span> <span class="n">kld_weight</span> <span class="o">*</span> <span class="n">kld_loss</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">Reconstruction_Loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">recons_loss</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">KLD</span><span class="sh">'</span><span class="p">:</span> <span class="n">kld_loss</span>
    <span class="p">}</span>
</code></pre></div></div> <h3 id="sampling">Sampling</h3> <p>When we want to sample, we can simply sample a latent vector \(z\) from our multivariate gaussian latent prior \(p(z)\), then feed it through the decoder.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">current_device</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dim</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">current_device</span><span class="p">)</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">decode</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">samples</span>
</code></pre></div></div> <h3 id="results">Results</h3> <p>Here are the results of the VAE that I trained on the Simpsons datasets after 100 epochs with 64 batch size. On the left are images recostructed by the model, and on the right are images sampled from the decoder. The results are quite blurry which is a typical symptom of VAEs as the Gaussian prior inductive bias might be acting too strong. Training for more epochs should yield better results.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/recons-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/recons-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/recons-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/recons.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-13-vae/sample-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-13-vae/sample-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-13-vae/sample-1400.webp"/> <img src="/assets/img/blog/2023-08-13-vae/sample.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name>Stephen Lu</name></author><category term="machine-learning"/><category term="vae"/><category term="generative"/><category term="distill"/><summary type="html"><![CDATA[Motivation Recently, I’ve been going down the rabbit hole of generative models, and I have been particularly interested in the Variational Autoencoder (VAE). This blog post aims to provide a gentle introduction to VAEs through a balanced mix of theory and implementation. Particularly, I want to focus on the intuition behind the VAE and derive the loss function from this intuition in an easy way to follow. I will also provide a simple walkthrough of the implementation of a VAE in PyTorch on a Simpsons character dataset.]]></summary></entry><entry><title type="html">The Beauty of the Late Night</title><link href="https://thematrixmaster.github.io/blog/2023/late-night/" rel="alternate" type="text/html" title="The Beauty of the Late Night"/><published>2023-08-07T23:01:00+00:00</published><updated>2023-08-07T23:01:00+00:00</updated><id>https://thematrixmaster.github.io/blog/2023/late-night</id><content type="html" xml:base="https://thematrixmaster.github.io/blog/2023/late-night/"><![CDATA[<p>Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.</p> <p>Now I do agree that a 2am to 10am sleep schedule is not something to be proud of, but being a night owl also comes with its own unique merits. So, before I comply with my grandma’s stern request of sleeping each night before 11pm, I want to tell you about the beauty of the late night.</p> <p>If you’re like me and suffer from slight anxiety, you’ll likely find that late at night is when your productivity peaks. Personally, I am much more easily distractable during the day, while after 10pm is when I can maintain my longest concentration span. Perhaps it is due to the fact that late nights offer the most quiet, peaceful environment without distractions. Or maybe the reduced cortisol levels at night can downregulate the stress response and temporarily keep procrastination in check. Regardless of the reason, the late night has become a precious personal refuge for me to make progress on my todo list.</p> <p>Beyond productivity boosts, it is my belief that the late night provides a magical window during which it is easier to connect with the authentic soul of your surrounding environment. In my experience, when the moon shines at night, everything becomes more authentic — from people to cities to conversations and memories. It is difficult to describe this elusive feeling to you unless you’ve experienced it yourself. Personally, I don’t take it as a coincidence that my strongest friendships, deepest thoughts, and favourite memories have been shaped by the late night. If you don’t believe me, I encourage you to take a stroll around your neighbourhood next time you stay up past bedtime and take the time to slowly rediscover your surroundings. You might find something that you missed before.</p> <p>An activity that I have absolutely fallen for is nighttime photography. In the hopes of capturing the essense of the magical late night that I’ve introduced to you above, here is a collection of photographs of my favourite late night moments this year.</p> <h4 id="singapore">Singapore</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5221-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_5221.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_5254-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_5254.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="korea--china">Korea &amp; China</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6077-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_6077.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_6929-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_6929.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="japan">Japan</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7774-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_7774.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-07-late-night/IMG_7782-1400.webp"/> <img src="/assets/img/blog/2023-08-07-late-night/IMG_7782.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="travel"/><category term="photography"/><summary type="html"><![CDATA[Recently, I’ve received a lot of backlash from my grandma for being a night owl. In Chinese belief, sleeping late at night is particularly detrimental to one’s long term health and prosperity. There’s even a famous four character Chengyu proverb that preaches the righteousness of the early bird — 早睡早起.]]></summary></entry></feed>