<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Latent Dirichlet Allocation | Stephen Z. Lu</title> <meta name="author" content="Stephen Z. Lu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/sl-icon-dark.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://thematrixmaster.github.io/blog/2023/latent-dirichlet/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Latent Dirichlet Allocation",
      "description": "",
      "published": "December 27, 2023",
      "authors": [
        {
          "author": "Stephen Lu",
          "authorURL": "https://matrixmaster.me",
          "affiliations": [
            {
              "name": "McGill University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Stephen </span>Z. Lu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">cv</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/assets/pdf/cv.pdf">Download</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Latent Dirichlet Allocation</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="motivation">Motivation</h2> <p>If you’ve ever played around with unsupervised clustering algorithms like k-means, then the concept of topic modeling should already be familiar to you. Informally, topic modelling can be thought of as discovering the underlying “topics” or “themes” that are present in a collection of documents. For example, if we were to apply topic modeling to a collection of news articles, we might expect to find topics like “politics”, “sports”, “entertainment”, etc.</p> <p>Many topic models are Bayesian probabilistic models that make assumptions about how the dataset documents are generated. We call this the generative process. By performing MLE of the dataset with respect to the model parameters, we can discover the latent variables in the Bayesian model that are responsible for generating the documents. Naturally, different generative processes lead to different topic models. In this post, we’ll be looking at Latent Dirichlet Allocation (LDA) introduced by Blei et al.<d-cite key="blei2003lda"></d-cite> in 2003.</p> <h2 id="generative-process">Generative Process</h2> <p>Given a dataset of \(D\) documents, a vocabulary \(V\), and a set of \(K\) topics, the LDA generative process to create a document \(d\) is as follows:</p> <ol> <li> <p>Define a prior distribution over the topic proportions in \(d\) from a Dirichlet distribution with parameter \(\alpha\):</p> \[\theta_d \sim \text{Dir}(\alpha)\] </li> <li> <p>Define a global prior distribution over the word proportions in each topic from a Dirichlet distribution with parameter $\beta$:</p> \[\phi_k \sim \text{Dir}(\beta)\] </li> <li> <p>while not done:</p> <ol> <li> <p>Sample a topic \(z\) from the topic proportions in \(d\):</p> \[z \sim \text{Multinomial}(\theta_d)\] </li> <li> <p>Sample a word \(w\) from the word proportions in \(z\):</p> \[w \sim \text{Multinomial}(\phi_z)\] </li> <li> <p>Stop with probability \(\epsilon\) or continue.</p> </li> </ol> </li> </ol> <p>We often use the plate notation to represent the generative process. The plate notation for LDA is shown below:</p> <p><img src="/assets/img/blog/2023-12-27-latent-dirichlet/plate.png" alt="Plate notation for LDA"></p> <h2 id="inference">Inference</h2> <p>First and foremost, the parameters that we wish to infer in this model are the topic proportions \(\theta_d\) and the word proportions \(\phi_k\). Notice that these two variables are sufficient to define the entire generative process. In other words, if we know the topic proportions \(\theta\) and the word proportions \(\phi\) that truly generated the dataset, then we can generate new documents that are indistinguishable from the original dataset. Since we don’t know the true values of \(\theta\) and \(\phi\), we must infer them from the observed documents $d$ using maximum likelihood estimation of the evidence with respect to these parameters:</p> \[\begin{align*} \theta^*, \phi^* &amp;= \arg\max_{\theta, \phi} \log p(d | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d, z | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d | z, \theta, \phi) p(z | \theta, \phi) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z p(d | z, \theta, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \log \sum_z \prod_{n=1}^N p(w_n | z, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \log \prod_{n=1}^N \sum_z p(w_n | z, \phi) p(z | \theta) \\ &amp;= \arg\max_{\theta, \phi} \sum_{n=1}^N \log \sum_z p(w_n | z, \phi) p(z | \theta) \end{align*}\] <p>Unfortunately, this likelihood function is intractable to optimize directly, because we cannot marginalize over all the possible latent topic assignments \(z\) for each word \(w_n\). Instead, to tackle the partition function \(p(d \| \theta, \phi)\), we must turn to approximate inference methods such as <a href="/blog/2023/variational-inf-1/">variational inference</a> which we introduced in a previous post. Although VI still works here, we will use an approach from the Monte Carlo Markov Chain (MCMC) family of methods called Gibbs sampling<d-cite key="gibbs1992"></d-cite>.</p> <p>The idea behind Gibbs sampling<d-cite key="gibbs1992"></d-cite> is pretty straightforward. In cases where marginalizing the joint distribution \(p(z_1, w_1, z_2, w_2, \dots, z_n, w_n \vert \theta, \phi)\) over the latents \(z\) is intractable, we can instead sample individual latents from their conditional distributions \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\), use these sampled latents to update the parameters \(\theta\) and \(\phi\) to their new expected values, and repeat this process until convergence.</p> <p>In the case of LDA, we iteratively sample the latent topic assignments \(z_{i,d}\) for each word \(w_{i,d}\) in document \(d\), and then update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. The conditional distribution for \(z_{i,d}\) is given by:</p> \[\begin{align*} p(z_{i,d} &amp;= k \vert z_{(j,e) \neq (i,d)}, w_{i,d}=v, \theta, \phi) \\ &amp;\propto (\alpha_k + n_{(\cdot,d,k) \neq (v,d,\cdot)})\frac{\beta + n_{(v,\cdot,k) \neq (v,d,\cdot)}}{\sum_w \beta + n_{(w,\cdot,k) \neq (v,d,\cdot)}} \end{align*}\] <p>Here, \(n_{(w,d,k)}\) is the number of times that the word \(w \in V\) in document \(d\) is assigned to topic \(k\). Thus, \(n_{(\cdot,d,k) \neq (v,d,\cdot)}\) is the number of words in document \(d\) that are assigned to topic \(k\), excluding counts of the current word \(w_{i,d}=v\). Similarly, \(n_{(v,\cdot,k) \neq (v,d,\cdot)}\) is the number of times that word \(v\) is assigned to topic \(k\) in all documents, excluding the occurrences of \(w_{i,d}=v\) in document \(d\). Finally, \(\sum_w \beta + n_{(w,\cdot,k) \neq (v,d,\cdot)}\) is the total number of words in the vocabulary \(V\) that are assigned to topic \(k\), excluding the occurrences of \(w_{i,d}=v\) in document \(d\).</p> <p>After sampling the topic assignments \(z_{i,d}\) for each word \(w_{i,d}\) in document \(d\), we can update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values as follows:</p> \[\begin{align*} \theta_{d,k} &amp;= \frac{\alpha_k + n_{(\cdot,d,k)}}{\sum_{j=1}^K \alpha_j + n_{(\cdot,d,j)}} \\ \phi_{k,v} &amp;= \frac{\beta + n_{(v,\cdot,k)}}{\sum_{w=1}^V \beta_w + n_{(w,\cdot,k)}} \end{align*}\] <p>I do not want to claim falsehoods on my blog by deriving these equations myself, so I will refer you to this great <a href="https://miningthedetails.com/LDA_Inference_Book/lda-inference.html" rel="external nofollow noopener" target="_blank">resource</a> by Chris Tufts for the full derivation.</p> <h2 id="implementation">Implementation</h2> <p>Now onto the fun part! Let’s implement LDA from scratch in Python. I’ll be using a stripped down version of the MIMIC-III dataset<d-cite key="pmid27219127"></d-cite>, which is a collection of de-identified medical records from patients admitted to the intensive care unit (ICU). My version contains only the ICD9 codes<d-cite key="pmid27219127"></d-cite> of timestamped diagnoses for 689 patients with a vocabulary size of 389 unique codes. The dataset is available <a href="https://github.com/TheMatrixMaster/lda-model/tree/main/data" rel="external nofollow noopener" target="_blank">here</a></p> <p>First, we’ll load the dataset and process it into the desired input format for our model which is a list of lists of strings where each sublist represents a document (see patient) and each string represents a word (see ICD code) in the document.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Format docs into the desired format list of lists
</span><span class="n">docs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">"</span><span class="s">data/MIMIC3_DIAGNOSES_ICD_subset.csv.gz</span><span class="sh">"</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">SUBJECT_ID</span><span class="sh">'</span><span class="p">])</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">.</span><span class="nf">groupby</span><span class="p">(</span><span class="sh">'</span><span class="s">SUBJECT_ID</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">].</span><span class="nf">apply</span><span class="p">(</span><span class="nb">list</span><span class="p">).</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">)</span>
<span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span><span class="p">[</span><span class="sh">'</span><span class="s">ICD9_CODE</span><span class="sh">'</span><span class="p">].</span><span class="nf">tolist</span><span class="p">()</span>
</code></pre></div></div> <p>Next, I setup a class to handle manipulations with the latent conditional distribution \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\) which we derived above. This class is initialized with the initial parameters \(\alpha\) and \(\beta\) for the dirichlet priors, the number of topics \(K\), and the topic assignment counts matrix \(n_{(w,d,k)}\) which is a 3D tensor of shape \(V \times D \times K\) where \(V\) is the vocabulary size, \(D\) is the number of documents, and \(K\) is the number of topics.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LatentDistribution</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">alpha</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>   <span class="c1"># 1d array holding alpha hyperparams a_{k}
</span>    <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span>         <span class="c1"># beta hyperparam
</span>    <span class="n">n_mat</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>   <span class="c1"># 3d array holding n_{k,d,w}
</span>    <span class="n">K</span><span class="p">:</span> <span class="nb">int</span>              <span class="c1"># number of topics
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">n_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span> <span class="o">=</span> <span class="n">n_mat</span>

        <span class="k">assert</span> <span class="n">n_mat</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span>
        <span class="k">assert</span> <span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">K</span>

        <span class="n">self</span><span class="p">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span> <span class="k">if</span> <span class="n">alpha</span> <span class="ow">is</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">alpha</span>
</code></pre></div></div> <p>This class contains methods to sample the conditional distribution \(p(z_i \vert z_{j\neq i}, x_i, \theta, \phi)\) and to update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. The sampling method is implemented as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">alpha_dw</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">n_dk</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>
    <span class="n">n_wk</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,</span><span class="n">w</span><span class="p">].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>

    <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:].</span><span class="nf">sum</span><span class="p">()</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span>
    
    <span class="nf">return </span><span class="p">(</span><span class="n">alpha_dw</span> <span class="o">+</span> <span class="n">n_dk</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">n_wk</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">n_k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">pmf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">g_k</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">K</span><span class="p">)])</span>
    <span class="k">return</span> <span class="n">g_k</span><span class="o">/</span><span class="n">g_k</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span>
    
<span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span> 
    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)[</span><span class="n">k</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">pmf</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">pmf</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pmf</span><span class="p">).</span><span class="nf">argmax</span><span class="p">()</span>
</code></pre></div></div> <p>The update method is implemented as follows. We also have helper methods to get the current expected values of \(\theta\) and \(\phi\).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_n</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">get_phi</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">n_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,:].</span><span class="nf">sum</span><span class="p">()</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,:,</span><span class="n">w</span><span class="p">].</span><span class="nf">sum</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">beta</span><span class="o">*</span><span class="n">V</span> <span class="o">+</span> <span class="n">n_k</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">get_theta</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
    <span class="n">n_d</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[:,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">()</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,:].</span><span class="nf">sum</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">alpha</span><span class="p">.</span><span class="nf">sum</span><span class="p">()</span> <span class="o">+</span> <span class="n">n_d</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we are ready to implement the iterative Gibbs inference algorithm. In the class init, we initialize the topic assignment counts matrix \(n_{(w,d,k)}\) to zeros, and instantiate a <code class="language-plaintext highlighter-rouge">LatentDistribution</code> instance with the correct initial parameters. We also create a vocabulary object which maps each string word to a unique integer index.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">LDA</span><span class="p">():</span>
    <span class="n">k</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of topics
</span>    <span class="n">d</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of documents
</span>    <span class="n">w</span><span class="p">:</span> <span class="nb">int</span>                              <span class="c1"># number of words in vocabulary
</span>    <span class="n">vocab</span><span class="p">:</span> <span class="n">defaultdict</span>                  <span class="c1"># vocabulary mapping words to indices
</span>    <span class="n">r_vocab</span><span class="p">:</span> <span class="n">defaultdict</span>                <span class="c1"># reverse vocabulary mapping indices to words
</span>    <span class="n">docs</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span>                    <span class="c1"># 2d list holding documents with raw words
</span>    <span class="n">n_iter</span><span class="p">:</span> <span class="nb">int</span>                         <span class="c1"># number of iterations
</span>    <span class="n">latent_dist</span><span class="p">:</span> <span class="n">LatentDistribution</span>     <span class="c1"># latent distribution Z
</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">docs</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">docs</span> <span class="o">=</span> <span class="n">docs</span>
        <span class="n">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">setup_vocab</span><span class="p">(</span><span class="n">docs</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">r_vocab</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">(</span><span class="nf">map</span><span class="p">(</span><span class="nb">reversed</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">.</span><span class="nf">items</span><span class="p">()))</span>
        <span class="n">self</span><span class="p">.</span><span class="n">w</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

        <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">,</span><span class="n">self</span><span class="p">.</span><span class="n">w</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span> <span class="o">=</span> <span class="nc">LatentDistribution</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">n_mat</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_vocab</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">docs</span><span class="p">):</span>
        <span class="n">vocab</span> <span class="o">=</span> <span class="nf">defaultdict</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">docs</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">doc</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">:</span>
                    <span class="n">vocab</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">vocab</span>
</code></pre></div></div> <p>In the fitting method, as explained above, we iterate over each word in each document and sample a new topic assignment \(z_{i,d}\) for that word from the conditional distribution. We then update the topic proportions \(\theta\) and word proportions \(\phi\) to their expected values. We repeat this process for the specified number of iterations.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">n_iter</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">d</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">docs</span><span class="p">[</span><span class="n">d</span><span class="p">])):</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">vocab</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">docs</span><span class="p">[</span><span class="n">d</span><span class="p">][</span><span class="n">n</span><span class="p">]]</span>
                <span class="n">k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">latent_dist</span><span class="p">.</span><span class="nf">update_n</span><span class="p">(</span><span class="n">k</span><span class="p">,</span><span class="n">d</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
</code></pre></div></div> <h2 id="results">Results</h2> <p>Now that we have implemented the LDA model, let’s see what topics it discovers in the MIMIC-III dataset. Given the final topic proportions \(\theta\) and word proportions \(\phi\), we can get easily get the top words for each topic and the top documents for each topic by sorting the rows and columns of \(\phi\) and \(\theta\) respectively. We can also correlate the topics with certain sets of keywords to see if the learned topics take on the meaning that we expect. In my dataset, I correlate the topics with 3 key ICD9 categories: Alzheimer’s disease, Parkinson disease, and Multiple Sclerosis. The results are linked below:</p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/top_words.png" rel="external nofollow noopener" target="_blank">Top words for each topic</a></p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/top_docs.png" rel="external nofollow noopener" target="_blank">Top 100 documents for each topic</a></p> <p><a href="https://raw.githubusercontent.com/TheMatrixMaster/lda-model/main/results/word_topic_corr.png" rel="external nofollow noopener" target="_blank">Correlation of topics with key ICD9 categories</a></p> <p>For the full-code and more results, check out my <a href="https://github.com/TheMatrixMaster/lda-model/tree/main" rel="external nofollow noopener" target="_blank">GitHub repo</a></p> <h2 id="conclusion">Conclusion</h2> <p>In the next post on topic models, I will tackle the older brother of LDA, the embedded topic model (ETM)<d-cite key="dieng2019etm"></d-cite> which is a neural network based topic model that learns the topic proportions \(\theta\) and word proportions \(\phi\) in an end-to-end fashion in a shared word-topic latent space. Stay tuned!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-12-27-latent-dirichlet.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Stephen Z. Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 19, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYZ3B9SH7B"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZYZ3B9SH7B");</script> </body> </html>