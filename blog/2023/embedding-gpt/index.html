<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Semantic Embedding w/ ChatGPT | Stephen Z. Lu</title> <meta name="author" content="Stephen Z. Lu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/sl-icon-dark.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://thematrixmaster.github.io/blog/2023/embedding-gpt/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Semantic Embedding w/ ChatGPT",
      "description": "",
      "published": "August 23, 2023",
      "authors": [
        {
          "author": "Stephen Lu",
          "authorURL": "https://matrixmaster.me",
          "affiliations": [
            {
              "name": "McGill University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Stephen </span>Z. Lu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">cv</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/assets/pdf/cv.pdf">Download</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Semantic Embedding w/ ChatGPT</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="motivation">Motivation</h2> <p>Ever since the initial release of ChatGPT in November 2022, large language models have rapidly taken the spotlight of machine learning applications in industry. Leaders like Anthropic and Cohere have reached the unicorn status in the blink of an eye followed by a swarm of startups trying to apply <strong>ChatGPT to X</strong>. Like any other groundbreaking technology, large language models will take some time to fully integrate into our society, but it is undeniably something that is here to stay for the long term.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-08-23-embedding-gpt/funding-1400.webp"></source> <img src="/assets/img/blog/2023-08-23-embedding-gpt/funding.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Funding for generative AI has shot up by over 7 fold in the first half of 2023 (Image source: CB Insights, 2023) </div> <p>Given that progress is being made at blinding speed, I wanted to try out some of the awesome new tools being developed and write a tutorial on how to harness gpt-style models to your own use cases. Before we dive into the code, I want to briefly do an overview of the large language model architecture then explain the two main approaches being used to customize these models: finetuning versus embedding.</p> <h2 id="transformers">Transformers</h2> <p>At a very high level, large language models use a novel neural network architecture called transformers<d-cite key="vaswani2023attention"></d-cite> to generate the best responses to your prompt. You can think of transformers as extremely powerful sentence completion models that have <strong>efficiently</strong> learnt the underlying semantic patterns of written language to the extent that they can sample continuations to a textual prompt that are semantically meaningful. For an in depth understanding of the transformer architecture, I recommend this <a href="https://jalammar.github.io/illustrated-transformer/" rel="external nofollow noopener" target="_blank">blog post</a> by Jay Alammar. ChatGPT is an augmented version of the transformer model that was finetuned with reinforcement learning to better align with human instructions. Whether in the form of q&amp;a or simply listening to instructions, ChatGPT seems <strong>smart</strong> because it has been trained very well to align with human intent.<d-cite key="ouyang2022training"></d-cite></p> <h2 id="embedding-versus-finetuning">Embedding versus Finetuning</h2> <p>Although large language models are trained on data from the entire internet, the goal of these models is to learn the semantic patterns in language, not to memorize all the information on the internet. When we ask ChatGPT to answer a factual question like <em>“Which country won the most gold medals at the 2012 olympics?”</em>, the model does not have this information embedded in its finite set of parameters. Instead, it will perform a semantic search <d-footnote>You can think of this like a Google search</d-footnote> and insert the raw information it needs to answer our question into its context. This technique is known as <strong>semantic embedding</strong> and we can also use it to help ChatGPT access information that is relevant to our application domain. For example, if we want to build a personal gpt assistant that can answer relevant questions about all my previous tax returns, then I can put these relevant text documents into a database and allow a large language model to query it before answering my questions pertaining to this topic. As you can see, semantic embeddings don’t modify the underlying large language model at all. It just fetches the relevant information to better align the model’s answer with the users prompt.</p> <p>Finetuning on the other hand does modify the weights of the model. We already gave an example of finetuning earlier when we described how ChatGPT was trained downstream with reinforcement learning to better align with human instructions. Thus, it can be understood that the weights of the model encode something related to the built-in biases and choices made by the model when instructions are open to interpretation.</p> <p>A better way to explain this is by drawing a parallel with human behaviour. Humans are extremely good at following instructions, especially when these instructions are very detailed and leave little to no room for interpretation. However, when we are given vague instructions or open-ended questions, the responses that we provide reflect our personal opinions and biases. Even in the case when the possible responses are relatively constrained, biases are still present in low-level constructs such as sentence structure, word choice, and semantic sentiment. For example, although two doctors might give you the same diagnosis and prescription, the way they convey that information, the amount of detail they provide, and the tone they convey could be drastically different.</p> <p>The same can be said for large language models when they are prompted with tasks that are open to interpretation. If we want a model that consistently and reliably provides responses in a particular tone with a particular set of predetermined biases, then it makes more sense to finetune the model so that it aligns its decisions with this set of priors. For example, Claude is the flagship llm by Anthropic that is finetuned using a reinforcement learning policy aligned with a set of <a href="https://www.anthropic.com/index/claudes-constitution" rel="external nofollow noopener" target="_blank">constitutional principles</a> such as being <em>ethical</em>, <em>virtuous</em>, etc. In essense, finetuning can be thought of as introducing prior preferences into the llm that guide its responses on top of the fundamental prior that the model should be aligned with the user intent.</p> <h2 id="implementing-a-conversational-retrieval-qa-agent-in-langchain">Implementing a Conversational Retrieval Q&amp;A agent in LangChain</h2> <p>In the following tutorial, I will implement a conversational Q&amp;A agent such as ChatGPT that has access to the standard chat history context as well as a database of private documents that can be used for <strong>semantic embedding</strong> search. I will address model finetuning in a future blog post.</p> <h3 id="setting-up-a-zep-server">Setting up a Zep server</h3> <p>Recall that semantic embedding search involves searching for relevant documents in a database so that we can inject relevant information into the context of the model before providing a response. In our implementation, we will use <a href="https://github.com/getzep/zep" rel="external nofollow noopener" target="_blank">Zep</a> which is a long term RAM memory store that is optimized for storing text embeddings and performing various search algorithms over these embeddings.</p> <p>You can follow these instructions on their <a href="https://docs.getzep.com/deployment/quickstart/#starting-a-zep-server-locally-is-simple" rel="external nofollow noopener" target="_blank">getting started</a> page to start your Zep server. It is as easy as setting up some <code class="language-plaintext highlighter-rouge">API_KEY</code> environment variables and running a <code class="language-plaintext highlighter-rouge">docker-compose up</code> command.</p> <h3 id="adding-documents-to-the-zep-server">Adding documents to the Zep server</h3> <p>Now that the Zep database is up and running, we want to add our document embeddings into the database, so that we can perform retrieval against them. To do this, I wrote a python seeder script, but Zep also supports an SDK in native JavaScript as well as integration with 3rd party libraries such as LangChain and LlamaIndex which you can use to interact with the database as well.</p> <h4 id="creating-a-collection">Creating a collection</h4> <p>Now, the first step is to connect to the database and create a collection. Zep organizes its data in the following hierarchy: <strong>collection &gt; documents &gt; embeddings</strong>. Going back to the tax returns example, we could think of a document as individual tax related files, while a collection might group all such files for a particular year. Zep takes our documents and does the embedding for us, so we need to provide an embedding model. Here I choose the <code class="language-plaintext highlighter-rouge">text-embedding-ada-002</code> model by OpenAI since we will be using ChatGPT for our llm.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">zep_python</span> <span class="kn">import</span> <span class="n">ZepClient</span>
<span class="kn">from</span> <span class="n">zep_python.document</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">zep_api_url</span> <span class="o">=</span> <span class="sh">"</span><span class="s">http://localhost:8000</span><span class="sh">"</span>

<span class="n">client</span> <span class="o">=</span> <span class="nc">ZepClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="n">zep_api_url</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
Step 1: Create a new collection in which to store our documents for a given task class
        We will use chatgpt text-embedding-ada-002 model which has embedding size of 1536
</span><span class="sh">"""</span>

<span class="n">collection_name</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;name the collection&gt;</span><span class="sh">"</span>

<span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">delete_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
    <span class="n">collection</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">add_collection</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">collection_name</span><span class="p">,</span>  <span class="c1"># required
</span>        <span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">&lt;some description&gt;</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional
</span>        <span class="n">embedding_dimensions</span><span class="o">=</span><span class="mi">1536</span><span class="p">,</span>  <span class="c1"># this must match the model you've configured for 
</span>        <span class="n">is_auto_embedded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># use Zep's built-in embedder. Defaults to True
</span>    <span class="p">)</span>
<span class="k">except</span> <span class="nb">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
</code></pre></div></div> <h4 id="chunking-up-documents">Chunking up documents</h4> <p>Our documents can have a large variation in their length and content size, so it doesn’t really make sense to squeeze each document into a fixed size embedding. Instead, we split up documents into fixed size chunks that have a predetermined token length, then embed each chunk into a vector embedding. Here I split my documents into chunks with a maximum of 1600 tokens per chunk, but this process will heavily depend on the nature and format of your documents. The code I provide below is just an example of how this chunking might be done, but you should write your own routine for this.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DOC_DIR</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents</span><span class="sh">"</span>
<span class="n">FILE_NAME</span> <span class="o">=</span> <span class="sh">"</span><span class="s">documents/raw_convo.txt</span><span class="sh">"</span>

<span class="c1"># Custom splitting for .txt file such that each entry in qa_data is a tuple of ([questions], answer)
# TODO: Add support for csv, json, and yml files
</span><span class="n">sections</span> <span class="o">=</span> <span class="nf">split_into_sections</span><span class="p">(</span><span class="n">FILE_NAME</span><span class="p">)</span>
<span class="n">qa_sections</span> <span class="o">=</span> <span class="nf">split_into_qa_pairs</span><span class="p">(</span><span class="n">sections</span><span class="p">)</span>
<span class="n">qa_data</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">section</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">qa_sections</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">questions</span><span class="p">,</span> <span class="n">answer</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">qa_data</span><span class="p">.</span><span class="nf">append</span><span class="p">((</span><span class="n">questions</span><span class="p">,</span> <span class="n">answer</span><span class="p">))</span>

<span class="c1"># Split the qa pairs into chunks with a predefined max token length
</span><span class="n">MAX_TOKENS</span> <span class="o">=</span> <span class="mi">1600</span>
<span class="n">qa_strings</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">section</span> <span class="ow">in</span> <span class="n">qa_data</span><span class="p">:</span>
    <span class="n">qa_strings</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">split_strings_from_subsection</span><span class="p">(</span><span class="n">section</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="n">MAX_TOKENS</span><span class="p">))</span>
</code></pre></div></div> <h4 id="embedding-chunks-into-zep">Embedding chunks into Zep</h4> <p>The last step is to embed the chunks into Zep using the <code class="language-plaintext highlighter-rouge">Document</code> class. Here we could choose to add metadata to each chunk to identify, for example, which original file it belongs to. These metadata can then serve as future <em>filters</em> when we search against the Zep database.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 3: Embed the document chunks and store them into the collection
</span><span class="sh">"""</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">Document</span><span class="p">(</span>
        <span class="n">content</span><span class="o">=</span><span class="n">chunk</span><span class="p">,</span>
        <span class="n">document_id</span><span class="o">=</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="s">-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="sh">"</span><span class="p">,</span>  <span class="c1"># optional document ID
</span>        <span class="n">metadata</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">bar</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="p">},</span>  <span class="c1"># optional metadata
</span>    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">qa_strings</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">uuids</span> <span class="o">=</span> <span class="n">collection</span><span class="p">.</span><span class="nf">add_documents</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span>
</code></pre></div></div> <p>Finally, we can also spin up a busy waiting watcher process that waits for the documents to be embedded before exiting.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="sh">"""</span><span class="s">
Step 4: Wait for the documents to be embedded and monitor the process
</span><span class="sh">"""</span>
<span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">document</span><span class="p">.</span><span class="nf">get_collection</span><span class="p">(</span><span class="n">collection_name</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">Embedding status: </span><span class="sh">"</span>
        <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_embedded_count</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">document_count</span><span class="si">}</span><span class="s"> documents embedded</span><span class="sh">"</span>
    <span class="p">)</span>
    <span class="n">time</span><span class="p">.</span><span class="nf">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">status</span> <span class="o">==</span> <span class="sh">"</span><span class="s">ready</span><span class="sh">"</span><span class="p">:</span>
        <span class="k">break</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Added </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">uuids</span><span class="p">)</span><span class="si">}</span><span class="s"> documents to collection </span><span class="si">{</span><span class="n">collection_name</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="giving-chatgpt-access-to-the-zep-database">Giving ChatGPT access to the Zep database</h3> <p>Now that we have our vector database ready to go, we just need to hook up a language model to query from it and add the relevant embeddings to its context before answering the user. I used <a href="https://github.com/hwchase17/langchainjs" rel="external nofollow noopener" target="_blank">LangChain</a> which is an awesome framework that enables easy interaction with popular llms as well as integration with 3rd party plugins and databases such as Zep. Using the LangChain JavaScript SDK, I simply need to do the following steps:</p> <ol> <li>Connect to the Zep database</li> <li>Retrieve the user’s chat history along with his current active prompt</li> <li>Embed the chat history along with the current active prompt use as a prototype search vector</li> <li>Use the search vector to find semantically related embeddings in the Zep database</li> <li>Feed all the relevant embeddings from Zep and the chat context to an instance of ChatGPT model</li> <li>Return the model response to the user</li> </ol> <p>Using SvelteKit server module, this can be done in very few lines of code.</p> <div class="language-typescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">import</span> <span class="p">{</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span> <span class="nx">OPENAI_ORGANIZATION</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">$env/static/private</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ChatOpenAI</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chat_models/openai</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">ConversationalRetrievalQAChain</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/chains</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">ZepVectorStore</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/vectorstores/zep</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">FakeEmbeddings</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/embeddings/fake</span><span class="dl">"</span><span class="p">;</span>
<span class="k">import</span> <span class="p">{</span> <span class="nx">BufferMemory</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">"</span><span class="s2">langchain/memory</span><span class="dl">"</span><span class="p">;</span>

<span class="k">import</span> <span class="p">{</span> <span class="nx">error</span> <span class="p">}</span> <span class="k">from</span> <span class="dl">'</span><span class="s1">@sveltejs/kit</span><span class="dl">'</span><span class="p">;</span>

<span class="k">export</span> <span class="kd">type</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="p">{</span> 
    <span class="na">question</span><span class="p">:</span> <span class="kr">string</span><span class="p">;</span>
    <span class="nl">settings</span><span class="p">:</span> <span class="p">{</span> <span class="na">temperature</span><span class="p">:</span> <span class="kr">number</span><span class="p">,</span> <span class="na">relatedness</span><span class="p">:</span> <span class="kr">number</span> <span class="p">};</span>
<span class="p">}</span>

<span class="kd">const</span> <span class="nx">zepConfig</span> <span class="o">=</span> <span class="p">{</span>
    <span class="na">apiUrl</span><span class="p">:</span> <span class="dl">"</span><span class="s2">http://localhost:8000</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// the URL of your Zep implementation</span>
    <span class="na">collectionName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">&lt;collection_name&gt;</span><span class="dl">"</span><span class="p">,</span>  <span class="c1">// the name of your collection. alphanum values only</span>
    <span class="na">embeddingDimensions</span><span class="p">:</span> <span class="mi">1536</span><span class="p">,</span>  <span class="c1">// much match the embeddings you're using</span>
    <span class="na">isAutoEmbedded</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>  <span class="c1">// will automatically embed documents when they are added</span>
<span class="p">};</span>

<span class="kd">const</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span> <span class="o">=</span> <span class="s2">`
    Given the following conversation and a follow up question,
    return the conversation history excerpt that includes any relevant context to the question
    if it exists and rephrase the follow up question to be a standalone question.
    
    Chat History: {chat_history}
    Follow Up Input: {question}
    
    Your answer should follow the following format:

    </span><span class="se">\`\`\`</span><span class="s2">
    &lt;Here you can give some additional behavioural instructions to the model in the form of prompting. The result will not be as good as finetuning the model on a large amount of
    examples that properly introduce a set of behavioural guidelines for the model to respect.&gt;
    ----------------
    &lt;Relevant chat history excerpt as context here&gt;
    Standalone question: &lt;Rephrased question here&gt;
    </span><span class="se">\`\`\`</span><span class="s2">

    Your answer:
`</span><span class="p">;</span>

<span class="kd">const</span> <span class="nx">embeddings</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">FakeEmbeddings</span><span class="p">();</span>

<span class="k">export</span> <span class="kd">const</span> <span class="nx">POST</span> <span class="o">=</span> <span class="k">async </span><span class="p">({</span> <span class="nx">request</span> <span class="p">})</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="kd">const</span> <span class="na">body</span><span class="p">:</span> <span class="nx">MessageBody</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">request</span><span class="p">.</span><span class="nf">json</span><span class="p">();</span>

    <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">body</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Data</span><span class="dl">'</span><span class="p">);</span>

    <span class="c1">// Connect to the Zep vector store server</span>
    <span class="kd">const</span> <span class="nx">vectorStore</span> <span class="o">=</span> <span class="k">await</span> <span class="k">new</span> <span class="nc">ZepVectorStore</span><span class="p">(</span><span class="nx">embeddings</span><span class="p">,</span> <span class="nx">zepConfig</span><span class="p">);</span>

    <span class="c1">// Create a new readable stream of the chat response</span>
    <span class="kd">const</span> <span class="nx">readableStream</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ReadableStream</span><span class="p">({</span>
        <span class="k">async</span> <span class="nf">start</span><span class="p">(</span><span class="nx">controller</span><span class="p">)</span> <span class="p">{</span>
            <span class="c1">// Create a new chat model</span>
            <span class="kd">const</span> <span class="nx">streamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-4</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">streaming</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
                <span class="na">callbacks</span><span class="p">:</span> <span class="p">[{</span>
                    <span class="na">handleLLMNewToken</span><span class="p">:</span> <span class="k">async </span><span class="p">(</span><span class="na">token</span><span class="p">:</span> <span class="kr">string</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="nx">controller</span><span class="p">.</span><span class="nf">enqueue</span><span class="p">(</span><span class="nx">token</span><span class="p">),</span>
                <span class="p">}]</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">nonStreamingModel</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">ChatOpenAI</span><span class="p">({</span>
                <span class="na">openAIApiKey</span><span class="p">:</span> <span class="nx">OPENAI_API_KEY</span><span class="p">,</span>
                <span class="na">modelName</span><span class="p">:</span> <span class="dl">"</span><span class="s2">gpt-3.5-turbo</span><span class="dl">"</span><span class="p">,</span>
                <span class="na">temperature</span><span class="p">:</span> <span class="nx">body</span><span class="p">.</span><span class="nx">settings</span><span class="p">.</span><span class="nx">temperature</span><span class="p">,</span>
            <span class="p">},</span> <span class="p">{</span>
                <span class="na">organization</span><span class="p">:</span> <span class="nx">OPENAI_ORGANIZATION</span><span class="p">,</span>
            <span class="p">});</span>

            <span class="kd">const</span> <span class="nx">chain</span> <span class="o">=</span> <span class="nx">ConversationalRetrievalQAChain</span><span class="p">.</span><span class="nf">fromLLM</span><span class="p">(</span>
                <span class="nx">streamingModel</span><span class="p">,</span>
                <span class="nx">vectorStore</span><span class="p">.</span><span class="nf">asRetriever</span><span class="p">(),</span>
                <span class="p">{</span>
                    <span class="na">memory</span><span class="p">:</span> <span class="k">new</span> <span class="nc">BufferMemory</span><span class="p">({</span>
                        <span class="na">memoryKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">chat_history</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// Must be set to "chat_history"</span>
                        <span class="na">inputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">question</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the input to the chain</span>
                        <span class="na">outputKey</span><span class="p">:</span> <span class="dl">"</span><span class="s2">text</span><span class="dl">"</span><span class="p">,</span> <span class="c1">// The key for the final conversational output of the chain</span>
                        <span class="na">returnMessages</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span> <span class="c1">// If using with a chat model</span>
                    <span class="p">}),</span>
                    <span class="na">returnSourceDocuments</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
                    <span class="na">questionGeneratorChainOptions</span><span class="p">:</span> <span class="p">{</span>
                        <span class="na">llm</span><span class="p">:</span> <span class="nx">nonStreamingModel</span><span class="p">,</span>
                        <span class="na">template</span><span class="p">:</span> <span class="nx">CUSTOM_QUESTION_GENERATOR_CHAIN_PROMPT</span>
                    <span class="p">},</span>
                <span class="p">},</span>
            <span class="p">);</span>

            <span class="kd">const</span> <span class="nx">question</span> <span class="o">=</span> <span class="nx">body</span><span class="p">.</span><span class="nx">question</span><span class="p">;</span>
            <span class="k">if </span><span class="p">(</span><span class="o">!</span><span class="nx">question</span><span class="p">)</span> <span class="k">throw</span> <span class="nf">error</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="dl">'</span><span class="s1">Missing Question</span><span class="dl">'</span><span class="p">);</span>

            <span class="kd">const</span> <span class="nx">resp</span> <span class="o">=</span> <span class="k">await</span> <span class="nx">chain</span><span class="p">.</span><span class="nf">call</span><span class="p">({</span> <span class="nx">question</span> <span class="p">});</span>
            <span class="nx">controller</span><span class="p">.</span><span class="nf">close</span><span class="p">();</span>
        <span class="p">},</span>
    <span class="p">});</span>

    <span class="c1">// Create and return a response of the readable stream</span>
    <span class="k">return</span> <span class="k">new</span> <span class="nc">Response</span><span class="p">(</span><span class="nx">readableStream</span><span class="p">,</span> <span class="p">{</span>
        <span class="na">headers</span><span class="p">:</span> <span class="p">{</span>
            <span class="dl">'</span><span class="s1">Content-Type</span><span class="dl">'</span><span class="p">:</span> <span class="dl">'</span><span class="s1">text/plain</span><span class="dl">'</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">});</span>
<span class="p">}</span>
</code></pre></div></div> <p>For more information on this script, please visit the following <a href="https://js.langchain.com/docs/modules/chains/popular/chat_vector_db" rel="external nofollow noopener" target="_blank">documentation</a>. The full code for my walkthrough can be found <a href="https://github.com/TheMatrixMaster/gpt-embeddings" rel="external nofollow noopener" target="_blank">here</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-08-23-embedding-gpt.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Stephen Z. Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 03, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYZ3B9SH7B"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZYZ3B9SH7B");</script> </body> </html>