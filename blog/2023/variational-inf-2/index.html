<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Variational Inference w/ EM algorithm (Part 2) | Stephen Z. Lu</title> <meta name="author" content="Stephen Z. Lu"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo/sl-icon-dark.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://thematrixmaster.github.io/blog/2023/variational-inf-2/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "Variational Inference w/ EM algorithm (Part 2)",
      "description": "",
      "published": "October 28, 2023",
      "authors": [
        {
          "author": "Stephen Lu",
          "authorURL": "https://matrixmaster.me",
          "affiliations": [
            {
              "name": "McGill University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Stephen </span>Z. Lu</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">cv</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/assets/pdf/cv.pdf">Download</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Variational Inference w/ EM algorithm (Part 2)</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="motivation">Motivation</h2> <p>In the last post of this two-part series, I introduced the theory behind mean-field variational inference. In this post, I’ll walk through my implementation of mean-field VI on the task of polygenic risk score (PRS) regression with spike-and-slab prior.</p> <h3 id="what-is-prs">What is PRS?</h3> <p>To understand polygenic risk score prediction, we first have to introduce the concept of genome-wide association studies (GWAS). In a GWAS, we are given a dataset of \(N\) individuals, each with a set of \(M\) genetic variants (SNPs) and a binary phenotype (e.g. disease status). The goal of GWAS is to identify which SNPs are associated with the phenotype, and to quantify the strength of these associations. In other words, we want to find the SNPs that are statistically significant, and to estimate the effect size \(\beta_i\) of each SNP \(i\) on the phenotype \(y\).</p> <p>Typically, we use a linear model to fit the data, where the phenotype \(y\) is a linear combination of the SNPs \(x\), with some noise \(\epsilon\):</p> \[y = \beta_0 + \sum_{i=1}^M \beta_i x_i + \epsilon\] <p>We can then use \(\beta_i\) to quantify the strength of the association between SNP \(i\) and the phenotype \(y\). After doing some statistical tests on \(\beta_i\), we typically end up with a list of SNPs that are statistically significant and we can obtain nice Manhattan plots like this:</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/manhattan-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/manhattan.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Manhattan plot of a GWAS study investigating kidney stone disease, so the peaks indicate genetic variants that are found more often in individuals with kidney stones (Source: Howles et a. 2019) </div> <p>Naturally, a good follow-up question is: how well can we predict the phenotype \(y\) given the SNPs \(x\)? This is where polygenic risk score (PRS) prediction comes in. The idea is to use the estimated effect sizes \(\beta_i\) to compute a weighted sum of the SNPs \(x\), which we call the polygenic risk score \(s\). These scores can inform us about the genetic risk of an individual developing a disease, and can be used to predict the phenotype \(y\).</p> <p>However, there are a few issues with this approach. First, the effect sizes \(\beta_i\) are estimated from a linear model, which assumes that the phenotype \(y\) is a linear combination of the SNPs \(x\). This is not always the case, since the phenotype \(y\) is often a non-linear function of the SNPs \(x\), like in epistasis. Second, the linear model assumes independence between the SNPs \(x\), the phenotype \(y\), and the noise \(\epsilon\), which is not necessarily true. These assumptions can lead to poor predictive performance of the PRS.</p> <p>Given that the effect sizes \(\beta\) predicted by the linear model are noisy as explained above, we can use Bayesian inference to inject our prior domain knowledge into the model. If the prior is meaningful, then the new posterior estimate of \(\beta\) should be more accurate than the original linear estimates, leading to better predictive performance of the PRS.</p> <p>In this post, we will use mean-field variational inference to estimate the posterior distribution of \(\beta\) with a spike-and-slab prior.</p> <h3 id="spike-and-slab-prior">Spike-and-slab prior</h3> <p>Intuitively, given a specific phenotype \(y\), only a very small subset of all the ~20000 genes will be causally related to the it. This means that the true effect sizes of most genes should be zero, and only a handful of genes should have non-zero effect sizes. The spike and slab prior allows us to model this belief by assuming that the effect sizes \(\beta_i\) are either zero or drawn from a normal distribution with zero mean and variance \(\sigma_{\beta}^2\). The probability of a non-zero effect size is given by the spike probability \(\pi\), which is the proportion hyperparameter of a Bernouilli.</p> \[\begin{equation} \label{eq:prior} \begin{split} s_i &amp; \sim \text{Bernouilli}(\pi) \\ \beta_i &amp; |s_i=1 \sim \mathcal{N}(0, \sigma_{\beta}^2) \\ p(\beta_i, s_i) &amp; = \mathcal{N}(\beta_i \vert 0, \sigma_{\beta}^2)\pi^{s_i}(1-\pi)^{1-s_i} \\\\ \end{split} \end{equation}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Bernoulli</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="o">**</span><span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span><span class="o">**</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">expectation</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span>
    
    <span class="k">def</span> <span class="nf">expectation_log</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">p</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">update_p</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
</code></pre></div></div> <h3 id="likehood">Likehood</h3> <p>The likelihood simply models the probability of the phenotype \(y\) given the SNPs \(x\), the effect sizes \(\beta\), and the boolean causal indicators \(s\) as a normal distribution centered at the model output with environmental variance \(\sigma_{\epsilon}^2\), that is not captured by the model.</p> \[\begin{split} p(y\vert X,\beta, s) = \mathcal{N}(X (s \circ \beta), \sigma_{\epsilon}^2) \\\\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Normal</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">sigma</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">.</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">mu</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">sigma</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">expectation</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">mu</span>
    
    <span class="k">def</span> <span class="nf">update_mu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">update_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span>
</code></pre></div></div> <h3 id="variational-distribution">Variational distribution</h3> <p>Now, we can derive the variational distribution \(q\) that we will use to approximate the posterior distribution \(p\). Recall from <a href="/blog/2023/variational-inf-1/">part 1</a>, that to get \(q\), we need to factorize the ELBO objective into a product of independent causal variables \(z_i\) using the mean-field assumption, and then maximize the ELBO with respect to each \(z_i\) to obtain closed form expression for \(q\).</p> <p>Recall that under the mean-field assumption, we can factorize the ELBO as follows:</p> \[\begin{equation} \label{eq:elbo} \begin{split} \text{ELBO}(q\vert\phi) &amp; = \sum_j \int q(z_j)\log \frac{\text{exp}(\mathbb{E}_{q(z_i)}[\log p(X,z)]_{i\neq j})}{q(z_j)}dz_j - \sum_{i\neq j}\int q(z_i)\log q(z_i)dz_i \\ &amp; = -\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] + \mathbb{H}(z_{i\neq j}) + C \\ \end{split} \end{equation}\] <p>Further, recall that this expression is maximized only if \(-\mathbb{KL}[q_j \| \tilde{p}_{i\neq j}] = 0\) for all \(j\), which occurs when \(\log q(z_j\vert\phi) = \log \tilde p_{i\neq j}\). We use this constraint to derive closed form for the variational distribution \(q(\beta,s\vert y,X) = \prod_{j} q(\beta_j\vert s_j)q(s_j)\).</p> <p>First, let’s derive \(q^*(\beta_j\vert s_j)\) by completing the square:</p> \[\begin{equation} \begin{split} \log q^*(\beta_j\vert s_j=1) &amp; = \log \tilde p_{i\neq j} \\ &amp; = \frac{1}{C'}\mathbb{E}_{q(\beta_i, s_i)}[\log p(y\vert X,\beta,s)+\log p(\beta_j)]_{i\neq j} \\ &amp; \propto \mathbb{E}_{q(\beta_i, s_i)}\Biggr[-\frac{1}{2}\Biggl\{(\tau_{\epsilon} x^T_jx_j+\tau_\beta)\beta_j^2 - 2\tau_\epsilon(y-\hat{y}_i)^Tx_j\beta_j\Biggl\}\Biggr]_{i\neq j} \\ &amp; \sim \mathcal{N}(\mu^*_{\beta_j}, \frac{1}{\tau^*_{\beta_j}}) \\ &amp; \text{where} \\ &amp; \tau^*_{\beta_j} = \tau_\epsilon x^T_jx_j+\tau_\beta \\ &amp; \mu^*_{\beta_j} = \frac{\tau_\epsilon}{\tau^*_{\beta_j}}\mathbb{E}_{q(\beta_i,s_i)}[(y-\hat{y_i})^Tx_j]_{i\neq j} = N\frac{\tau_\epsilon}{\tau^*_{\beta_j}}\Biggl(\frac{y^Tx_j}{N}-\sum_{i\neq j}\gamma^*_i \mu^*_{\beta_i} r_{ij} \Biggl) \\ \end{split} \end{equation}\] <p>Now, let’s derive \(q^*(s_j)\):</p> \[\begin{split} \log q(s_j=1) &amp; = \log \tilde p_{i\neq j} \\ &amp; = \frac{1}{C'}\mathbb{E}_{q(s_i)}[\log p(y\vert \hat{y_i},s_j=1)+\log p(s_j=1)]_{i\neq j} \\ &amp; \propto \frac{N}{2}\log\tau_{\epsilon}-C+\frac{1}{2}\log\tau_{\beta}-\frac{1}{2}\tau^*_{\beta_j}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2+\log{\pi} \\\\ \log q(s_j=0) &amp; \propto \frac{N}{2}\log\tau_{\epsilon}-C+\log{1-\pi} \\\\ q^*(s_j) &amp; = \frac{\text{exp}(\log q(s_j=1))}{\text{exp}(\log q(s_j=0)) + \text{exp}(\log q(s_j=1))} \\ &amp; = \frac{1}{1+\text{exp}(-u_j)} = \gamma^*_j \\ &amp; \text{where}\;\; u_j = \log\frac{\pi}{1-\pi} + \frac{1}{2}\log\frac{\tau_\beta}{\tau^*_{\beta_j}}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2 \\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Variational</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="n">m</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">latents</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">Normal</span><span class="p">,</span> <span class="n">Bernoulli</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="n">m</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">precision</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span> <span class="o">=</span> <span class="p">[(</span><span class="nc">Normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="n">precision</span><span class="p">),</span> <span class="nc">Bernoulli</span><span class="p">(</span><span class="n">gamma</span><span class="p">))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">pdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">pdf</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">pdf</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">logpdf</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">logpdf</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>

    <span class="k">def</span> <span class="nf">update_mu</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">update_mu</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">precision</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">update_sigma</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">precision</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">j</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">update_p</span><span class="p">(</span><span class="n">gamma</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_mu</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">mu</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">get_sigma</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="n">sigma</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
    
    <span class="k">def</span> <span class="nf">get_gamma</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">p</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">m</span><span class="p">)])</span>
</code></pre></div></div> <p>Now that we have found closed form expressions for \(q^*(\beta_j\vert s_j)\) and \(q^*(s_j)\), we can use them to compute the ELBO objective and maximize it with respect to the hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). However, we will soon realize that we cannot directly solve this optimization problem since there is a circular dependency between the hyperparameters \(\phi\) and the variational distribution parameters \(\tau^*_{\beta_j}, \mu^*_{\beta_j}, \gamma^*_j\). In other words, we need to know the optimal hyperparameters to compute the optimal variational distribution, but we also need to know the optimal variational distribution to compute the optimal hyperparameters. This is where the EM algorithm comes in.</p> <h3 id="e-step">E-step</h3> <p>In the E-step of the EM algorithm, we simply update the variational distribution parameters \(\tau^*_{\beta_j}, \mu^*_{\beta_j}, \gamma^*_j\) using the current fixed hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). This is done by evaluating the closed form expressions we derived above, i.e.:</p> \[\begin{equation} \label{eq:estep} \begin{split} &amp; \tau^*_{\beta_j} = \tau_\epsilon x^T_jx_j+\tau_\beta \\ &amp; \mu^*_{\beta_j} = N\frac{\tau_\epsilon}{\tau^*_{\beta_j}}\Biggl(\frac{y^Tx_j}{N}-\sum_{i\neq j}\gamma^*_i \mu^*_{\beta_i} r_{ij} \Biggl) \\ &amp; \gamma^*_j = \frac{1}{1+\text{exp}(-u_j)},\;\; u_j = \log\frac{\pi}{1-\pi} + \frac{1}{2}\log\frac{\tau_\beta}{\tau^*_{\beta_j}}+\frac{\tau^*_{\beta_j}}{2}{\mu^*_{\beta_j}}^2 \\\\ &amp; q(\beta_j\vert s_j=1) = \mathcal{N}(\mu^*_{\beta_j}, \frac{1}{\tau^*_{\beta_j}})\qquad q(s_j=1) = \gamma^*_j \end{split} \end{equation}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">E_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Update the latent distribution parameters using the other latents parameters,  
    the hyperparameters and the observed data.
    </span><span class="sh">"""</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="n">new_precision</span> <span class="o">=</span> <span class="n">n</span> <span class="o">*</span> <span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span>

        <span class="n">new_mu</span> <span class="o">=</span> <span class="n">n</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">/</span><span class="n">new_precision</span> <span class="o">*</span>  \
            <span class="p">(</span><span class="n">mbeta</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">delete</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">prod</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span>    \
                <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">]]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">j</span><span class="p">)))</span>

        <span class="n">new_uj</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]))</span> \
            <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">/</span> <span class="n">new_precision</span><span class="p">)</span> \
            <span class="o">+</span> <span class="n">new_precision</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">new_mu</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">new_gamma</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">new_uj</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_mu</span><span class="p">(</span><span class="n">new_mu</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_sigma</span><span class="p">(</span><span class="n">new_precision</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_gamma</span><span class="p">(</span><span class="n">new_gamma</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>

    <span class="c1"># After a full cycle of updates, we cap gamma to avoid numerical instability
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">update_gamma</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">),</span> <span class="n">j</span><span class="p">)</span>
</code></pre></div></div> <h3 id="m-step">M-step</h3> <p>In the M-step, we use the updated variational distribution to maximize the ELBO objective with respect to the hyperparameters \(\phi=(\tau_\epsilon,\tau_\beta, \pi)\). This is done by setting the gradient of the ELBO objective with respect to each hyperparameter to zero, and solving for the optimal hyperparameters.</p> \[\begin{split} &amp; \frac{\partial \, \text{ELBO}}{\partial \tau_\epsilon} = 0 \iff \tau_\epsilon^{-1} = \mathbb{E}_q[\frac{1}{N}(y-X(s\circ\beta)^T(y-X(s\circ\beta)))] \\\\ &amp; \frac{\partial \, \text{ELBO}}{\partial \tau_\beta} = 0 \iff \tau_\beta^{-1} = \sum_j \gamma^*_j({\mu^*_j}^2+{\tau^*_{\beta_j}}^-1)/\sum_j\gamma^*_j \\\\ &amp; \frac{\partial \, \text{ELBO}}{\partial \pi} = 0 \iff \pi = \frac{1}{M}\sum_j \gamma^*_j \\ \end{split}\] <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">M_step</span><span class="p">(</span><span class="n">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Update the hyperparameters using the current latent parameter estimates and the data.
    In this tutorial, we don</span><span class="sh">'</span><span class="s">t update the tau_epsilon hyperparameter for simplicity.
    </span><span class="sh">"""</span>
    <span class="n">new_tau_epsilon</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span>

    <span class="n">new_tau_beta_inv</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span>  \
        <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_sigma</span><span class="p">()))</span>   \
            <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>
    
    <span class="n">new_pi</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>

    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_tau_epsilon</span>
    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">new_tau_beta_inv</span>
    <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_pi</span>
</code></pre></div></div> <h3 id="algorithm">Algorithm</h3> <p>Now that we have derived the E and M steps, we can simply alternate between them until the ELBO objective converges to a maximum.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_elbo</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="sh">"""</span><span class="s">
    Compute the evidence lower bound (ELBO) of the model by using the current variational 
    distribution and the joint likelihood of the data and the latent variables. These 
    distributions are parameterized by our current estimates of hyperparameter values and 
    latent distribution parameters.
    </span><span class="sh">"""</span>
    <span class="n">exp_var_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">([</span><span class="n">v</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation_log</span><span class="p">()</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">])</span>
    <span class="n">exp_var_beta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">])</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span>

    <span class="n">summand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> \
                              <span class="n">np</span><span class="p">.</span><span class="nf">power</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">(),</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_sigma</span><span class="p">())</span>
    
    <span class="n">exp_true_beta</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_beta</span><span class="sh">"</span><span class="p">]</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">summand</span><span class="p">)</span>
    <span class="n">exp_true_s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">])</span>   \
                        <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">())</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">pi</span><span class="sh">"</span><span class="p">]))</span>
    
    <span class="n">double_summand</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">m</span><span class="p">):</span>
            <span class="n">gamma_j</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">mu_j</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">gamma_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">1</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">mu_k</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="n">latents</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="mi">0</span><span class="p">].</span><span class="nf">expectation</span><span class="p">()</span>
            <span class="n">double_summand</span> <span class="o">+=</span> <span class="n">gamma_j</span><span class="o">*</span><span class="n">mu_j</span><span class="o">*</span><span class="n">gamma_k</span><span class="o">*</span><span class="n">mu_k</span><span class="o">*</span><span class="n">ld</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">]</span>
    
    <span class="n">exp_true_y</span> <span class="o">=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">])</span>  \
        <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">n</span> \
        <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">multiply</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_gamma</span><span class="p">(),</span> <span class="n">self</span><span class="p">.</span><span class="n">var</span><span class="p">.</span><span class="nf">get_mu</span><span class="p">())</span><span class="o">@</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">mbeta</span><span class="p">)</span>    \
        <span class="o">-</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">n</span><span class="o">*</span><span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">summand</span><span class="o">*</span><span class="n">ld</span><span class="p">.</span><span class="nf">diagonal</span><span class="p">())</span>   \
        <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">hparams</span><span class="p">[</span><span class="sh">"</span><span class="s">tau_epsilon</span><span class="sh">"</span><span class="p">]</span><span class="o">*</span><span class="p">(</span><span class="n">n</span><span class="o">*</span><span class="n">double_summand</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">exp_true_y</span> <span class="o">+</span> <span class="n">exp_true_beta</span> <span class="o">+</span> <span class="n">exp_true_s</span> <span class="o">-</span> <span class="n">exp_var_beta</span> <span class="o">-</span> <span class="n">exp_var_s</span>

<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">mbeta</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">ld</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tol</span><span class="p">:</span> <span class="nb">float</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
    <span class="sh">"""</span><span class="s">
    Run the EM algorithm for a given number of iterations or until convergence.
    </span><span class="sh">"""</span>
    <span class="n">elbo</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nc">E_step</span><span class="p">(</span><span class="n">mbeta</span><span class="p">,</span> <span class="n">ld</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nc">M_step</span><span class="p">()</span>
        <span class="n">elbo</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">compute_elbo</span><span class="p">(</span><span class="n">mbeta</span><span class="p">,</span> <span class="n">ld</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nf">abs</span><span class="p">(</span><span class="n">elbo</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">elbo</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">])</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
            <span class="k">break</span>
    <span class="k">return</span> <span class="n">elbo</span>
</code></pre></div></div> <h3 id="results">Results</h3> <p>Here are some results I obtained from a simulated dataset with M=100 SNPs given the marginal effect sizes \(\beta\) and the ld matrix \(R\). First let’s take a look at the training curve of the ELBO objective. We can see that the ELBO converges to a maximum after 5 iterations.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/em_elbo-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/em_elbo.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 1. ELBO loss as a function of interation number of EM algorithm </div> <p>Next, we can take a look at the linear model’s predictions of the phenotype \(y\) given the baseline marginal effect sizes \(\beta\), and compare them to the predictions of the variational model with the new posterior effect sizes \(\beta^{new}\). We can see that the variational model is able to better capture the relationship between the SNPs and the phenotype, which is expected since the variational model is able to incorporate our prior domain knowledge about the SNPs.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/train_preds.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/train_preds_marginal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/test_preds.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/test_preds_marginal.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Fig 2. Predictions of the phenotype y given the SNPs x using the variational model (left) and the marginal model (right) </div> <p>Finally, let’s take a look at posterior inclusion probability of each SNP, which is the probability that the SNP has a non-zero effect size. We can see that the variational distribution \(q(s)\) is able to correctly identify the SNPs with non-zero effect sizes.</p> <div class="fake-img"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/2023-10-15-variational-inf/snp_pips-1400.webp"></source> <img src="/assets/img/blog/2023-10-15-variational-inf/snp_pips.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="caption"> Fig 3. Posterior inclusion probability of each SNP. Causal SNPs are highlighted in red. </div> <p>The complete code for this project can be found <a href="https://github.com/TheMatrixMaster/variational-inference" rel="external nofollow noopener" target="_blank">here</a>.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2023-10-09-variational-inf.bib"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"alshedivat/al-folio","data-repo-id":"MDEwOlJlcG9zaXRvcnk2MDAyNDM2NQ==","data-category":"Comments","data-category-id":"DIC_kwDOA5PmLc4CTBt6","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2024 Stephen Z. Lu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a>. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: July 02, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZYZ3B9SH7B"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-ZYZ3B9SH7B");</script> </body> </html>